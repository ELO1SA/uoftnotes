% Meta data
\documentclass[oneside, 12pt]{book}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage[pdfencoding=auto, psdextra]{hyperref}
\usepackage{fancyhdr} 

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\qed}{\hfill $\mathcal{Q}.\mathcal{E}.\mathcal{D}.\dagger$}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\text{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% Call settag{\ldots} first to initialize, and then \para{} for a new paragraph
\newcommand{\para}[1]{\item \tbf{#1}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vzero}{\mathbf{0}}
% For convenience, I am setting both of these to refer to the same thing.
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bzero}{\mathbf{0}}


\pdfinfo{
   /Author (Tingfeng Xia)
   /Title  (MAT224 Linear Algebra: Definitions, Lemmas, Theorems, Corollaries and their related proofs)
   /CreationDate (D:20190107)
   /Subject (MAT224 Linear Algebra)
}

\title{%
  \textbf{MAT224 Linear Algebra}\\
  \large Definitions, Lemmas, Theorems, Corollaries \\
    and their related proofs}
\author{Tingfeng Xia}
\date{Winter 2019}

\begin{document}

\frontmatter
\maketitle
\newpage % make a new page for actual contents, possibly a content table
\mbox{}
\vfill
\noindent by Tingfeng Xia \\


\noindent Materials in this booklet are based heavily on Prof. Nicholas Hoell's lectures as well as 
A COURSE IN LINEAR ALGEBRA by David B. Damiano and John B. Little.\newline\newline
Items in this booklet are, in fact, very similar to those in the book and are intended to be used as a bullet point guide to (almost) all the knowledge points and should \tit{certainly not} be used as a substitute for the actual learning material. Please notice that I make\tit{ no promise} about the accuracy of statements appearing in these notes.\newline\newline
The course website could be found here:\newline \url{http://www.math.toronto.edu/nhoell/MAT224/}
\newpage

\tableofcontents

\mainmatter

\chapter{Vector Spaces}
\section{(Real) Vector Space}
\begin{enumerate}
    \settag{1.1.1}
    \item{\textbf{Definition of real vector space:}} A real vector space is a set $V$ together with
    \begin{enumerate}
        \item \textbf{Closure under vector addition:} an operation called vector addition, which for each pair of vectors $\mathbf{x}, \mathbf{y}\in V$ produces another vector in $V$ denoted $\mathbf{x} + \mathbf{y}$, (i.e. $\forall \mathbf{x}, \mathbf{y}\in V, \mathbf{x} + \mathbf{y} \in V$) and
        \item \textbf{Closure under scalar multiplication:} an operation called multiplication by a scalar (a real number), which for each vector $\mathbf{x}\in V$, an each scalar $c\in \mathbb{R}$ produces another vector in $V$ denoted $c\mathbf{x}$. (i.e. $\forall \mathbf{x}\in V, \forall c \in \mathbb{R}, c \mathbf{x} \in V$)
    \end{enumerate}
    Furthermore, the two operations must satisfy the following axioms:
    %\begin{enumerate}
        %\item For all vectors \textbf{x, y}, and \textbf{z} $\in V$, %(\textbf{x}+\textbf{y})+\textbf{x} 
    %\end{enumerate}
    \begin{enumerate}
        \item $\forall \mathbf{x}, \mathbf{y}, \mathbf{z} \in V, (\mathbf{x} + \mathbf{y}) + \mathbf{z} = \mathbf{x} + (\mathbf{y} + \mathbf{z})$
        \item $\forall \mathbf{v}, \mathbf{y} \in V, \mathbf{x} + \mathbf{y} = \mathbf{y} + \mathbf{x}$
        \item $\exists \vzero \in V s.t. \forall \mathbf{x} \in V, \mathbf{x} + \vzero = \mathbf{x}$ (Note that this property is a.k.a existence of additive identity)
        \item $\forall \mathbf{x} \in V, \exists (-\mathbf{x}) \in V~s.t.~\mathbf{x} + (-\mathbf{x}) = \vzero$ (Note that this property is a.k.a existence of additive inverse)
        \item $\forall \mathbf{x}, \mathbf{y} \in V, c \in \mathbb{R}, c(\mathbf{x} + \mathbf{y}) = c\mathbf{x} + c\mathbf{y}$
        \item $\forall \mathbf{x} \in V, c,d \in \mathbb{R}, (c + d)\mathbf{x} = c\mathbf{x} + d\mathbf{x}$
        \item $\forall \mathbf{x} \in V, c,d \in \mathbb{R}, (cd)\mathbf{x} = c(d\mathbf{x})$
        \item $\forall \mathbf{x} \in V, 1\mathbf{x} = \mathbf{x}$
    \end{enumerate}
    \tbf{Remark.} Note that here we are not explicitly defining a vector space to be non-empty, however, if a set ever fails to have a larger than zero cardinality, it must not be a vector space. This is a consequence of axiom $(c)$, the existence of the one and only zero vector in the space.
    
    \settag{1.1.6}
    \para{Propositions for a R-v.s.} Let $V$ be a vector space. Then 
    \begin{enumerate}
        \item The zero vector is unique. Note that it might not necessarily be actually the zero vector 
        in $\mathbb{R}^n$ that we are somewhat used to use.\newline
        \proof \newline
        Suppose, for the sake of contradiction, that $\mathbf{a},\mathbf{b}$ are two \tit{different} zero 
        vectors of the vector space $V$. Then, by the definition of zero vector, we have 
        \begin{equation*}
            \forall \mathbf{x}\in V,\mathbf{x} +\mathbf{a} = \mathbf{x} \land \mathbf{x} + \mathbf{b} = 
            \mathbf{x}
        \end{equation*}
        Simple algebraic manipulation yields us $\mathbf{x} + \mathbf{a} = \mathbf{a} + \mathbf{x} \implies 
        \mathbf{a} = \mathbf{b}$ $\longrightarrow\!\longleftarrow$ Contradiction! \qed
        
        \item $\forall \mathbf{x} \in V, 0\mathbf{x}=0$ \newline
        \proof \newline 
        We have $0\vx = (0+0)\vx =0\vx +0\vx$, by axiom 6. By axiom $(d)$, I know there exists a additive 
        inverse of $0\vx$, so I subtract on both sides of the equation such additive inverse. This yields 
        us $\vzero = 0\vx$ as wanted. \qed
        
        \item $\forall \mathbf{x} \in V, $ the additive inverse is unique. Note that it might not necessarily 
        be actually just $(-1)$ times the vector in $\mathbb{R}^n$ that we are somewhat used to use. \newline
        \proof \newline
        Let $\vx\in V$, and let $(-\vx),(-\vx)'$ be two additive inverse of $\vx$. Then, on one hand, 
        by axioms 1, 4 and 3 we have 
        \begin{align*}
            \vx + (-\vx) + (-\vx)'
            &= (\vx + (-\vx)) + (-\vx)' \\
            &= \vzero + (-\vx)' \\
            &= (-\vx)'
        \end{align*}
        On the other hand, by axiom 2, we have 
        \begin{align*}
            \vx + (-\vx) +(-\vx)' 
            &= \vx + (-\vx)' + (-\vx) \\
            &= (\vx + (-\vx)') + (-\vx) \\
            &= \vzero + (-\vx) \\
            &= -\vx
        \end{align*}
        Hence we conclude that $(-\vx) = (-\vx)'$, and this completes the proof. \qed
        
        \item $\forall \mathbf{x} \in V,~\forall c\in \mathbb{R}, ~(-c)\mathbf{x}=~-(c\mathbf{x})$ \newline
        \proof \newline
        We have
        \begin{align*}
            c\vx + (-c)\vx &= (c +~-c)\vx \\
            &= \vzero\vx \\
            &= \vzero
        \end{align*}
        Then we notice that equating the fist and the last of the equations above completes the proof. \qed
        
    \end{enumerate}
\end{enumerate}

\section{Sub-spaces}
    \begin{enumerate}
        \settag{1.2.4}
        \item \textbf{Lemma on functions in $C^0(\mathbb{R})$}. Note that by $C^n(\cdot)$ we mean the function 
        in this set are all of $Class-n$. Let $f,g \in C^0(\mathbb{R)}, \text{let} ~c\in \mathbb{R}$. Then, 
        \begin{enumerate}
            \item $f+g\in C^0(\R)$, and
            \item $cf \in C^0(\R)$
        \end{enumerate}
        The proof of this lemma relies on limit theorems of calculus.\newline
        
        \settag{1.2.6}
        \item \textbf{(Intuitive) definition of (vector) subspace.} Let $V$ be a vector space and let $W\subseteq V$
         be a subset. Then $W$ is a (vector) subspace if $W$ is a vector subspace itself under the operations of vector
          sum and scalar multiplication from $V$.
        
        \settag{1.2.8}
        \item \textbf{Quick check rule for a subspace.} Let $V$ be a vector subspace, and let $W$ be a \textbf{non empty} 
        subset of $V$. Then $W$ is a subspace of $V$ \tit{if and only if} 
        \begin{equation*}
            \forall \mathbf{x}, \mathbf{y}\in W,~\forall c\in \R, \text{ we have } c\mathbf{x}+\mathbf{y}\in W
        \end{equation*}
        Notice that the \tit{if and only if} makes this theorem extremely powerful in the sense that it will save you a
         lot of time to come up with a dis-proofing counter example. However, do keep in mind that there are cases that 
         this theorem is very hard to check, for example, Exercise 1.2, Question 3(c).
        
        \settag{1.2.9}
        \item \textbf{Remark on the necessary condition of non-emptiness of subspace.} According to the definition of 
        vector space that we gave in 1.1.1, a vector space must contain an additive identity element, hence it is necessary 
        that we ensure $W\subseteq V$(from 1.2.6) is not an empty set.
        
        \settag{1.2.13}
        \item \textbf{Theorem: Intersection of sub-spaces is a subspace.} Let $V$ be a vector space. Then the intersection
         of any collection of sub-spaces of $V$ is a subspace of $V$.
        
        \settag{1.2.14}
        \item \textbf{Corollary: Hyper planes in $\R^n$ are sub-spaces of $\R^n$.} Let $a_{ij}(1\leq i\leq m)$, let
         $W_i = \{(x_1, \ldots, x_n)\in \R^n|a_{i1}x_1 + \ldots + a_{in}x_n = 0,~\forall 1 \leq i \leq m\}$. Then 
         $W$ is a subspace of $\R^n$.
        
    \end{enumerate}
    
\section{Linear Combinations}
    \begin{enumerate}
        \settag{1.3.1}
        \item \textbf{Definitions regarding L.C. and derived spans.} Let $S$ be a subset of a vector space $V$, that is $S\subseteq V$.
        \begin{enumerate}
            \item a \textit{linear combination} of vectors in $S$ is any sum $a_1\mathbf{x}_1 + \ldots + a_n\mathbf{x}_n$, 
            where the $a_i \in \R$, and the $x_i \in S$.
            \item we define the $Span$ of a set of vectors as follows to consider the special case of 
            $S\stackrel{?}{=}\emptyset \in V$.
            \newline \underline{\tit{Case1: $S\neq \emptyset$:}} In this case, we define $Span(S)$ to be all
             possible linear combinations using vectors in $S$.\newline
            \underline{\tit{Case2: $S= \emptyset$:}} In this case, we define $Span(S = \emptyset)=\{\vzero\}$. We call 
            this the zero-space.
            
            \item If $W=Span(S)$, we say $S$ \textit{spans(\text{or} generates)} $W$.
        \end{enumerate}
        
        \settag{1.3.4}
        \item \textbf{Span of a  subset of a vector space is a subspace.} Let $V$ be a vector space and let $S$ be any subset of $V$. Then $Span(S)$ is a subspace of $V$.
        
        \settag{1.3.5}
        \item \textbf{Sum of sets(with application to subs-paces).} Let $W_1~\text{and}~W_2$ be sub-spaces of a vector space $V$. The sum of $W_1$ and $W_2$ is the set
        \begin{center}
            $W_1+W_2 :=\{\mathbf{x}\in V |\mathbf{x}=\mathbf{x_1}+\mathbf{x_2}, \text{for some } \mathbf{x_1}\in W_1, \mathbf{x_2} \in W_2\}$
        \end{center}
        We think of the sum of the two sub-spaces(the two sets) as the set of vectors that can be built up from the vectors in $W_1$ and $W_2$ by linear combinations. Conversely, the vectors in the set $W_1+W_2$ are precisely the vectors that can be broken down into the sum of a vector in $W_1$ and a vector in $W_2$. One may find it helpful to view this as an analogue to a Cartesian product of the two set with a new constraint on the result.
        
        \settag{1.3.6}
        \item \textbf{Example.} If $W_1 = \{(a_1, a_2)\in \R^2|a_2 = 0\}$ and $W_2 \{(a_1, a_2)\in \R^2|a_1 = 0\}$, then $W_1 + W_2= \R^2$, since every vector in $\R^2$ can be written as the sum of vector in $W_1$ and a vector in $W_2$. For instance, we have $(5, -6)=(0, 5)+(0, -6)$, and $(5, 0)\in W_1 \land (0, -6)\in W_2$
        
        
        \settag{1.3.8} 
        \item \textbf{Proposition: The sum of spans of sets is the span of the union of the sets.} Let $W_1 = Span(S_1)$ and $W_2 = Span(S_2)$ be sub-spaces of a\textit{(the same)} vector space $V$. Then $W_1 + W_2 = Span(S_1 \cup S_2)$. Notice that the proof of this gave the important idea of mutual inclusion in proving sets are equal to each other.
        
        \settag{1.3.9}
        \item \textbf{The sum of sub-spaces is also a subspace.} Let $W_1$ and $W_2$ be sub-spaces of a vector space $V$. Then $W_1 + W_2$ is also a subspace of $V$.\newline
        \underline{\textit{Proof:}}\newline
        It is clear that $W_1 + W_2$ is non-empty, since neither $W_1$ nor $W_2$ is empty. Let $\mathbf{x}, \mathbf{y}$ be two vectors in $W_1+W_2$, let $c\in \R$. By our choice of $\mathbf{x}\text{and } \mathbf{y}$, we have
        \begin{align*}
            c\mathbf{x} + \mathbf{y} & = c(\mathbf{x}_1 + \mathbf{x}_2) + (\mathbf{y_1} + \mathbf{y_2}) \\
            & = (c\mathbf{x}_1 + \mathbf{y}_1) + (c\mathbf{x}_2 + \mathbf{y}_2) \\
            &\in W_1+W_2
        \end{align*}
        Since $W_1$ and $W_2$ are sub-spaces of $V$, we have $(c\mathbf{x}_1 + \mathbf{y}_1) \in W_1\land (c\mathbf{x}_2 + \mathbf{y}_2)\in W_2$. Then by (1.2.8), we see that indeed $W_1 + W_2$ is a subspace of $V$. \qed
        
        \settag{1.3.10}
        \item \textbf{Remark.} In general, if $W_1$ and $W_2$ are sub-spaces of $V$, then $W_1 \cup W_2$ will not be a subspace of $V$. For example, consider the two sub-spaces of $\R^2$ given in example (1.3.6). In that case $W_1\cup W_2$ is the union of two lines through the origin in $\R^2$. 
        
        \settag{1.3.11}
        \item \textbf{Proposition.} Let $W_1$ and $W_2$ be sub-spaces of vector space $V$ and let $W$ be a subspace of $V$ such that $W\supseteq W_1 \cup W_2$, then $W\supseteq W_1 + W_2$. Informally speaking, this proposition saying: "$W_1+W_2$ is the smallest subspace containing $W_1\cup W_2$", i.e., Any subspace that contains $W_1\cup W_2$ must be a super set of $W_1 + W_2$. \newline
        \proof\newline
        We want to show: $W\supseteq W_1\cup W_2\Longrightarrow W\supseteq W_1 + W_2$\newline
            \begin{align*}
                \text{Assume that } \text{$W\supseteq W_1 \cup W_2$. Let $w_1\in W_1,~w_2\in W_2$.}\\
                \text{We notice that } w_1,~w_2\in W_1\cup W_2 \subseteq W \\
                \implies& w_1, w_2\in W \\
                \text{(Since $W$ is a subspace, so it is closed under addition)}\\
                \implies& w_1 + w_2 \in W\\
                \implies W_1 + W_2 \subseteq W \iff& W \supseteq W_1 + W_2 
            \end{align*}
            \qed
        
    \end{enumerate}

\section{Linear (In)dependence}
    \begin{enumerate}
        \settag{1.4.2}
        \item \textbf{Algebraic definition of linear dependence.} Let $V$ be a vector space, and let $S\subseteq V$.
            \begin{enumerate}
                \item A \textit{linear dependence} among the vectors of $S$ is an equation $a_1\mathbf{x} + \ldots + a_n\mathbf{x}_n = \vzero$ where the $x_i\in S$, and the $a_i\in \R$ are not all zero(i.e., at least one of the $a_i\neq 0$). In familiar\footnote{Familiar from MAT223, Prof. Jason Siefken's IBL(Inquiry Based Learning) notes.} words, there exists a non-trivial solution to the equation mentioned above.
                \item the set $S$ is said to be \textit{linearly dependent} if there exists a linear dependence among the vectors in $S$.
            \end{enumerate}
            \tbf{Remark: }It can be shown that the geometric\footnote{A set of vectors is said to be dependent of each other there exists a vector in this set, that it is in the Span of all other vectors in the set. I.e., There is some vectors in this set that are "redundant", it's position can be taken by some linear combination of the other vectors in the set.} definition and this are, in-fact, equivalent to each other. I will now produce the proof. \newline
        \tit{\underline{Proof of equivalence of definitions:}} \newline
        Let $V$ be a vector space, and let $S\subseteq V$. Consider the following equation:
        \begin{align*}
            a_1\mathbf{x}_1 + a_2\mathbf{x}_2 + \ldots + a_n\mathbf{x}_n &= 0\text{, where } \exists a_i\neq 0 \\
            \text{(WLOG, assume that } a_n &\neq \vzero\text{)}\\
            \implies \frac{a_1\mathbf{x}_1 + a_2\mathbf{x}_2 + \ldots + a_n\mathbf{x}_n}{a_n}&=\vzero\\
            \implies \mathbf{x}_n &= -\sum_{i=1}^{n-1}a_i\mathbf{x}_i
        \end{align*}
        Notice that the result $\mathbf{x}_n$ is in terms of all the other $(n-1)$ vectors in the set, hence a linear combination of those vectors, and this completes the proof.
        \qed\newline
        \textbf{Re-Remark: }We can also use this proof as an argument towards the following problem: Show that at least one of the vectors in a linearly dependent set is redundant. We could take take a similar proof and argue that the linear combination could be written without at least one of the vectors.
        
        \settag{1.4.4}
        \item \textbf{Algebraic definition of linear independence.} Let $V$ be a vector space, and $S\subseteq V$. Then $S$ is \textit{linearly independent} if whenever we have $a_i\in \R$ and $x_i\in S$ such that $a_1\mathbf{x}_1+\ldots+a_n\mathbf{x}_n = \vzero$, then $a_i = 0,~\forall i$. A more conceivable way to understand this is if the aforementioned equation exists and only exists a set of trivial solution then the vectors involved in the equation are \textit{linearly independent}. \newline \tbf{Remark: }A set of vector is linearly independent \tit{if and only if} it is not linearly dependent.
        
        \settag{1.4.7}
        \item \textbf{Propositions regarding linear (in)dependency.}
            \begin{enumerate}
                \item Let $S$ be a linearly dependent subset of a vector space $V$, and let $S'$ be another subset of $V$ that contains $S$. Then $S'$ is also linearly dependent.
                \item Let $S$ be a linearly independent subset of vector space $V$ and let $S'$ be another subset of $V$ that is contained in $S$. Then $S'$ is also linearly independent.
            \end{enumerate}
        \underline{\tit{Proof of (a):}}
        Since $S$ is linearly dependent, there exists a linear dependence among the vectors in $S$, say, $a_1\mathbf{x}_1 + \ldots+ a_n\mathbf{x}_n = \vzero$. Since $S$ is contained in $S'$, this is also a linear dependence among the vectors in $S'$. Hence $S'$ is linear dependent.\qed\newline
        \underline{\tit{Proof of (b):}}
        Consider any equation $a_1\mathbf{x}_1 + \ldots+ a_n\mathbf{x}_n = \vzero$, where the $a_i \in \R,~\mathbf{x}_i \in S'$. Since $S'$ is contained in $S$, we can also view this as a potential linear dependence among vectors in $S$. However, $S$ is linearly independent, so it follows that all the $a_i = 0\in \R$. Hence $S'$ is also linearly independent.\qed
        
        \settag{1.4.*}
        \item \tbf{Example of showing linear independence.(1)} Show that the set $\{1,x,x^2,x^3,\ldots,x^n\}$ is linearly independent. We consider the following equation:
        \begin{equation*}
            0 = a_0 + a_1x + \ldots + a_{n-1}x^{n-1}+a_n x^n~~~~~~(*)
        \end{equation*}
        Then we take the derivative:
        \begin{equation*}
            \frac{d^n}{dx^n}(*)= 0 = n!\cdot a_n
        \end{equation*}
        Then since $n! \neq 0$, we know $a_n=0$. We repeat the same process, taking derivatives $(n-1)$ times w.r.t. $x$ we get $a_{n-1}=0$ and so on. As a last step we have:
        \begin{equation*}
            \frac{d}{dx}(*) = 0 = 1!\cdot a_1 \implies a_1 = 0
        \end{equation*}
        Then we have $0 = 0 + 0 +\ldots + a_0 \implies a_0 = 0$. So the equation has and only has a trivial solution, thus the set $\{1,x,x^2,x^3,\ldots,x^n\}$ is linearly independent. \qed
        
        \settag{1.4.*}
        \item \tbf{Example of showing linear independence.(2)} Show that the set $\{e^x, e^{2x}\}$ is linearly independent. We consider the following equation:
        \begin{equation*}
            0 = ae^x + be^{2x}~~~~~(1)
        \end{equation*}
        We take derivative on both sides of $(1)$ w.r.t. $x$ and we have:
        \begin{equation*}
            0 = ae^x + 2be^{2x}~~~~(2)
        \end{equation*}
        We subtract $(2)$ from $(1)$ to get:
        \begin{equation*}
            0 = be^{2x} \implies b = 0 \implies 0 = ae^x + 0 \implies a = 0
        \end{equation*}
        Since $a=b=0$ is the one and only solution to $(1)$, we claim they are linearly independent. \qed
    \end{enumerate}
    
\section{Interlude on Solving SLEs}
    \begin{enumerate}
        \settag{1.5.*}
        \item \tbf{Note Aside: }This section of the book is covered, although not rigorously but completely, in MAT223. Hence the vast majority of definitions and corollary in this section were omitted. Consult the book for more detail on this.
        
        \settag{1.5.1}
        \item \tbf{Definition of (homogeneous) SLEs\footnote{System of Linear Equations}} A system of $m$ equations in $n$ unknowns $x_1,\ldots,x_n$ of the form:
        \begin{align*}
            a_{11}\mathbf{x_1}+\ldots+a_{1n}\mathbf{x_n} &= b_1 \\
            a_{21}\mathbf{x_1}+\ldots+a_{2n}\mathbf{x_n} &= b_2 \\
            &\mbox{\ldots} \\
            a_{m_1}\mathbf{x_1}+\ldots+a_{mn}\mathbf{x_n} &= b_m
        \end{align*}
        where the $a_{ij},~b_i\in \R$, is called a \tit{system of linear equations.} The scalars $a_{ij}$ are called coefficients or \tit{weights} of the equations. We call this system \textbf{homogeneous} if and only if all the $b_i$ are 0's.
        
        \settag{1.5.2}
        \item \tbf{Definition of equivalent SLEs} Two systems of linear equations are said to be equivalent if their sets of solutions are the same(i.e., mutual inclusion of the two solution sets)
        
        \settag{1.5.3}
        \item \tbf{Propositions on operations on SLEs\footnote{This should be familiar from MAT223, operations involved in row reducing an augmented matrix for a system of linear equations}}
        \begin{enumerate}
            \item The system obtained buy adding any multiple of any one equation to any second equation, while leaving the other other equations unchanged, is an equivalent system.
            \item The system obtained by multiplying any one equation by a non-zero scalar and leaving the other equations unchanged is an equivalent system.
            \item The system obtained by interchanging any two equations is an equivalent system.
        \end{enumerate}
        
        \settag{1.5.13}
        \item \tbf{Corollary} If $m<n$, every homogeneous system of $m$ linear equations in $n$ unknowns has a non-trivial solution.
    \end{enumerate}
    

\section{Bases and Dimension}
    \begin{enumerate}
        \settag{1.6.1}
        \item \tbf{Definition of basis.} A subset $S$ of a vector space $V$ is called a basis if $V=Span(S)$ and
         the set $S$ is linearly independent.
        
        \settag{1.6.3}
        \item \tbf{Theorem. }Let $V$ be a vector space, and let $S$ be a non-empty subset of $V$. Then $S$ is a
         basis of $V$ if and only if $\forall \mathbf{x} \in V, \mathbf{x}$ can be written \tit{uniquely} as a 
         linear combination of the vectors in $S$.
        
        \settag{1.6.6}
        \item \tbf{Theorem. }Let $V$ be a vector space that has a finite spanning set, and let $S$ be a linearly
         independent subset of $V$. Then there exists a basis $S'$ of $V$, such that $S\subseteq S'.$ Note that 
         this theorem is can be summarized as follows: Every linearly independent set of vectors could be extended
          to a basis. We do so by adding yet another vector that is linearly independent to all the vectors already
           in the set, but notice that this process should possibly be repeated but \tit{not} infinite.
        
        \settag{1.6.8}
        \item \tbf{Lemma on linear independence towards a set and a vector.} Let $S$ be a linearly independent subset
         of $V$ and let $\mathbf{x}\in V$, but $\mathbf{x} \notin S$. Then $S\cup \{\mathbf{x} \}$ is linearly 
         independent if and only if $\mathbf{x} \notin Span(S)$.
        
        \settag{1.6.10}
        \item \tbf{Theorem on cardinality of linearly independent set.} Let $V$ be a vector space and let $S$ 
        be a spanning set for $V$, which has $m$ elements. Then no linearly independent set in $V$ can have 
        more than $m$ elements.\newline
        \proof\newline
        To show that there are no linearly independent set in $V$ that can have more than $m$ elements in it, 
        it suffices to show that every set in $V$ that has more than $m$ elements in it is linearly dependent.
         Let $S=\{y_1,\ldots,y_m\}$ and $S'=\{y_1,\ldots,y_n\}\subset V$ where $n>m$. Now we consider the 
         following equation:
        \begin{equation}
            a_1\mathbf{x}_1 + \ldots + a_n\mathbf{x}_n = \vzero~~~~\text{    where }\mathbf{x}_i \in S',a_i\in \R
        \end{equation}
        Since $S$ is a spanning set for $V$, then $\exists b_{ij}\in \R,~s.t.~\forall 1\leq i \leq m$:
        \begin{equation*}
            \mathbf{x}_i = b_{i1}\mathbf{y}_1 + \ldots + b_{im}\mathbf{y}_m = \sum_{j=1}^{m}b_{ij}\mathbf{y}_j
        \end{equation*}
        Then we substitute the $\mathbf{x}_i$'s into equation (1.1):
        \begin{equation}
            a_1\left(\sum_{j=1}^{m}b_{1j}\mathbf{y}_j\right) + \ldots +  a_n\left(\sum_{j=1}^{m}b_{nj}\mathbf{y}_j\right) = \vzero
        \end{equation}
        Rearranging (1.2), we have:
        \begin{equation}
            \left(\sum_{j=1}^{n}b_{1j}a_{j}\right)\mathbf{y}_1 + \ldots +  \left(\sum_{j=1}^{n}b_{mj}a_{j}\right)\mathbf{y}_m = \vzero
        \end{equation}
        Now we consider the following SLE(The coefficients of (1.3)):
        \begin{center}
            $
            \begin{cases}
            b_{11}a_1 + \ldots + b_{1n}a_n &= 0 \\
            b_{21}a_1 + \ldots + b_{2n}a_n &= 0 \\
            &\vdots \\
            b_{m1}a_1 + \ldots + b_{mn}a_n &= 0
            \end{cases}
            $
        \end{center}
        If we can find a set of $a_1, \ldots, a_n$ that are not all zero that solves the above SLE, then those scalars will 
        give us a linear dependence among the vectors in $S'$ in (1.1). But the above SLE is a system of homogeneous
         linear equations in the $n$ unknowns $a_i$'s, hence since $m < n$, by Corollary (1.5.13), there exists a 
         non-trivial solution. Then $S'$ is linearly dependent and this completes the proof. \qed
        
        
        \settag{1.6.11}
        \item \tbf{Bases of a vector space shall have same cardinality.} Let $V$ be a vector space and let $S$ and 
        $S'$ be two bases of  with $m$ and $m'$ elements, respectively. Then $m=m;$. \newline
        \tit{\underline{Proof: (Euclid's style)}} \newline
        Let $S$ and $S'$ with properties given as above. Since they are bases, we know: (By Theorem 1.6.10)
        \begin{enumerate}
            \item $S$ is a spanning set and $S'$ is a linearly independent set \newline $\Longrightarrow$ $m'=|S'| \leq |S|=m$
            \item $S'$ is a spanning set and $S$ is a linearly independent set \newline $\Longrightarrow$ $m=|S| \leq |S'|=m'$
        \end{enumerate}
        Then, $m'=m$. \qed
        
        \settag{1.6.12}
        \item \tbf{Definitions of (in)finite-dimension}
        \begin{enumerate}
            \item If $V$ is a vector space with some finite basis (possibly empty), we sat $V$ is 
            \tit{finite-dimensional}. We say the vector space is \tit{infinite-dimensional} otherwise.
            \item Let $V$ be a \tit{finite-dimensional} vector space. The dimension of $V$, denoted as dim($V$), 
            is the number of elements in a (hence any) basis of $V$.
            \item If $V=\{\vzero\}$, we define dim($V$)$=0$. \tbf{Remark: }Please note that is consistent with
             our definition of span of a empty set. We defined the span of empty set to be $\{\vzero\}$, and 
             hence $\{\vzero\}$ is the resulting space of all the possibly linear combination of vectors in the 
             empty set(where there is none). Then the cardinality of the empty set(which is zero) is here defined 
             as the dimension of the zero vector space.
        \end{enumerate}
        
        \settag{1.6.13}
        \item \tbf{Examples on dimensions of vector spaces. }If we have a basis of $V$, then computing dim($V$) 
        is simply a matter of counting the number of vectors in a (hence any) basis for the vectors space. We 
        explore this by looking at the following examples
        \begin{enumerate}
            \item For each $n$, dim($\R^n$) $=n$. This is a consequence of the standard basis for $\R^n$ is of 
            the form: $\{\mathbf{e}_1, \ldots, \mathbf{e}_n\}$ has $n$ elements in it.
            \item dim($P_n(\R)$) $=n+1$, since $|\{1, x^1, x^2, \ldots, x^n\}|=n+1$.\footnote{The absolute value 
            marks around a set returns the cardinality of the set.}
            \item The vector spaces $P(\R)$\footnote{Here $P(\R)$ means the vector space of all polynomials that 
            are $\R \xrightarrow{} \R$.}$,~C^n(\R)$, where $n\in \mathbb{N}^{\geq 0}$, are not of finite dimension,
             and hence are called \tit{infinite dimensional}.
        \end{enumerate}
        
        \settag{1.6.14}
        \item \tbf{Corollary. }Let $W$ be a subspace of a finite-dimensional vector space $V$. Then dim($W$)
         $\leq$ dim($V$). Furthermore, dim($W$) $=$ dim($V$) if and only if $W=V$.
        
        \settag{1.6.15}
        \item \tbf{Corollary. }Let $W$ be a subspace of $\R^n$ defined by a system of homogeneous linear 
        equations. The dim($W$) is equal to the number of free variables in the corresponding echelon form 
        of the equations.\newline
        \tbf{Generalization.} It is also worth pointing out that by setting the free variables equal to one
         in turn\footnote{By in turn we mean set one of them to one and all others zero, one at a time}, we 
         can always generate a basis for the subspace. Referring to example $(1.6.16)$ in the book, we see that 
         we have two free variables, and the two (linearly independent) vectors obtained from the method described 
         above that form a 2-dimensional subspace of $\R^5$. 
        
        
        \settag{1.6.18}
        \item \tbf{Inclusion-Exclusion principle of dimensions.} Let $W_1$ and $W_2$ be finite dimensional 
        sub-spaces of a vector space $V$. Then,
        \begin{center}
            dim($W_1+W_2$) = dim($W_1$) $+$ dim($W_2$) $-$ dim($W_1\cap W_2$)
        \end{center}
        \tbf{Remark:} This theorem could not be generalized to higher dimensions as does the 
        Inclusion-Exclusion Principle in set theory. (Consequence of the challenge problem of Tutorial 2), 
        more information can be found \href{https://www.jstor.org/stable/24337937?seq=1#metadata_info_tab_contents}{\underline{here}}, on a paper of generalizing this formula. 
        
    \end{enumerate}
    
\chapter{Linear Transformations}
\section{Linear Transformations}

    \begin{enumerate}
        \settag{2.1.1}
        \item \tbf{Definition of linear transformation.\footnote{Familiar from MAT223}} A function $T:V\xrightarrow{}W$ is called a
         \tit{linear mapping} or a \tit{linear transformation} if it satisfies:
        \begin{enumerate}
            \item $\forall \mathbf{u},\mathbf{v}\in V,~T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{u})$
            \item $\forall \alpha \in \R, \mathbf{v} \in V,~ T(\alpha\mathbf{v}) = \alpha T(\mathbf{v})$
        \end{enumerate}
        
        \settag{2.1.2}
        \item \tbf{Proposition: alternative definition of L.T.s} A function $T:V\xrightarrow{}W$ is a linear transformation if and only if 
        \begin{equation*}
            \forall \alpha,\beta \in \R, \forall \mathbf{u}, \mathbf{v} \in V, T(\alpha \mathbf{u}+\beta \mathbf{v})=\alpha T(\mathbf{u}) + \beta T(\mathbf{v}).
        \end{equation*}
        \tit{\underline{Proof($\implies$):}}\newline
        Assuming that $T$ is a linear transformation, then the definition in (2.1.1) must satisfy. Let $\alpha,\beta \in \R, \mathbf{u},\mathbf{v}\in V$, then: 
        \begin{align*}
            T(\alpha\mathbf{u} + \beta\mathbf{v}) &= T(\alpha\mathbf{u}) + T(\beta\mathbf{u})~~~~\text{// by (a)} \\
            &= \alpha T(\mathbf{u}) + \beta T(\mathbf{v})~~~~\text{// by (b)}
        \end{align*}
        \tit{\underline{Proof($\impliedby$):}}\newline
        Assuming the alternative definition, we want to show the definition given in (2.1.1). Since the quantifier is 
        $\forall$, we can take $\alpha = \beta  =1, \text{(arbitrary)}~\mathbf{u},\mathbf{v}\in V$ in the alternative
         definition which directly yields us (a) of (2.1.1). Then we take $\alpha \in \R, \beta =0, \mathbf{u}, \mathbf{v}\in V$. In this case, we want to show that $T(\alpha \mathbf{u}) = \alpha T(\mathbf{u})$. We proceed as follows: 
        \begin{align*}
            T(\alpha\mathbf{u} + \beta\mathbf{v}) &= T(\alpha\mathbf{u} + 0\mathbf{v}) = T(\alpha\mathbf{u})\\
            &= \alpha T(\mathbf{u}) + 0T(\mathbf{u}) \\
            &= \alpha T(\mathbf{u})
        \end{align*}
        Since $\alpha\in \R, \mathbf{u}\in V$ are arbitrary, this completes the proof. \qed
        
        \settag{2.1.3}
        \item \tbf{Corollary.} A function $T:V\xrightarrow{}W$ is a linear transformation if and only if
        \begin{equation}
            \forall a_1,\ldots,a_k\in \R, \forall \mathbf{v}_1, \ldots,\mathbf{v}_k\in V:~ 
            T\left(\sum_{i=1}^{k}a_i\mathbf{v}_i\right) = \sum_{i=1}^{k}a_i T\left(\mathbf{v}_i\right)
        \end{equation}
        \proof To show this if and only if relationship, we have to consider the implication of both directions. Since $(2.1.2)$ is just a special case of $(2.1.3)$ then we are done in proving $(2.1.3)\implies (2.1.2)$. We will prove the other direction by mathematical induction (on $k$) to generalize the case of $k=2$ to arbitrary $k\in \double{N}^{\geq 2}$. \newline
        Define predicate $P(k): \double{N}^{\geq 2} \rightarrow{} \{0, 1\}~\text{as ``}(2.1)~\text{holds"}$. \newline
        Claim that $\forall k \in \double{N}^{\geq 2}, P(k)$.\newline
        \basecase $k = 2$. In this case the wanted equality is the same as the one proved in $(2.1.2)$, so $P(2)$. \newline
        \inductive Let $k\in \double{N}^{\geq 2}$, assume $P(k-1)$, we want to show that $P(k)$ follows.\newline
        \begin{align*}
            T\left(\sum_{i=1}^{k}a_i\mathbf{v}_i\right) &= T\left(a_k\mathbf{v}_k + \sum_{i=1}^{k-1}a_i\mathbf{v}_i\right) \\
            &= a_k T(\mathbf{v}_k) + T\left(\sum_{i=1}^{k-1}a_i\mathbf{v}_i\right)~~~~\text{// By $(2.1.1)$} \\
            &= a_k T(\mathbf{v}_k) + \sum_{i=1}^{k-1}a_i T\left(\mathbf{v}_i\right)~~~~~~\text{// By $P(k-1)$}\\
            &= \sum_{i=1}^{k}a_i T\left(\mathbf{v}_i\right)
        \end{align*}
        So $P(k)$ follows in this case.
        \qed
        
        \settag{2.1.9}
        \item \tbf{Angle between two vectors.} If $\vzero\neq \mathbf{a}\in \R^2$ and $\vzero\neq\mathbf{b}\in \R^2$, then the angle $\theta$ between them must be\footnote{The angle brackets here denotes the inner product of vectors}
        \begin{equation*}
            \theta = \arccos\left( \frac{\left<\mathbf{a},\mathbf{b}\right>}{\lVert\mathbf{a}\rVert \cdot \lVert\mathbf{b}\rVert}\right)
        \end{equation*}
        \tbf{Remark.} Notice that this definition could also be extended to $\R^3$.
        
        \settag{2.1.10}
        \item \tbf{Corollary on orthogonality of vectors.} If $\R^2\ni \mathbf{a}\neq \vzero$ and $\R^2\ni \mathbf{b}\neq \vzero$, then the angle $\theta$ between them is a right angle if and only if $\left<\mathbf{a},\mathbf{b}\right>=0$.\newline
        \tbf{Remark.} Note this definition of orthogonality could be extended to all euclidean vector spaces. The orthogonal complement of a subspace is the space of all vectors that are orthogonal to every vector in the subspace. In a three-dimensional Euclidean vector space, the orthogonal complement of a line through the origin is the plane through the origin perpendicular to it, and vice versa. In four-dimensional Euclidean space, for example, the orthogonal complement of a line is a hyper-plane and vice versa, and that of a plane is a plane.
        
        \settag{2.1.14}
        \item \tbf{Proposition.} If $T:V\xrightarrow{}W$ is a linear transformation and $V$ is finite-dimensional, then $T$ is uniquely determined by its values on the members of a basis of $V$. To make this proposition more clear, we present the proof below. We will show that if $S$ and $T$ are linear transformations that take the same values on each member of a basis for $V$, then in fact $S=T$.\newline
        \proof \newline
        Let $\{ \mathbf{v}_1, \ldots, \mathbf{v}_k\}$ be a basis for $V$, and let $S$ and $T$ be two linear transformations that satisfy $T(\mathbf{v}_1)=S(\mathbf{v}_1), \forall i \in \{1,\ldots,k\}$. If $\mathbf{v}\in V, ~\mathbf{v} = a_1\mathbf{v}_1 + \ldots + a_k\mathbf{k}$, then
        \begin{align*}
            T(\mathbf{v}) &= T(a_1\mathbf{v}_1 + \ldots + a_k\mathbf{k}) \\
            &= a_1T(\mathbf{v}_1) + \ldots + a_k T(\mathbf{v}_k)~~~~\text{// Since $T$ is linear}\\
            &= a_1S(\mathbf{v}_1) + \ldots + a_k S(\mathbf{v}_k) \\
            &= S(a_1\mathbf{v}_1 + \ldots + a_k\mathbf{k})~~~~~~~~~~\text{//  Since $S$ is linear} \\
            &=S(\mathbf{v})
        \end{align*}
        Hence $S$ and $T$ are equal as mappings from $V$ to $W$. \qed
        
    \end{enumerate}
    
\section{Linear Transformations between finite dimensional vector spaces}
    \begin{enumerate}
        \settag{2.2.1}
        \item \tbf{Proposition.} Let $T:V\xrightarrow{}W$ be a linear transformation between the finite dimensional vector spaces $V$ and $W$. If $\{\mathbf{v}_1,\ldots,\mathbf{v}_k\}$ is a basis for $V$ and $\{\mathbf{w}_1,\ldots,\mathbf{w}_l\}$ is a basis for $W$, then $T:V\xrightarrow{}W$ is uniquely determined by the $l\times k$ scalars used to express $T(\mathbf{v}_j)$, where $j\in \{1,\ldots,k\}$, in terms of $\mathbf{w}_1,\ldots,\mathbf{w}_l$.
        
        \settag{2.2.6}
        \item \tbf{Matrix of a transformation w.r.t.$\alpha,\beta$.} Let $T:V\xrightarrow{}W$ be a linear transformation
         between finite-dimensional vector spaces $V$ and $W$, and let $\alpha = \{\mathbf{v}_1,\ldots,\mathbf{v}_k \}$ 
         and $\beta = \{\mathbf{w}_1,\ldots,\mathbf{w}_l\}$, respectively, be any basis for $V$ and $W$. 
        Let $a_{ij}$, where $1\leq i \leq l$ and $1\leq j \leq k$ be the $l\cdot k$ scalars that determined $T$ 
        with respect to the bases $\alpha$ and $\beta$. The matrix whole entries are the scalars $a_{ij}$, given above,
         is called the \tit{matrix of the linear transformation $T$ with respect to the bases $\alpha$ for $V$ and 
         $\beta$ for $W$}. We denote such matrix with following notation: $\left[T\right]^\beta_\alpha$. \newline 
        \tbf{Notice that}, again, $\alpha$ is a basis for $V$, the domain of the transformation and $\beta$ is a
         basis for $W$, the co-domain of the transformation.
        \newline
        \tbf{Remark.} This should be familiar from MAT223, where we perform transformation on each and every element 
        in a basis for a space, and transcribe them, in terms of another basis, into a standard matrix for the transformation. 
        In terms of standardized formulaic calculations, we have
        \begin{equation*}
            \left[T\right]_\alpha^\beta =
            \begin{bmatrix}
                \left[T(\alpha_1) \right]_\beta & \ldots & \left[T(\alpha_n) \right]_\beta
            \end{bmatrix}
        \end{equation*}
        where the $\alpha_i$'s denotes the $i$-th element of the $\alpha$ basis, and is itself a vector.
        
        \settag{2.2.15}
        \item \tbf{Proposition: Linear Transformation on alternative basis.} Let $T:V\xrightarrow{} W$ 
        be a linear transformation between vector spaces $V$ of dimension $k$ and $W$ of dimension $l$. 
        Let $\alpha = \{\mathbf{v}_1,\ldots,\mathbf{v}_k\}$ be a basis for $V$, and let $\beta = \{\mathbf{w}_1,\ldots,\mathbf{w}_l\}$ be a basis for $W$. 
        Then for each $\mathbf{v} \in V$, we have the following:
        \begin{equation*}
            \left[T(\mathbf{v})\right]_\beta = \left[T\right]^\beta_\alpha\left[\mathbf{v}\right]_\alpha
        \end{equation*}
        One can think about this in the sense that the same transformation could have been accomplished under another basis.
         We just have to convert the original problem into some easy to solve basis, perform the transformation under that basis, 
         and then convert the result back. We now present the proof for this proposition.\newline
        \proof Let $\mathbf{v}=x_1\mathbf{v}_1 + \ldots + x_k\mathbf{v}_k\in V$. Then if $T(\mathbf{v}_j)=a_{1j}\mathbf{w}_1 + \ldots + a_{lj}\mathbf{w}_l$, 
        \begin{align*}
            T(\mathbf{v}) &= \sum_{j=1}^k x_j T(\mathbf{v}_j) \\
            &= \sum_{j=1}^k x_j \left(\sum_{i=1}^{l}a_{ij}\mathbf{w}_i\right) \\
            &= \sum_{i=1}^l\left(\sum_{j=1}^k x_j a_{ij} \right)\mathbf{w}_i
        \end{align*}
        Thus the $i$-th coefficient of $T(\mathbf{v})$ in terms of $\beta$ is $\sum_{j=1}^k x_j a_{ij}$, and
        \begin{equation*}
            \left[T(\mathbf{v})\right]_\beta = 
            \begin{bmatrix}\sum_{j=1}^kx_ja_{1j} 
            \\ \dots 
            \\ \dots 
            \\ \dots 
            \\ \sum_{j=1}^kx_ja_{lj}
            \end{bmatrix}
            =
            \begin{bmatrix}
            a_{11} & \cdots & a_{1k} \\
            \vdots & \ddots & \vdots \\
            a_{l1} & \cdots & a_{lk}
            \end{bmatrix}
            \begin{bmatrix}
            a_1 \\ \vdots \\ a_k
            \end{bmatrix}
            =\left[T\right]_\alpha^\beta\left[\vv\right]_\alpha
        \end{equation*}
        \qed
        
        \settag{2.2.18}
        \item \tbf{Proposition: Property of matrix \tit{times} L.C. of vectors.} Note that in this proposition, we assume the vectors and the matrix are compatible\footnote{i.e., They can \tit{always} perform the operations that we want}. Let $A$ be an $l\times k$ matrix and $\mathbf{u}$ and $\mathbf{v}$ be column vectors with $k$ entries. Then,
        \begin{equation*}
            \forall~ \text{pairs of}~a\in\R, b\in\R, A(a\mathbf{u} + b\mathbf{v}) = aA\mathbf{u} + bA\vv
        \end{equation*}
        \proof Since $\mathbf{u}, \mathbf{v}\in \R^k$, let them be $\mathbf{u} = (u_1,\ldots,u_k)^T$ and $\mathbf{v} = (v_1,\ldots,v_k)^T$.\newline 
        Fix $a,b\in \R$. Then,
        \begin{align*}
            A\left(a\mathbf{u} + b\mathbf{v}\right) &=  A\left(a\left(u_1,\ldots,u_k\right)^T + b\left(v_1,\ldots,v_k\right)^T\right) \\
            &=  A\left(\left(a\cdot u_1,\ldots,a\cdot u_k\right)^T + \left(b \cdot v_1,\ldots,b\cdot v_k\right)^T\right) \\
            &=  A\left(a\cdot u_1 + b \cdot v_1,\ldots,a\cdot u_k + b\cdot v_k \right)^T \\
            &\text{\%TODO: change this vector notation and finish the proof..}
        \end{align*}
        
        \settag{2.2.19}
        \item \tbf{Proposition.} Let $\alpha = \{\vv_1,\ldots,\vv_k\}$ be a basis for $V$ and $\beta= \{\vw_1,\ldots,\vw_l\}$ be a basis for $W$, and let $\mathbf{v}=x_1\vv_1+\ldots+x_k\vv_k\in V$.
        \begin{enumerate}
            \item If $A$ is an $l \times k$ matrix, then the function $T(\mathbf{v}) = \mathbf{w}$, where $\left[\vw\right]_\beta A\left[\vv\right]_\alpha$ is a linear transformation.
            \item If $A = [S]_\alpha^\beta$ is the matrix of a transformation $S:V\xrightarrow{} W$, then the transformation $T$ constructed from $[S]_\alpha^\beta$ is equal to $S$.
            \item If $T$ is the transformation of (a) constructed from $A$, then $[T]_\alpha^\beta = A$.
        \end{enumerate}
        
        \settag{2.2.20}
        \item \tbf{Proposition.} Let $V$ and $W$ be finite-dimensional vector spaces. Let $\alpha$ be a basis for $V$ and $\beta$ a basis for $W$. Then the assignment of a matrix to a linear transformation from $V$ to $W$ given by $T$ goes to $[T]_\alpha^\beta$ is bijective\footnote{Injective and surjective}.
    \end{enumerate}
    
\section{Kernel and image}
    \begin{enumerate}
        \settag{2.3.1}
        \item \tbf{Definition of Kernel.} The \tit{kernel} of $T$, denoted Ker($T$), is the subset of $V$ consisting of all vectors $\vv\in V$ such that $T(\vv) = \vzero$. Writing in familiar set builder notation:
        \begin{equation*}
            \text{Ker}(T) := \{\vv \in V | T(\vv) = \vzero\}
        \end{equation*}
        One should notice the difference between the familiar Null Space of a transformation and the Kernel here. Kernel is defined for all vector spaces, however, null-spaces are for $\R^n$ only.
        
        \settag{2.3.2}
        \item \tbf{Proposition: Kernel is a subspace.} Let $T: V\xrightarrow{} W$ be a linear transformation. Ker($T$) is a subspace of $V$. \newline
        \proof \newline
        Since Ker($T$) $\subset V$, it suffices to show that Ker($T$) is closed under addition and scalar multiplication. Since $T$ is linear, $\forall \vu, \vv \in \text{Ker}(T)~\text{and}~a\in \R$, we have
        \begin{equation*}
            T(\vu + a\vv) = T(\vu) + aT(\vv) = \vzero + a\vzero \implies \vu + a\vv \in \text{Ker}(T)
        \end{equation*}
        \qed
        
        \settag{2.3.7}
        \para{Proposition.} Let $T:V\rightarrow{} W$ be a linear transformation of finite-dimensional vector spaces, and let $\alpha, \beta$ be bases for $V,W$ respectively. Then $\bx \in \text{Ker}(T)$ if and only if the coordinate vector of $\bx$, $[\bx]_\alpha$, satisfies the system of equations
        \begin{center}
            $\begin{cases}
            a_{11}x_1 + \ldots + a_{1k}x_k &= 0 \\
            &\vdots \\
            a_{l1}x_1 + \ldots + a_{lk}x_k &= 0
            \end{cases}$
        \end{center}
        where the coefficient $a_{ij}$ are the entries of the matrix $[T]_\alpha^\beta$
        % \begin{center}
        %     $
        %     \begin{cases}
        %     b_{11}a_1 + \ldots + b_{1n}a_n &= 0 \\
        %     b_{21}a_1 + \ldots + b_{2n}a_n &= 0 \\
        %     &\vdots \\
        %     b_{m1}a_1 + \ldots + b_{mn}a_n &= 0
        %     \end{cases}
        %     $
        % \end{center}
        
        \settag{2.3.8}
        \item \tbf{Independence is basis-independent.} If $\alpha = \{v_1,\ldots,v_k\}$ is a basis for $V$, then $\vx_1,\ldots,\vx_m\in V$ are independent if and only if $[\bx_1]_\alpha,\ldots,[\bx_m]_\alpha$ are independent.
        
        \settag{2.3.10}
        \para{Definition of Image.} The subset of $W$ constsing of vectors $\vw\in W$ for which there exists a $\vv\in V$ such that $T(\vv) = \vw$ is called the \tit{image} of $T$ and is denoted by Im$(T)$. In set builder notation we have
        \begin{equation*}
            \text{Im}(T) := \{\vw\in W|T(\vv) = \vw~\text{for some}~\vv \in V\}~~\text{where}~T:V\rightarrow{} W
        \end{equation*}
        
        \settag{2.3.11}
        \para{Proposition: Image is subspace.} Let $T:V\rightarrow{} W$ be a linear transformation. The image of $T$ is a subspace of $W$, the co-domain.\newline
        \proof\newline
        Let, $\vw_1,\vw_2\in\text{Im}(T)$, and let $a\in \R$. Since $\vw_1$ and $\vw_2\in V$ with $T(\vv_1) = \vw_1$ and $T(\vv_2) = \vw_2$. Then we have
        \begin{align*}
            a\vw_1 + \vw_2 
            &= aT(\vv_1) + T(\vv_2) \\
            &= T(a\vv_1 + \vv_2) 
            \implies a\vw_1 +\vw_2 \in \text{Im}(T)\text{ by linearity}
        \end{align*}
        Hence, by the ``quick check rule", we know that image is a subspace of the co-domain. \qed
        
        \settag{2.3.12}
        \para{Proposition.} If $\{\vv_1,\ldots,\vv_m\}$ is ant set that spans $V$ (in particular, it could be a basis of $V$), then $\{T(\vv_1),\ldots,T(\vv_m)\}$ spans\footnote{By ``spans" we mean equal to each other} Im$(T)$. \newline
        \underline{\tit{Proof($\supseteq$):}} \newline
        Let $\vw\in \text{Im}(T)$, then $\exists \vv\in V$ with $T(\vv) = \vw$. Since $Span\{\vv_1,\ldots,\vv_m\} = V$, then $\exists a_1,\ldots,a_m$ s.t. $a_1\vv_1 + \ldots + a_m\vv_m = \vv$. Then, 
        \begin{align*}
            \vw &= T(\vv) \\
            &= T\left(\sum_{i=1}^m a_i\vv_i \right) \\
            &= \sum_{i=1}^m a_iT(\vv_i) ~~~~~~\mbox{// by linearity}
        \end{align*}
        Therefore, Im($T$) is contained in $Span\{T(\vv_1),\ldots,T(\vv_m)\}$. \newline
        \proofsubset \newline
        Let $\vw\in Span\{T(\vv_1,\ldots,\vv_m)\}$, then (reversing what we did previously) we have
        \begin{align*}
            \vw &= \sum_{i=1}^m a_iT(\vv_i) \\
            &= T\left(\sum_{i=1}^m a_i\vv_i \right) ~~~~~~\mbox{// by linearity}\\
            &= T(\vv)\in \text{Im}(T)
        \end{align*}
        Hence mutual inclusion yields us the wanted result, and this completes the proof. \qed
        
        \settag{2.3.13}
        \para{Corollary.} If $\alpha = \{\vv_1,\ldots,\vv_k\}$ is a basis for $V$, and $\beta = \{\vw_1,\ldots,\vw_l\}$ is a basis for $W$, then the vectors in $W$ whose coordinate vectors (in terms of $\beta$) are the column of $[T]_\alpha^\beta$ span Im($T$).
        
        \settag{2.3.17}
        \para{Rank-Nullity Theorem.\footnote{Known as The Dimension Theorem in book}} If $V$ is a finite-dimensional vector space and $T:V\rightarrow{}W$ is a linear transformation, then 
        \begin{equation*}
            \text{dim(Ker($T$))} + \text{dim(Im($T$))} = \text{dim}(V)
        \end{equation*}
        Or equivalently\footnote{also, dim(Im($T$)) = dim(Rol($T$)) = dim(Col($T$)) = \#pivot in r.r.e.f},
        \begin{equation*}
            \text{dim(Ker($T$))} + \text{Rank($T$)} = \text{dim}(V)
        \end{equation*}
    \end{enumerate}
    
\section{Applications of Rank-Nullity Theorem}
    \begin{enumerate}
        \settag{2.4.2}
        \para{Proposition.} A linear transformation $T:V\rightarrow{} W$ is injective if and only of $\text{dim(Ker}(T\text{))}=0$. Informally speaking, we can think of this as ``No information is lost during the linear transformation". \newline
        \proofforward \newline
            If $T$ is injective, then by definition, there $\exists!\vv \in V$ with $T(\vv) = \vzero$. Since we know that $T(\vzero) = 0, \forall~\text{linear mappings}$, the zero vector is the unique vector $\vv$ satisfying $T(\vv) = \vzero$. Thus the kernel of $T$ consists of only the zero vector. Therefore, dim(Ker($T$)) = 0. \newline
        \proofback \newline
            Conversely, assume that dim(Ker($T$)) = 0. Let $\vv_1, \vv_2 \in V$ with $T(\vv_1) = T(\vv_2)$. We want to show that $\vv_1 = \vv_2$. Since $T(\vv_1) = T(\vv_2)$, $T(\vv_1 - \vv_2) = \vzero$, so $\vv_1 - \vv_2 \in \text{Ker}(T)$. But if dim(Ker($T$)) = 0, it follows that Ker($T$) = $\{\vzero\}$. It follows that $\vv_1 - \vv_2 = \vzero \implies \vv_1 = \vv_2$ as wanted. Thus, $T$ is injective. \qed \newline
        \textbf{Remark.} This proposition is so important that we summarize it as follows, again. Let \trans{T}{V}{W}, with $\dim{V} = \dim{W}$, then
        \begin{align*}
            &~~~~~~~~~ \text{$T$ is injective} \\
            &\iff \text{$T$ is surjective} \\
            &\iff \text{$\ker(T)$ is trivial} \\
            &\iff \text{Im($T$) $= W$}
        \end{align*}
        
        \settag{2.4.3}
        \para{Corollary.} A linear mapping $T:V\rightarrow{} W$ on a finite-dimensional vector space $V$ is injective if and only if $\text{dim(Im(}T\text{)) = dim(}V\text{)}$
        
        \settag{2.4.4}
        \para{Corollary.} If dim($W$) $<$ dim($V$) and \trans{T}{V}{W} is a linear mapping, then $T$ is not injective.
        
        \settag{2.4.5}
        \para{Corollary.} If $V$ and $W$ are finite dimensional, then a linear mapping \trans{T}{V}{W} can be injective only if $\dime{W} \geq \dime{V}$
        
        \settag{2.4.7}
        \para{Proposition.} If $W$ is finite-dimensional, then a linear mapping \trans{T}{V}{W} is surjective if and only if dim(Im($T$))$ =\dime{W}$. \newline
        \textbf{Remark.} Since dim(Im($T$)) $\leq$ dim($V$) by the theorem, if dim($V$) $<$ dim($W$), then we have dim(Im($T$)) $<$ dim($W$), hence $T$ is not surjective.
        
        \settag{2.4.8}
        \para{Corollary.} If $V$ and $W$ are finite-dimensional, with $\dime{V} < \dime(W)$, then there is no surjective linear mapping \trans{T}{V}{W}.
        
        \settag{2.4.9}
        \para{Corollary.} A linear mapping \trans{T}{V}{W} can be surjective only if $\dime{V} \geq \dime{W}$.
        
        \settag{2.4.10}
        \para{Proposition.} Let $\dime{V} = \dime{W}$. A linear transformation \trans{T}{V}{W} is injective if and only if it is surjective. \newline
        \proofforward \newline
        If $T$ is injective, then dim(Ker($T$)) = 0 by proposition (2.4.2). By Theorem (2.3.17), we have dim(Im($T$)) = dim($V$). Therefore, by proposition (2.4.7), $T$ is injective \newline
        \proofback \newline
        If $T$ is surjective, then by Proposition (2.4.7), dim(Im($T$)) = dim($W$) = dim($V$). Therefore, by Theorem (2.3.17), dim(Ker($T$)) = 0. Hence, by proposition (2.4.2), $T$ is injective. \qed
        
        \settag{2.4.11}
        \para{Proposition.} Let \trans{T}{V}{W} be a linear transformation, and let $\vw \in \text{Im}(T)$. Let $\vv_1$ be any fixed vector with $T(\vv_1) = \vw$. Then every vector $\vv_2\in T^{-1}\left(\{\vw\}\right)$ can be written uniquely as $\vv_2 = \vv_1 + \vu$, where $\vu \in \text{ker}(T)$
        
        \settag{2.4.15}
        \para{Corollary.} Let \trans{T}{V}{W} be a linear transformation of finite-dimensional vector spaces, and let $\vw\in W$. Then $\exists! \vv \in V~s.t.~T(\vv) = \vw$ if and only if
        \begin{enumerate}
            \item $\vw \in \text{Im}(T)$, and
            \item dim(Ker($T$)) = $0$
        \end{enumerate}
        
        \settag{2.4.16}
        \para{Proposition.}
        \begin{enumerate}
            \item The set of solutions of the system of linear equations $A\vx = \vb$ is the subset $T^{-1}\left(\{\vb\}\right)$ of $V = \R^n$
            \item The set of solutions of the system of linear equations $A\vx = \vb$ is a subspace of $V$ if and only if the system is homogeneous, in which case the set of solutions is Ker($T$).
        \end{enumerate}
        
        \settag{2.4.17}
        \para{Corollary.} 
        \begin{enumerate}
            \item The number of free variables in the homogeneous system $A\vx = \vzero$ (or its echelon form equivalent) is equal to dim(Ker($T$)).
            \item The number of basic variables of the system is equal to dim(Im($T$)).
        \end{enumerate}
        Now, if the system is \tit{not homogeneous}, then from Proposition (2.4.16) we see that $A\vx = \vb$ has a solution if and only if $\vb \in \text{Im}(T)$. Let us assume then that $\vb \in \text{Im}(T)$, then this yields us the following terminology:
        
        \settag{2.4.18}
        \para{Definition of particular solution of a SLE (in-homo).} Given an in-homogeneous system of equations, $A\vx = \vb$, any single vector $\vx$ satisfying the system (necessarily $\vx \neq \vzero$) is called a \tit{particular solution} of the system of equations.
        
        \settag{2.4.19}
        \para{Proposition.} Let $\vx_p$ be a particular solution of the system $A\vx = \vb$. Then every other solution to $A\vx = \vb$ is of the form $\vx = \vx_p + \vx_h$, where $\vx_h$ is a solution of the corresponding homogeneous system of equations $A\vx = \vzero$. Furthermore, given $\vx_p$ and $\vx$, there is a unique $\vx_h$ such that $\vx = \vx_p + \vx_h$.
        
        \settag{2.4.20}
        \para{Corollary.} The syetem $A\vx = \vb$ has a unique solution if and only if $\vb \in \text{Im}(T)$ and the only solution to $A\vx = \vzero$ is the zero vector.
    \end{enumerate}
    
\section{Composition of Linear Transformations}
\begin{enumerate}
    \settag{2.5.1}
    \para{Proposition.} If \trans{S}{U}{V} and \trans{T}{V}{W} are linear transformations, then so is $TS$. \newline
    \proof \newline
    Let $\alpha ,\beta \in \R$ and let $\vu_1,\vu_2 \in U$. We must show that $TS$ satisfies Proposition (2.1.2).
    \begin{align*}
        TS(\alpha \vu_1 + \beta \vu_2) &= T(S(\alpha \vu_1 + \beta \vu_2)) ~~~~~~~~~~~~~\text{// by definition of $TS$}\\
        &= T(\alpha S(\vu_1) + \beta S(\vu_2))~~~~~~~~~\text{// by linearity of $S$}  \\
        &= \alpha T(S(\vu_1)) + \beta T(S(\vu_2))~~~~\text{// by linearity of $T$} \\
        &= \alpha TS(\vu_1) + \beta TS(\vu_2)
    \end{align*}
    \qed
    
    \settag{2.5.4}
    \para{Propositions.} 
    \begin{enumerate}
        \item \textbf{Associativity.} Let \trans{R}{U}{V}, and \trans{T}{W}{X} be linear transformation of the vectors spaces $W,V,W$, and $X$ as indicated, then 
        \begin{equation*}
            T(SR) = (TS)R
        \end{equation*}
        \item \textbf{Distributivity I.} Let \trans{R}{U}{V}, \trans{S}{U}{V} and \trans{T}{V}{W} be linear transformations of the vectors spaces $U,V$ and $W$ as indicated, then
        \begin{equation*}
            T(R+S) = TR+ TS
        \end{equation*}
        \item \textbf{Distributivity II.} Let \trans{R}{U}{V}, \trans{S}{V}{W} and \trans{T}{V}{W} be linear transformations of the vectors spaces $U, W$ and $W$ as indicated, then
        \begin{equation*}
            (T+S)R = TR + SR
        \end{equation*}
    \end{enumerate}
    
    \settag{2.5.6}
    \para{Proposition.} Let \trans{S}{U}{V} and \trans{T}{V}{W} be linear transformation, then
    \begin{enumerate}
        \item $\text{Ker}(S)\subset \text{Ker}(TS)$
        \item $\text{Im}(TS)\subset \text{Im}(T)$
    \end{enumerate}
    \proof \newline
    We will prove (a) here. If $\vu \in \text{Ker}(S)$, $S(\vu) = \vzero$. Then 
    \begin{equation*}
        TS(\vu) = T(\vzero) = \vzero
    \end{equation*}
    where we notice that the first and last term tells us $\vu \in \text{Ker}(TS)$ and this completes the proof. \qed
    
    \settag{2.5.7}
    \para{Corollary.} Let \trans{S}{U}{V} and \trans{T}{V}{W} be linear transformations of fin-dim vector spaces, then
    \begin{enumerate}
        \item dim(Ker($S$)) $\leq$ dim(Ker($TS$))
        \item dim(Im($TS$)) $\leq$ dim(Im($T$))
    \end{enumerate}
    \tbf{Remark.} In naive words, the statement (a) is saying that no transformation can bring what has been smashed to zero back, and thus the composition result of two linear transformations must have a larger kernel. The statement (b) is saying that each time we perform a linear transformation, the ``Image Range" of the final output would be restricted, and thus a composition would only result in a tighter restriction.
    
    \settag{2.5.9}
    \para{Proposition: Compatible Matrix Product.} If $\left[S\right]_\alpha^\beta$ has entries $a_{ij}$, where $i\in \{1, \ldots, n\}$ and $j\in \{1, \ldots, m\}$ and $\left[T\right]_\beta^\gamma$ has entries $b_{kl}$, where $k\in \{1, \ldots, p\}$ and $l\in \{1, \ldots, n\}$, then the entries of $\left[TS\right]_\alpha^\gamma$ are 
    \begin{equation*}
        \sum_{l=1}^nb_{kl}a_{lj}
    \end{equation*}
    
    \settag{2.5.13}
    \para{Proposition: Composition of Transformations Induced By Matrices.} Let \trans{S}{U}{V} and \trans{T}{V}{W} be linear transformation between fin-dim vector spaces. Let $\alpha, \beta, \gamma$ be bases for $U, V$ and $W$, respectively. Then
    \begin{equation*}
        \left[TS\right]_\alpha^\gamma = \left[T\right]_\beta^\gamma \left[S\right]_\alpha^\beta
    \end{equation*}
    One can comprehend this as: the standard matrix of a transformation that is a composition of two transformations induced by matrices is the matrix product of those matrices. Also note that matrix product \tit{does not} commute.
    
    \settag{2.5.14}
    \para{Propositions.}
    \begin{enumerate}
        \item \tbf{Associativity.} Let $A\in M_{m\times n}(\R), B\in M_{n\times p}(\R), C\in M_{p\times r}(\R)$, then
        \begin{equation*}
            (AB)C = A(BC)
        \end{equation*}
        
        \item \tbf{Distributivity I.} Let $A\in M_{m\times n}(\R)$ and $B, C\in M_{n\times p}(\R)$, then
        \begin{equation*}
            A(B+C) = AB + AC
        \end{equation*}
        
        \item \tbf{Distributivity II.} Let $A, B\in \mat{m}{n}, C\in \mat{n}{p}$, then
        \begin{equation*}
            (A+B)C = AC + BC
        \end{equation*}
    \end{enumerate}
\end{enumerate}

\section{The Inverse of A Linear Transformation}
\begin{enumerate}
    \settag{2.6.1}
    \para{Proposition.} Let \trans{T}{V}{W} be bijective, then the inverse function \trans{S}{W}{V} is a linear transformation.
    
    \settag{2.6.2}
    \para{Proposition.} A linear transformation \trans{T}{V}{W} has an inverse linear transformation $S$ \tit{if and only if} $T$ is injective and surjective.
    
    \settag{2.6.3}
    \para{Definition of Inverse and Invertible.} If \trans{T}{V}{W} is a linear transformation that has an inverse transformation \trans{S}{W}{V}, we say that $T$ is \tit{invertible}, and we denote the inverse of $T$ here $T^{-1} := S$ 
    
    \settag{2.6.4}
    \para{Definition of isomorphism.} If \trans{T}{V}{W} is an invertible linear transformation, $T$ is called an \tit{isomorphism}, and we say $V$ and $W$ are \tit{isomorphic} vector spaces.
    
    \settag{2.6.5}
    \para{Remark on the $T^{-1}$ notation.} Please note that the inverse here that is acting on a vector is denotes the inverse transformation. For example we have \trans{T}{V}{W}, then $w\in W, T^{-1}(w)\in V$ denotes the inverse transformation. We \textit{must} differentiate this from the other notation we encountered earlier which acts on a set rather than on a vector. As an example, we have \trans{T}{V}{W} and $\vzero\in W$, then
    \begin{equation*}
        T^{-1}\left(\{\vzero\}\right) \equiv \{\vv\in V, T(\vv) = \vzero\}
    \end{equation*}
    Notice that this special example is a.k.a the Kernel of the transformation by definition.
    
    \settag{2.6.7}
    \para{Proposition.} If $V$ and $W$ are finite-dimensional vector spaces, then there is an isomorphism \trans{T}{V}{W} \tit{if and only if} dim($V$) $=$ dim($W$). \newline
    \proofforward \newline
    If $T$ is an isomorphism, then we know that $T$ is bijective. So,
    dim(Ker($T$)) = 0 and dim(Im($T$)) = dim($W$). Then by the Rank-Nullity Theorem, we conclude that dim($V$) = dim($W$).\newline
    \proofback \newline
    If dim($V$) = dim($W$), we must produce an isomorphism \trans{T}{V}{W}. Let $\alpha = \{\vv_1, \ldots, \vv_n\}$ be a basis for $B$ and $\beta = \{\vw_1,\ldots,\vw_n\}$ be a basis for $W$. Define $T$ to be the linear transformation from $V$ to $W$ with $T(\vv_i) = w_i, i = 1,\ldots,n$. By Proposition (2.1.14), $T$ is uniquely determined by tis choice of values on $\alpha$. To see that $T$ is injective, notice that if
    \begin{equation*}
        T(a_1\vv_1 + \ldots + a_n\vv_n) = \vzero
    \end{equation*}
    then we have
    \begin{equation*}
        a_1\vw_1 + \ldots + a_n\vw_n = \vzero
    \end{equation*}
    Since the $\vw$'s are a basis, we know immediately that $a_1 = \ldots = a_n = 0$. Then Ker($T$) = $\{\vzero\}$ and $T$ is injective. By proposition (2.4.10) $T$ is also surjective, and then by Proposition (2.6.2) it is an isomorphism.
    
    \settag{2.6.9}
    \para{The Gauss-Jordan Method of Inverse\footnote{Extended based on this example}.} Suppose we have the following matrix:
    \begin{equation*}
    A = \left[T\right]_\alpha^\beta = 
        \begin{bmatrix}
            a_{11} & \hdots & a_{1n} \\
            \vdots & \ddots & \vdots \\
            a_{n1} & \ldots & a_{nn}
        \end{bmatrix}
    \end{equation*}
    and we want to find the matrix inverse $B = \left[T^{-1}\right]_\beta^\alpha$. To achieve this, we augment this matrix with the compatible identity map $I_{n\times n}$.
    \begin{equation}
        \begin{bmatrix}
            a_{11} & \hdots & a_{1n} &\aug & 1 & \ldots & 0 \\
            \vdots & \ddots & \vdots &\aug & \vdots & \ddots & \vdots \\
            a_{n1} & \ldots & a_{nn} &\aug & 0 & \ldots & 1
        \end{bmatrix} 
        = \left[A|I\right]
    \end{equation}
    We row reduce (2.2) \textit{w.r.t} the left half matrix with a result of the following form:
    \begin{equation*}
        \begin{bmatrix}
            1 & \ldots & 0 &\aug & b_{11} & \hdots & b_{1n}  \\
            \vdots & \ddots & \vdots &\aug & \vdots & \ddots & \vdots \\
            0 & \ldots & 1 &\aug & b_{n_1} & \ldots & b_{nn}
        \end{bmatrix} 
        = \left[I|B\right] = \left[I|A^{-1}\right]
    \end{equation*}
    We then record the $n\times n$ coefficients ($b_{ij}$) on the right hand side which is our end result of the inverse.
    
    \settag{2.6.10}
    \para{Definition of invertible matrices.} An $n \times n $ matrix is called invertible if there exists an $n\times n$ matrix $B$ so that $AB = BA = I$. Where the $I$ denotes the compatible identity map and $B$ is called an inverse of $A$ and $A^{-1} := B$
    
    \settag{2.6.11}
    \para{Proposition.} Let \trans{T}{V}{W} be an isomorphism of finite-dimensional vector spaces. Then for any choice of bases $\alpha$ for $V$ and $\beta$ for $W$, we have
    \begin{equation*}
        \left[T^{-1}\right]_\beta^\alpha = \left[\left[T\right]^\beta_\alpha\right]^{-1}
    \end{equation*}
    \tit{\underline{Proof(from the book):}} \newline
    To show the above identity, it suffices to show that $\left[T^{-1}\right]_\beta^\alpha$ is the matrix inverse of $\left[T\right]_\alpha^\beta$. On one hand we have
    \begin{equation*}
        \left[T^{-1}\right]_\beta^\alpha \left[T\right]^\beta_\alpha = \left[T^{-1}T\right]_\alpha^\alpha = \left[I_{n \times n}\right]_\alpha^\alpha
    \end{equation*}
    On the other hand, we have
    \begin{equation*}
        \left[T\right]^\beta_\alpha \left[T^{-1}\right]_\beta^\alpha = \left[TT^{-1}\right]_\beta^\beta = \left[I_{n \times n}\right]_\beta^\beta
    \end{equation*}
    Then, since the $n\times n$ identity map is unique, $\left[I_{n \times n}\right]_\beta^\beta = \left[I_{n \times n}\right]_\alpha^\alpha$, we can equate the above two equations, which, by definition tells us that $\left[T^{-1}\right]_\beta^\alpha$ is the matrix inverse of $\left[T\right]_\alpha^\beta$. We conclude that the above identity is true.\qed \newline
    \tit{\underline{Proof(from class):}} \newline
    Starting from the identity matrix in the $\alpha$ basis, we have
    \begin{equation}
        \map{I}{\alpha}{\alpha} = \map{T^{-1}T}{\alpha}{\alpha} = \map{T^{-1}}{\alpha}{\beta} \map{T}{\beta}{\alpha} 
    \end{equation}
    from where we equate the first and last term of the above line, and notice that by definition \map{T}{\beta}{\alpha} is a inverse of \map{T^{-1}}{\alpha}{\beta}, so $\left[\map{T}{\beta}{\alpha} \right]^{-1} = \map{T^{-1}}{\alpha}{\beta}$ and this completes the proof. \qed
\end{enumerate}

\section{Change of Basis}
\begin{enumerate}
    \settag{2.7.2}
    \para{Remarks on the notation.} Note that the here the $[I]_\alpha^{\alpha'}$ is \textit{not} the identity map since $\alpha \neq \alpha'$. This notation will come up in the following chapter naturally but it is important to differentiate this notation with the identity map.
    
    \settag{2.7.*}
    \para{Remark on the identity map.} Note that the inverse of a identity map is itself, that is
    \begin{equation*}
        \left[\map{I}{\alpha}{\beta}\right]^{-1} \equiv \map{I}{\alpha}{\beta}, \forall~\text{$\alpha,\beta$ bases $\forall~\text{vector spaces}~V$}
    \end{equation*}
    
    \settag{2.7.3}
    \para{Proposition.} Let $V$ be a finite-dimensional vector space, and let $\alpha, \alpha'$ be bases for $V$. Let $\vv \in V$. Then the coordinate vector $[\vv]_{\alpha'}$ of $\vv$ in the basis $\alpha'$ is related to the coordinate vector $[\vv]_\alpha$ of $\vv$ in the basis $\alpha$ by
    \begin{equation*}
        [I]_\alpha^{\alpha'}[\vv]_\alpha = [\vv]_{\alpha'}
    \end{equation*}
    
    \settag{2.7.5}
    \para{Theorem.} Let \trans{T}{V}{W} be a linear transformation Between FDVS $V$ and $W$. Let \trans{I_V}{V}{V} and \trans{I_W}{W}{W} be respective identity transformations of $V$ and $W$. Let $\alpha, \alpha'$ be two bases for $V$ and let $\beta, \beta'$ be two bases for $W$, then
    \begin{equation*}
        \map{T}{\alpha'}{\beta'} = \map{I_W}{\beta}{\beta'}\cdot \map{T}{\alpha}{\beta}\cdot \map{I_V}{\alpha'}{\alpha}
    \end{equation*}
    \textbf{Remark.} In naive words, in order to produce a transformation from bases $\alpha'$ to $\beta'$ we first make the change of basis from $\alpha'$ to $\alpha$, then we perform the transformation from $\alpha$ basis to $\beta$ basis and lastly we change the basis to the wanted $\beta'$ through a ``cross basis identity map". \newline
    We now present the proof, using the change of basis facts: \newline
    \proof \newline
    Since $T = I_WTI_V$, we can write
    \begin{align*}
        \map{T}{\alpha'}{\beta'} &= \map{I_WTI_V}{\alpha'}{\beta'} \\
        &= \map{I_WT}{\alpha}{\beta'}\map{I_V}{\alpha'}{\alpha} \\
        &= \map{I_W}{\beta}{\beta'}\map{T}{\alpha}{\beta} \map{I_V}{\alpha'}{\alpha}
    \end{align*}
    \qed
    
    \settag{2.7.6}
    \para{Definition of Similar Matrices.} Let $A, B$ be $n\times n$ matrices. $A$ and $B$ are said to be \textit{similar} if there exists an invertible matrix $Q$ such that:
    \begin{equation*}
        B = Q^{-1}AQ
    \end{equation*}
    Applying this new definition, we can see the follows: Let \trans{T}{V}{V} is a linear transformation and $\alpha, \alpha'$ are two bases for $V$, then $A = \map{T}{\alpha}{\alpha}$ is similar to $B = \map{T}{\alpha'}{\alpha'}$, and the invertible matrix $Q$ in the definition is the matrix $Q = \map{I_V}{\alpha'}{\alpha}$
\end{enumerate}

\chapter{The Determinant Function}
\section{The Determinant as Area}
\begin{enumerate}
    \settag{3.1.1}
    \para{Propositions: Geometry of a parallelogram in $\R^2$} 
    \begin{enumerate}
        \item The area of the parallelogram with vertices $\vzero, \va_1, \va_2$, and $\va_1 + \va_2$ is $\pm\left(\va_{11}\va_{22} - \va_{12}\va_{21}\right)$
        \item The area is not zero if and only if the vectors $\va_1$ and $\va_2$ are linearly independent.
    \end{enumerate}
    
    \settag{3.1.2}
    \para{Corollary.} Let $V=\R^2$, \trans{T}{V}{V} is an isomorphism if and only if the area of the parallelogram constructed previously is non-zero
    
    \settag{3.1.3}
    \para{Proposition.} The function $Area(\va_1, \va_2)$ has the following properties for $\va_1,\va_2, \va_1'$, and $\va_2'\in \R^2$
    \begin{enumerate}
        \item $Area(b\va_1 + c\va_1', \va_2) = b~Area(\va_1,\va_2) + c~Area(\va_1', \va_2)~\text{for $b, c \in \R$}$
        \item \footnote{This essentially the same as (a)}$Area(\va_1, b\va_2 + c\va_2') = b~Area(\va_1,\va_2) + c~Area(\va_1, \va_2')~\text{for $b, c \in \R$}$
        \item $Area(\va_1, \va_2) = -Area(\va_2, \va_1)$, and
        \item $Area((1, 0), (0, 1)) = 1$.
    \end{enumerate}
    \textbf{Remark.} Recall that (d) is the \textit{n-cube} in $\R^2$, where n-cube is defined as follows: (with $\ve_i$'s as components of $\xi$ basis for $\R^n$ Euclidean Space)
    \begin{equation*}
        C_n := \left\{\vx \in \R^n| \vx = \sum_{i = 1}^n \alpha_i \ve_i~\text{for some $\alpha_i\in \left[0, 1\right]$}\right\} \equiv \left[0, 1\right]^n
    \end{equation*}
    
    \settag{3.1.4}
    \para{Proposition.} If $B(\va_1, \va_2)$ is any real-valued function of $\va_1, \va_2\in \R^2$ that satisfies Properties (a), (c) and (d) of Proposition (3.1.3), then $B$ is equal to the area function.
    
    \settag{3.1.5}
    \para{Definition of \textit{determinant} of a $2 \times 2$ matrix.}
    \begin{enumerate}
        \item $\det(b\va_1 + c\va_1', \va_2) = b\det(\va_1, \va_2) + c\det(\va_1', \va_2)$ for $b, c\in \R$
        \item $\det(\va_1, \va_2) = -\det(\va_2, \va_1)$, and
        \item $\det(\ve_1, \ve_2) = 1$
    \end{enumerate}
    As a consequence of (3.1.4), $\det(A)$ is given by
    \begin{equation*}
        \det(A)\equiv a_{11}a_{22} - a_{12}a_{21}
    \end{equation*}
    where $a_{ij}$ refers to the $i$-th row, $j$-th column of the matrix $A$.
    
    \settag{3.1.6}
    \para{Propositions: Determinant in relations.}
    \begin{enumerate}
        \item A $2 \times 2$ matrix $A$ is invertible if and only if $\det(A)\neq 0$
        \item If \trans{T}{V}{V} is linear transformation of a two-dimensional vector space $V$, then $T$ is an isomorphism if and only if $\det(\map{T}{\alpha}{\alpha}) \neq 0$
    \end{enumerate}
\end{enumerate}

\section{The Determinant of an $n \times n$ Matrix}
\begin{enumerate}
    \settag{3.2.1}
    \para{Definition of Multi-linear.} A function $f$ of the rows of a matrix $A$ is called multi-linear of $f$ is linear function of each of its rows when the remaining rows are held fixed. That is, $f$ is multi-linear if for all $b, b' \in \R$,
    \begin{multline*}
        f(\va_1,\ldots,b\va_i + b'\va_i',\ldots,\va_n) \\
        = bf(\va_1,\ldots,\va_i,\ldots,\va_n) + b'f(\va_1,\ldots,\va_i',\ldots,\va_n)
    \end{multline*}
    
    \settag{3.2.2}
    \para{Definition of Alternating.} A function $f$ of rows of a matrix $A$ is said to be alternating if whenever any two rows of A are interchanged $f$ changes sign. That is, for all $i\neq j, 1\leq i, j \leq n$, we have
    \begin{equation*}
        f(\va_1,\ldots,\va_i,\ldots,\va_j,\ldots,\va_n) = -f(\va_1,\ldots,\va_j,\ldots,\va_i,\ldots,\va_n)
    \end{equation*}
    Do notice the switched position of $\va_i$ and $\va_j$ in the above equality.
    
    \settag{3.2.2}
    \para{Lemma.} If $f$ is an alternating real-valued function of the rows of an $n\times n$ matrix $A$ are identical, then $f(A) = 0$. \newline
    \proof \newline
    Assume that $\va_i = \va_j$. Then,
    \begin{align*}
        f(A) &= f(\va_1,\ldots,\va_i,\ldots,\va_j,\ldots,\va_n) \\
        &= -f(\va_1,\ldots,\va_j,\ldots,\va_i,\ldots,\va_n) \\
        &= -f(A)
    \end{align*}
    Hence $f(A) = 0$. \qed \newline
    \textbf{Remark: Sanity check in $\R^2$.} Notice that we have
    \begin{equation*}
        \det
        \begin{bmatrix}
            a & b \\
            a & b \\
        \end{bmatrix}
        =ab-ab = 0
    \end{equation*}
    
    \settag{3.2.4}
    \para{Definition of Minor.} Let the $A$ be an $n\times n$ matrix with entries $a_{ij}$, where $i, j = 1, \ldots, n$. The $ij$-th \tit{minor matrix} of $A$ is defined to be the $(n-1) \times (n -1)$ matrix by deleting the $i$-th row and $j$-th column of $A$. We denote such minor matrix as $A_{ij}$.
    
    \settag{3.2.5}
    \para{Proposition.} Let $A$ be a $3 \times 3$ matrix, and let $f$ be an alternating multi-linear function. Then
    \begin{equation*}
        f(A) = \left[a_{11}\det(A_{11}) - a_{12}\det(A_{12}) + a_{13}\det(A_{13})\right]f(I)
    \end{equation*}
    
    \settag{3.2.6}
    \para{Corollary.} There exists exactly one multi-linear alternating function $f$ of the rows of a $3 \times 3$ matrix such that $f(I) = 1$.
    
    \settag{3.2.7}
    \para{The \tit{det} of $3 \times 3$ matrices.} The determinant function of a $3 \times 3$ matrix is the unique alternating multi-linear function $f$ with $f(I) = 1$. We denote this function by $\det{(A)}$.
    
    \settag{3.2.8}
    \para{Theorem.} There exits exactly one alternating multi-linear function \trans{f}{M_{n\times n}(\R)}{\R} satisfying $f(I) = 1$, which is called the determinant function $f(A) = \det(A)$. Further, any alternating multi-linear function $f$ satisfies $f(A) = \det(A)f(I)$.
    
    \settag{3.2.10}
    \para{Proposition: non-invertible square matrix have zero \tit{det}.} If an $n \times n$ matrix $A$ is not invertible, then $\det(A) = 0$. \newline
    \proof \newline
    If $A$ is not invertible, then the row space of $A$ has dimension at most $(n - 1)$. Therefore, there is a linear dependence among the rows of $A$, 
    \begin{equation*}
        x_1\va_1 + \ldots + x_n\va_n = \vzero~~~~~~\exists i~s.t.~ x_i \neq 0
    \end{equation*}
    Without loss of generality, we assume that $x_1 \neq 0$, then we can divide the entire equation by $x_1$. Solving for $\va_1$ yields us 
    \begin{equation*}
        \va_1 = -\frac{1}{x_1(x_2\va_2 + \ldots + x_n\va_n)} = \sum_{i = 2}^{n}\left(\frac{-x_i}{x_1}\va_i\right)
    \end{equation*}
    Then we consider the determinant function, 
    \begin{align*}
        \det(A) &= \det(\va_1,\ldots,\va_n) \\
        &= \det\left(\sum_{i = 2}^{n}\left(\frac{-x_i}{x_1}\va_i\right), \va_2, \ldots, \va_n\right) \\
        &= \sum_{i = 2}^{n} \left(\frac{-x_i}{x_1}\right)\det(\va_i, \va_2,\ldots,\va_n) \\
        &= 0
    \end{align*}
    since each term in the final sum is the determinant of a matrix with repeated rows, and by (3.2.2), we conclude the proof. \qed
    
    \settag{3.2.11}
    \para{Proposition.} The following equality always holds:
    \begin{equation*}
        \det(\va_1,\ldots,\va_n) = \det(\va_1,\ldots,\va_i + b\va_j,\ldots,\va_n)
    \end{equation*}
    where $\va_i + b\va_j$ appears in the $i$-th position. \newline
    \proof \newline
    This result could be shown using some algebraic manipulation, we proceed as follows:
    \begin{multline*}
        \det(\va_1,\ldots,\va_i + b\va_j,\ldots,\va_n) = \\ \det(\va_1,\ldots,\va_i,\ldots,\va_n) +  b\det(\va_1,\ldots,\va_j,\ldots,\va_j,\ldots,\va_n) \\ 
        = \det(\va_1,\ldots,\va_n)
    \end{multline*}
    Since the term $(\va_1,\ldots,\va_j,\ldots,\va_j,\ldots,\va_n)$ contains repeated rows, hence $\det$ evaluates to zero. \qed
    
    \settag{3.2.12}
    \para{Lemma: Determinant of a diagonal matrix.} If $A \in M_{n\times n}(\R)$ is a diagonal matrix, then $\det(A) = a_{11}a_{22}\cdots a_{nn} = \prod^n_{i = 1}a_{ii}$ \newline
    \proof \newline
    We can actually show a more general claim than this. \newline
    \underline{Claim:} If $A \in M_{n\times n}(\R)$ is a upper/lower triangular matrix, then $\det(A) = \prod^n_{i = 1}a_{ii}$. Notice that we will only prove the case for the upper triangular matrix, since the other is basically a mirror image. \newline
    We proceed by mathematical induction on $n$, the size of the matrix.
    Define predicate $P(n):\text{If}~T_{n} \in M_{n\times n}(\R)$ is upper triangular than $\det(A) = \prod^n_{i = 1}a_{ii}$. This is our Induction Hypothesis. \newline
    WTS: $\forall n \geq 1, P(n).$\newline
    \basecase $n = 1$. It is clearly the case that 
    \begin{equation*}
        \det(A) = a_{11} = \left[\prod^n_{i = 1}a_{ii}\right|_{n = 1}
    \end{equation*}
    so $P(1)$ holds.\newline
    \inductive Let $n \in \double{N}^{\geq 1}$. Assume $P(n)$, we must show that $P(n + 1)$ follows. Let $T_{n+1}$ be any upper triangular matrix of size $(n+1)\times (n + 1)$. Using the minor matrices expansion of the left most column, we have
    \begin{align}
        \det \left({T_{n + 1}}\right) &= \sum_{i=1}^{n+1}(-1)^{i+1}\left(T_{n+1}\right)_{i1}\det\left(\left(T_{n+1}\right)_{i1}\right) \\
        &= \left[\left(T_{n+1}\right)_{i1}\det\left(\left(T_{n+1}\right)_{i1}\right)\right|_{i = 1} + 0 +\ldots +0
    \end{align}
    where the first $\left(T_{n+1}\right)_{i1}$ means the $i1$-th element in the matrix $T_{n+1}$ and the second one means the $i1$ minor. We notice that our I.H. tells us that
    \begin{equation*}
        \left[\det\left(\left(T_{n+1}\right)_{i1}\right)\right|_{i=1} = \prod_{i = 2}^{n+1}a_{ii}
    \end{equation*}
    Then, we can rewrite (3.2) as follows
    \begin{equation*}
        \det(T_{n+1}) = (T_{n+1})_{11}\prod_{i = 2}^{n+1}a_{ii} = \prod_{i = 1}^{n+1}a_{ii}
    \end{equation*}
    where the $a_{ij}$ represents the $ij$-th element in $T_{n+1}$. So $P(n)$ follows, and this completes the proof. \qed
    \settag{3.2.13}
    \para{Proposition.} If $A$ is invertible, then $\det(A)\neq 0$
    
    \settag{3.2.14}
    \para{Theorem: Necessary and Sufficient Condition for Invertibility.} Let $A \in M_{n\times n}(\R)$. Then, $A$ is invertible if and only if $\det(A) \neq 0$
\end{enumerate}

\section{Further Properties of The Determinant} 
\begin{enumerate}
    \settag{3.3.*}
    \para{Definition of Cofactor} Given that $A_{ij}$ is the minor matrix for row $i$ and column $j$ in a square matrix $A$, the \tit{cofactor} at position row $i$ and column $j$ is denoted as $C_{ij}$ and is calculated by the formula 
    \begin{equation*}
        C_{ij}=(-1)^{i+j}\det(A_{ij})
    \end{equation*}
    \tbf{Remark.} Although the formula seems daunting, it is essentially assigning alternating plus and minus to each slot in each row, going left to right and top to bottom.

    \settag{3.3.1}
    \para{Proposition.} $AA'=\det(A)I$, where $A':=\left[ C_{ij} \right]_{ij} \in M_{n \times n}(\R)$ is the cofactor matrix.
    
    \settag{3.3.2}
    \para{Corollary.} If $A$ is an invertible $n \times n$ matrix, then $A^{-1}$ is the matrix whose $ij$-th column is
    \begin{equation*}
        (-1)^{i+j}\frac{\det(A_{ji})}{\det(A)}
    \end{equation*}
    We can re-write this in notation familiar from FM\footnote{CIE Advanced-Level Further Mathematics}
    \begin{equation*}
        A^{-1} = \frac{1}{\det(A)}\left[C_A\right]^T
    \end{equation*}
    where $C_A$ denotes the cofactor matrix, with entires calculated using the formula specified in (3.3.*) above.
    
    \settag{3.3.4}
    \para{Proposition.} For any fixed $j$ such that $1 \leq j \leq n$, we have
    \begin{equation*}
        \det(A) = \sum_{i=1}^{n}(-1)^{i+j}a_{ij}\det(A_{ij})
    \end{equation*}
    
    \settag{3.3.7}
    \para{Proposition.} If $A$ and $B$ are $n\times n$ matrices, then
    \begin{enumerate}
        \item $\det(AB) = \det(A)\det(B)$
        \item If $A$ is invertible, then $\det(A^{-1}) = 1/\det(A)$
    \end{enumerate}
    
    \settag{3.3.8}
    \para{Corollary.} If \trans{T}{V}{V} is a linear transformation, $\dim(V) = n$, then
    \begin{equation*}
        \det(\map{T}{\alpha}{\alpha}) = \det(\map{T}{\beta}{\beta}),\forall~\text{bases}~\alpha, \beta~\text{for}~V
    \end{equation*}
    
    \settag{3.3.9}
    \para{Definition: Determinant of a Linear Transformation.} The determinant of an linear transformation \trans{T}{V}{V} of a finite dimensional vector space is the determinant of \map{T}{\alpha}{\alpha} for any choice of $\alpha$. We denote this with $\det(T)$.
    
    \settag{3.3.11}
    \para{Proposition: Determinant In Relation w/ Isomorphism.} A linear transformation \trans{T}{V}{V} of a fin-dim vector space is an isomorphism if and only if $\det(T)\neq 0$. \newline
    \proof \newline
    Notice that we proved $\det(T)\neq 0 \iff \map{T}{\alpha}{\alpha}$ is an isomorphism, so this proof is trivial. \qed
    
    \settag{3.3.12}
    \para{Proposition.} Let \trans{S}{V}{V} and \trans{T}{V}{V} be linear transformations of a fin-dim vector space, then 
    \begin{enumerate}
        \item $\det(ST) = \det(S)\det(T)$
        \item $T$ is an isomorphism $\implies \det(T^{-1}) = \det(T)^{-1}$
    \end{enumerate}
\end{enumerate}

\begin{center}
\begin{large}
\tbf{THIS CHAPTER IS CURRENTLY ARCHIVED...}
\end{large}
\end{center}


\chapter{Eigen-Problems and Spectral Theorem}
\section{Eigenvalues and Eigenvectors}
\begin{enumerate}
    \settag{4.1.2}
    \para{Definitions.} Let \trans{T}{V}{V} be any linear mapping, then
    \begin{enumerate}
        \item A vector $\vv \in V$ is called an eigenvector of $T$ if
        \begin{equation*}
            \vv \neq \vzero \land \exists \lambda \in \R, T(\vv) = \lambda \vv
        \end{equation*} 
    \end{enumerate}
    
    \settag{4.1.5}
    \para{Proposition.} A vector $\vx$ is an eigenvector of $T$ with eigenvalue $\lambda$ if and only if $\vx \neq \vzero \land \vx \in \ker(T-\lambda I)$
    
    \settag{4.1.6}
    \para{Definition.} Let \trans{T}{V}{V} be a linear mapping, and let $\lambda \in \R$. The $\lambda-$eigenspace of $T$, denoted $E_\lambda$, is the set
    \begin{equation*}
        E_\lambda = \{\vx\in V|T(\vx) = \lambda \vx\}
    \end{equation*}
    
    \settag{4.1.9}
    \para{Proposition.} Let $A\in \mat{n}{n}$. Then $\lambda \in \R$ is an eigenvalue of $A$ if and only if $\det(A-\lambda I) = 0$
    
    \settag{4.1.11}
    \para{Definition of Characteristic Polynomial.} Let $A\in \mat{n}{n}$. The polynomial $\det(A-\lambda I)$ is called the characteristic polynomial of $A$.
    
    \settag{4.1.12}
    \para{Proposition: Similar matrices have equal char-polies}. If two matrices are similar to each other, then they have equal characteristic polynomial. \newline
    \proof \newline
    Suppose $A, B$ are two similar matrices, so that $B = Q^{-1}AQ$ for some invertible matrix $Q$. Then we have
    \begin{align*}
        \det(B-\lambda I) 
        &= \det(Q^{-1}AQ - \lambda I) \\
        &= \det(Q^{-1}AQ - \lambda(Q^{-1} IQ)) \\
        &= \det(Q^{-1}AQ - Q^{-1}\lambda IQ) \\
        &= \det(Q^{-1}(A-\lambda I)Q) \\
        &= \det(Q^{-1})\det(A-\lambda I)\det(Q) \\
        &= \frac{\det(A-\lambda I)\det(Q)}{\det(Q)} = \det(A-\lambda I)
    \end{align*}
    and this completes the proof. \qed \newline
    
    \settag{4.1.13}
    \para{Alternative form of char-poly of a $2 \times 2$ matrix.\footnote{Based on example (4.1.13).}} For a general $2\times 2$ matrix A, we have
    \begin{equation*}
        Char_A(\lambda)= \det(A-\lambda I) = \lambda^2 - Tr(A)\cdot\lambda + \det{A} 
    \end{equation*}
    \textbf{Remark.} This form of characteristic polynomial also takes a more general form for larger square matrices, but the expanded formula is messy so we omit it. \newline
    \textbf{A special case of the Cayley-Hamilton Theorem.} Consider some $2\times 2$ matrix A, with characteristic polynomial $Char_A(\lambda)$. Then, if we treat $A$ as a number and brutally plug $A$ into its own characteristic polynomial, i.e. $Char_A(A)$, we will find that $Char_A(A) = 0_{2 \times 2}$, as if $A$ solves its own characteristic polynomial!
    
    \settag{4.1.14}
    \para{Corollary.} Let $A\in \mat{n}{n}$. Then $A$ has no more than $n$ distinct eigenvalues. In addition, if $\lambda1,...,\lambda_k$ are the distinct eigenvalues of $A$ and $\lambda_i$ is an $m_i$-fold root\footnote{Root of the form $(\lambda_i \pm k)^{m_i}$, where $k \in \R$} of the characteristic polynomial, then $m_1+...+m_k \leq n$.
    
    \settag{4.1.18}
    \para{Statement of the Cayley-Hamilton Theorem.} Let $A$ be any $n\times n$ matrix with characteristic polynomial $Char_A(\lambda)$, then
    \begin{equation*}
        Char_A(A) = 0_{n \times n}
    \end{equation*}
\end{enumerate}

\section{Diagonalizability}
\begin{enumerate}
    \settag{4.2.1}
    \para{Definition of Diagonalizability.} Let $V$ be a finite-dimensional vector space, and let \trans{T}{V}{V} be a linear mapping. $T$ is said to be diagonalizable if there exists a basis $V$, all of whose vectors are eigenvectors of $T$.
    
    \settag{4.2.2}
    \para{Proposition.} \trans{T}{V}{V} is diagonalizable if and only if, for any basis $\alpha$ of $V$, the matrix $\map{T}{\alpha}{\alpha}$ is similar to a diagonal matrix. \newline
    \proofforward \newline
    \% to appear added later... \newline
    \proofback \newline
    If $A = \map{T}{\alpha}{\alpha}$ is similar to a diagonal matrix $D$, then by definition, there is some invertible matrix $P$ such that $P^{-1}AP = D$. Then, by re-arranging, we have $AP = DP$. Notice that since $D$ is diagonal matrix, the columns of $PD$ are just scalar multiples of the columns of $P$. Hence, the columns of $P$ are all eigenvectors for the matrix $A$, or equivalently, the columns of $P$ are the coordinate vectors of eigenvectors for the mapping $T$. On the other hand, since $P$ is invertible, we know its columns are linearly independent, hence those columns are coordinate vectors for a basis of $V$. Therefore, by definition, $T$ is diagonalizable. \tbf{Note} that eigenvalues of $T$ are the diagonal entries of $D$, and this completes the proof. \qed\newline
    \tbf{In naive words, }in order for a linear mapping to be diagonalizable, it needs to have enough linearly independent eigenvectors to form a basis of $V$. To be specific, we need $n$ linearly independent eigenvectors for a $n$-dimensional to $n$-dimensional mapping to be diagonalizable.
    
    \settag{4.2.4}
    \para{Proposition.} Let $\vx_i(1 \leq i \leq k)$ be eigenvectors of a linear mapping \trans{T}{V}{V} corresponding to distinct eigenvalues $\lambda_i$, then $\{\vx_1,...,\vx_k\}$ is linearly independent subset of $V$.
    
    \settag{4.2.5}
    \para{Corollary.} For each $i(1 \leq i \leq k)$, let $\{\vx_{i, 1},...,\vx_{i, n_i}\}$ be a linearly independent set of eigenvectors of $T$ all with eigenvalue $\lambda_i$ and suppose the $\lambda_i$ are distinct. Then, 
    \begin{equation*}
        S=\bigcup_{i=1}^k\{\vx_{i, 1},...,\vx_{i, n_i}\}
    \end{equation*}
    is linearly independent.
    
    \settag{4.2.6}
    \para{Proposition.} Let $V$ be finite-dimensional, and let \trans{T}{V}{V} be linear. Let $\lambda$ be an eigenvalue of $T$, and assume that $\lambda$ is an $m$-fold root of the characteristic polynomial of $T$. Then, $1\leq \dim(E_\lambda) \leq m$.
    
    \settag{4.2.7}
    \para{Theorem.} Let \trans{T}{V}{V} be a linear mapping on a finite-dimensional vector space $V$, and let $\lambda_1,...,\lambda_k$ be its distinct eigenvalues. Let $m_i$ be the multiplicity of $\lambda_i$ as a root of the characteristic polynomial of $T$. Then $T$ is diagonalizable if and only if
    \begin{enumerate}
        \item $m_1+...+m_k = n = \dim(V)$, \tit{\tbf{and}} \newline
        \tit{In naive words, }this means $T$ has to have $n$ real eigenvalues
        \item $\forall i, \dim(E_{\lambda_i}) = m_i$ \newline
        \tit{In naive words, }this means each of the $E_{\lambda_i}$ attains its maximum dimension specified in (4.2.6), i.e. $\dim(E_{\lambda_i}) = m_i$
    \end{enumerate}
    
    \settag{4.2.8}
    \para{Corollary.} Let \trans{T}{V}{V} be linear mapping on a finite-dimensional space $V$, and assume that $T$ has $\dim(V) = n$ distinct real eigenvalues. Then $T$ is diagonalizable. \newline
    \proof \newline
    Since $T$ has $n$ distinct real roots, then the algebraic multiplicity associated with each root must be 1. Then notice that $1\leq \dim(E_{\lambda_i}) \leq m_i = 1, \forall i$, and we conclude the result by applying Theorem (4.2.7). This completes the proof. \qed
    
    \settag{4.2.9}
    \para{Corollary.} A linear mapping \trans{T}{V}{V} on a finite dimensional space $V$ is diagonalizable if and only if the sum of the multiplicities of the real eigenvalues is $n = \dim(V)$, and either
    \begin{enumerate}
        \item We have $\sum_{i=1}^k \dim(E_{\lambda_i}) = n$, where $\lambda_i$ are the distinct eigenvalues of $T$, or
        \item We have $\sum_{i=1}^k (n - \dim(\text{Im}(T-\lambda_i I))) = n$, where again $\lambda_i$'s are the distinct eigenvalues.
    \end{enumerate}
    \tbf{Remark.} The second statement is a direct consequence of the Rank-Nullity Theorem.
\end{enumerate}

\section{Geometry in Euclidean Space}
\begin{enumerate}
    \settag{4.3.1}
    \para{Definition: Standard Inner Product.} \footnote{The version of Inner product that appeared earlier in these notes will \tit{always} refer to the standard inner product for the $n$-dimensional Euclidean Space.}The standard inner product (or dot product) on $\R^n$ is the function
    $\langle\cdot,\cdot\rangle: \R^n \times \R^n \rightarrow{} \R$
    defined be the following rule: If $\vx = (x_1,...,x_n), \vy = (y_1,...,y_n)$ in the $\xi$ coordinate basis, then 
    \begin{equation*}
        \langle\vx,\vy \rangle := \sum_{i=1}^n x_iy_i
    \end{equation*}
    
    \settag{4.3.2}
    \para{Propositions.} The Inner Product has the following properties:
    \begin{enumerate}
        \item \textbf{Bi-linearity:} $\forall \vx, \vy, \vz \in \R^n, \forall c\in \R, \langle c\vx + \vy,\vz\rangle = c\langle\vx,\vz\rangle + \langle \vy,\vz \rangle$
        \item \textbf{Symmetric:} $\forall \vx, \vy\in\R^n, \langle \vx,\vy\rangle = \langle \vy,\vx \rangle$
        \item \textbf{Positive-Definite:} $\forall \vx \in \R^n, \langle \vx,\vx \rangle \geq 0 \land \langle \vx,\vx \rangle = 0 \iff \vx = \vzero$
    \end{enumerate}
    
    \settag{4.3.3}
    \para{Definitions.} We have the following related definitions:
    \begin{enumerate}
        \item The length, norm, of $\vx \in \R^n$ is the scaller defined as
        \begin{equation*}
            \norm{\vx} = \sqrt{\langle\vx,\vx\rangle}
        \end{equation*}
        \item $\vx$ is called a unit vector if $\norm{\vx} \equiv 1$, we can normalize 
        a vector $\vx\in \R^n$ by $\frac{\vx}{\norm{\vx}}$
    \end{enumerate}
    
    \settag{4.3.4-(1)}
    \para{The Cauchy-Schwarz Inequality.}
    \begin{equation*}
        \left|\langle \vx, \vy \rangle\right| \leq \norm{\vx} \cdot \norm{\vy}
    \end{equation*}
    This is a historically important inequality and we shall present the proof here: \newline
    \proof \newline
    Consider the following inner product, where $\vx,\vy\in \R^n, c\in \R$. We can expand the terms
    using bilinearity and the symmetric property of of inner product
    \begin{align*}
        \langle \vx - c\vy,\vx - c\vy \rangle &= \langle \vx,\vx\rangle - c\langle \vx,\vy\rangle
        - c\langle \vy,\vx\rangle + c^2\langle \vy,\vy\rangle \\
        &= \norm{\vx}^2 + c^2\norm{\vy}^2 - 2c\langle \vx,\vy\rangle \geq 0
    \end{align*}
    notice that the last ``$\geq$'' conclusion was drawen based on the positive definite property
    of the inner product of a vector with itself. Then, we consider
    \begin{equation*}
        c = \frac{\langle \vx,\vy \rangle}{\norm{\vy}}\in \real
    \end{equation*}
    Then our former inequality gives us
    \begin{align*}
        0 &\leq \norm{\vx}^2 - \frac{2\langle \vx,\vy\rangle^2}{\norm{\vy}^2} + \frac{\langle \vx,\vy\rangle^2}{\norm{\vy}^2} \\
        &= \norm{\vx}^2 - \frac{\langle \vx,\vy\rangle^2}{\norm{\vy}^2} \\
        \implies \norm{\vx}^2 &\geq \frac{\langle \vx,\vy\rangle}{\norm{\vy}^2} \\
        \implies \langle \vx,\vy\rangle^2 &\leq \norm{\vx}^2\norm{\vy}^2
    \end{align*}
    where we can take the square root on both sides, and this yields us desired 
    $\left|\langle \vx,\vy\rangle\right| \leq \norm{\vx}\cdot \norm{\vy}$. \qed

    \settag{4.3.4-(2)}
    \para{The Triangular Inequality.}
    \begin{equation*}
        \forall \vx, \vy \in \R^n, \norm{\vx + \vy}\leq \norm{\vx} + \norm{\vy}
    \end{equation*}
    This geometrically says that any side of a triangle is less than the sum of the other 
    two sides. This is a consequence of the Cauchy-Schwarz Inequality above.\newline
    \proof \newline
    Consider the inequality presented in the proof for Cauchy-Schwarz above, and take the special
    case of $c = -1\in\R$
    \begin{align*}
        0 \leq \langle \vx + \vy,\vx+\vy \rangle &= \norm{\vx}^2 + 2\langle \vx,\vy\rangle + \norm{\vy}^2 \\
        \implies \norm{\vx+\vy}^2 &= \norm{\vx}^2 + 2\langle \vx,\vy\rangle + \norm{\vy}^2 \\
        &\leq \norm{\vx}^2 + 2\norm{\vx}\norm{\vy} + \norm{\vy}^2 \\
        &= \left(\norm{\vx} +\norm{\vy}\right)^2
    \end{align*}
    Taking square root on both sides of the above inequality yields the trigular inequality. \qed
    
    \settag{4.3.5}
    \para{Definition of angle between vectors.} The angle between two non-zero vectors 
    $\vx,\vy \in \R^n$ is defined to be
    \begin{equation*}
        \theta = \cos^{-1}\left(\frac{\langle\vx,\vy\rangle}{\norm{\vx} \cdot \norm{\vy}}\right)
    \end{equation*}
    Notice that we defined this angle in chapter 2, but we focused on $\R^2$ and $\R^3$. 
    This is an extension to arbitrary $n$-dimensional Euclidean Space.
    
    \settag{4.3.7}
    \para{Definition of Orthogonality.} Two vectors $\vx,\vy \in \R^n$ are defined to be 
    orthogonal if $\langle \vx,\vy\rangle = 0$
    
    \settag{4.3.9}
    \para{Definitions: Orthogonal and Orthonormal}
    \begin{enumerate}
        \item A set of vectors $S\subset \R^n$ is said to be \tit{orthogonal} if $\forall 
        \vx,\vy \in S, \vx \neq \vy \implies \langle \vx,\vy\rangle = 0$
        \item A set of vectors $S\subset \R^n$ is said to be \tit{orthonormal} if $S$ 
        is orthogonal and, in addition, $\forall \vx \in S, \norm{\vx} = 1$
    \end{enumerate}
    
    \settag{4.3.10}
    \para{Proposition: Orthogonal vectors are linearly independent.} If $\vx,\vy \in \R^n$ 
    are orthogonal, with $\vx \neq \vzero \neq \vy$, then $\{\vx,\vy\}$ is linearly independent.

    \settag{4.3.10*}
    \para{Extension to Proposition.}\footnote{Question 8, Exercise 4.3} If we wish, we could extend the Proposition (4.3.10) to the
    following: Let $S = \{\vx_1,...,\vx_k\}\subseteq \R^n$ be any orthogonal set of non-zero vectors. 
    Then, $S$ is linearly independent. To prove this, we can use a simple induction argument that
    adding a vector that is linearly independent to already existing vectors in a set would not
    destroy the linear independence ecosystem.

    \settag{4.3.*}
    \para{Polarization Identity.} This is question 6 in the Exercises section of 4.3. We have the
    following statement
    \begin{equation*}
        \forall \vx,\vy\in \R^n, \langle \vx,\vy\rangle = (1/4)\left( \norm{\vx+\vy}^2 - \norm{\vx-\vy}^2 \right)
    \end{equation*}
    this shows that we can recover the inner product on $\R^n$ just by measuring lengths of vectors.\newline
    \proof \newline
    Let $\vx,\vy\in \R^n$, then
    \begin{align*}
        \frac{1}{4}\left( \norm{\vx+\vy}^2 - \norm{\vx-\vy}^2\right)
        &= \frac{1}{4}\left(\langle \vx+\vy,\vx+\vy\rangle - \langle \vx-\vy,\vx-\vy\rangle\right)\\
        &= \frac{1}{4}\left(4\langle \vx,\vy\rangle \right) \\
        &= \langle \vx,\vy\rangle
    \end{align*}
    and this completes the proof. \qed

\end{enumerate}

\section{Orthogonal Projections and The Gram-Schmidt Process}
\begin{enumerate}
    \settag{4.4.1}
    \para{Definition of Orthogonal Complement.} The orthogonal complement of $W$, denoted as 
    $W^{\perp}$, is defined to be
    \begin{equation*}
        W^\perp := \{\vv\in \R^n | \langle \vv,\vw\rangle = 0, \forall \vw \in W\}
    \end{equation*}
    \textbf{Remark.} Notice that the $\vzero$ vector should be in any such $W^\perp$ sets, 
    since the zero vectors is orthogonal to every vector.
    
    \settag{4.4.2}
    \para{(Interesting) Examples.}
    \begin{enumerate}
        \item If we consider $W = \{\vzero\}$, then we see that every vector in $\R^n$ (including $\vzero$)
        is in the orthogonal complement of $W$. So, $\{\vzero\}^\perp\equiv \R^n$
        \item On the otherhand, if we consider $W = \R^n$. Then only the zero vector is in the orthogonal 
        complement of $W$. So, $(\R^n)^\perp \equiv \{\vzero\}$
    \end{enumerate}

    \settag{4.4.3}
    \para{Propositions.}
    \begin{enumerate}
        \item $\forall$ subspace $W \subset \R^n, W^\perp\subset \R^n$ is also a subspace
        \item $\dim(W) + \dim(W^\perp) = \dim(\R^n) = n$
        \item $\forall\, W\subset \R^n$, $W$ is a subspace $\implies W \cap W^\perp = \{\vzero\}$
        \item Given a subspace $W\subset \R^n$, $\forall \vx \in \R^n$, $\vx$ can be written 
        uniquely as $\vx = \vx_1 + \vx_2$, where $\vx_1 \in W, \vx_2 \in W^\perp$. In summary, 
        we have $\R^n = W \oplus W^\perp$
    \end{enumerate}
    
    \settag{4.4.5}
    \para{Propositions.}
    \begin{enumerate}
        \item $P_W$ is a linear mapping
        \item Im$(P_W) = W, \vw \in W \implies P_W(\vw) = \vw$
        \item Ker$(P_W) = W^\perp$
    \end{enumerate}
    
    \settag{4.4.6}
    \para{Proposition.} Let $\{\vw_1,...,\vw_k\}$ be an orthonormal basis for the subspace $W \subseteq \R^n$
    \begin{enumerate}
        \item $\forall \vw \in W, \vw = \sum_{i = 1}^k \langle \vw,\vw_i \rangle \vw_i$
        \item $\forall \vx \in \R^n, P_W(\vx) = \sum_{i = 1}^k \langle \vx,\vw_i \rangle \vw_i$
    \end{enumerate}
    
    \settag{4.4.9}
    \para{Theorem: Every Subspace has an Orthonormal Basis.} Let $W$ be a subspace of $\R^n$. 
    Then there exists an orthonormal basis of $W$. \newline 
    \textbf{Remark.} A way to think of this theorem and make this trivial is that we first 
    pick some orthogonal basis for such subspace, and then scale them so that they are unit vectors.
    
    \settag{4.4.9*}
    \para{Gram-Schmidt Process.} Consider $W = span\{\vw_j\}_{j=1}^k$, were the $\vw$'s are 
    basis vectors for the vector space $W$. Then we calculate, in sequence
    \begin{align*}
        \vv_1 &= \vw_1 \\
        \vv_2 &= \vw_2 - P_{span\{\vv_1\}}\vw_2 \\
              &\cdots  \\
        \vv_k &= \vw_k - P_{span\{\vv_1,...,\vv_{k-1}\}}\vw_k
    \end{align*}
    after which we normalize $\vv_i$ by $\hat{\vv}_i = \vv_i/\norm{\vv_i}$, then $\alpha = 
    \{\hat{\vv}_1,...,\hat{\vv}_k\}$ is an orthonormal basis for the real vector space $W$.
\end{enumerate}

\section{Symmetric Matrices}
\begin{enumerate}
    \settag{4.5.2a}
    \para{Proposition.} Let $A \in \mat{n}{n}$.
    \begin{enumerate}
        \item $\forall \vx,\vy \in \R^n, \langle A\vx,\vy\rangle = \langle \vx, A^T\vy\rangle$
        \item $A$ is symmetric if and only if $\langle A\vx,\vy\rangle = \langle \vx,A\vy 
        \rangle, \forall \vx, \vy \in \R^n$
    \end{enumerate}
    
    \settag{4.5.2b}
    \para{Corollary.} Let $V$ be any subspace of $\R^n$, let \trans{T}{V}{V} be any linear 
    transformation, and let $\alpha = \{\vx_1,...,\vx_k\}$ be any orthogonal basis for $V$. 
    Then \map{T}{\alpha}{\alpha} is a symmetric matrix if and only if $$\langle T(\vx), 
    \vy\rangle = \langle \vx,T(\vy)\rangle, \forall \vx, \vy \in V$$
    
    \settag{4.5.3}
    \para{Definition.} Let $V$ be a subspace of $\R^n$. A linear mapping \trans{T}{V}{V} 
    is said to be symmetric if $\langle T(\vx),\vy\rangle = \langle \vx, T(\vy) \rangle, \forall \vx, \vy \in V$
    
    \settag{4.5.6}
    \para{Theorem: Symmetric Matrices have Real Eigenvalues.} Let $A \in \mat{n}{n}$ be a 
    symmetric matrix. Then all the roots to the characteristic polynomial $char_A(\lambda)$ are real.
    
    \settag{4.5.7}
    \para{Theorem.} Let $A \in \mat{n}{n}$ be a symmetric matrix, let $\vx_1, \vx_2$ be 
    eigenvectors of $A$ with eigenvalues $\lambda_1, \lambda_2$ respectively. We have 
    $\lambda_1 \neq \lambda_2 \implies \vx_1$ and $\vx_2$ are orthogonal vectors in $\R^n$.
\end{enumerate}

\section{The Spectral Theorem}
\begin{enumerate}
    \settag{4.6.1}
    \para{Theorem.} Let \trans{T}{V}{V} be a symmetric linear mapping. Then there is 
    an \textit{orthonormal} basis of $\R^n$ consisting of eigenvectors of $T$. In particular, 
    $T$ is diagonalizable. In naive words, we can view this as ``Symmetric matrices are really really
    diagonalizable, in the sense that the set of eigenvectors is orthonormal.''
    
    \settag{4.6.3}
    \para{Theorem.} Let \trans{T}{\R^n}{\R^n} be a symmetric linear mapping, and let 
    $\lambda_1,..,\lambda_k$ be the distinct eigenvalues of $T$. let $P_i$ be the 
    orthogonal projection of $\R^n$ onto the eigenspace $E_{\lambda_i}$. Then
    \begin{enumerate}
        \item $T = \lambda_1P_1+...+\lambda_k P_k$, and
        \item $I = P_1+...+P_k$
    \end{enumerate}
    \textbf{Note:} Although the part (b) seems like a nothing in euclidean space, it is 
    actually very powerful in other spaces, for example in the function space, this is 
    the algebraic fundation for the Fourier Series.\footnote{Familiar from MAT237.}

    \settag{4.6.*}
    \para{Summrization during tutoral.} There are two common versions of the spectral theorem, 
    in which the second version is the same as the first one after choosing some basis. 
    \paragraph{Version 1.} Let $A$ be a $n\times n$ real matrix that is symmetric, then there exits
    a inverse of $T$, denoted $T^{-1}$ s.t. 
    \begin{equation*}
        TAT^{-1} = 
        \begin{bmatrix}
            \lambda_1 & \cdots & 0 \\
            \vdots & \ddots & \vdots \\
            0 & \cdots & \lambda_n
        \end{bmatrix}
    \end{equation*}
    where $\lambda_i\in \real, \forall i$ are eigenvalues of $T$

    \paragraph{Version 2.} For a innner product space $V$, consider \map{T}{V}{V} a symmetric linear map,
    then $\exists \alpha = \{\ve_1,...,\ve_n\}$, where $T\ve_j = \lambda_j \ve_j \land \lambda_j \in \real$.
    \begin{equation*}
        \langle T\vx,\vy\rangle = \langle \vx,T^t\vy \rangle = \langle \vx,T\vy\rangle \forall \vx,\vy\in V
    \end{equation*}
\end{enumerate}

\chapter{Complex Numbers and Complex V.S.}
\section{Complex Numbers}
\begin{enumerate}
    \settag{5.1.1}
    \para{Definition.} The set of complex numbers, denoted as $\mathbb{C}$, is the set 
    of ordered pairs of real numbers $(a,b) = (a+bi)$ with operations of addition and 
    multiplication defined by
    \begin{enumerate}
        \item $\forall (a,b), (c,d)\in \mathbb{C}$, the sum of $(a,b)$ and $(c,d)$ is 
        the complex number defined by
        \begin{equation*}
            (a+ib) + (c+id) = (a+c)+i(b+d)
        \end{equation*}
        \item $\forall (a,b),(c,d) \in \mathbb{C}$, the product of $(a,b)$ and $(c,d)$ 
        is the complex number defined by
        \begin{equation*}
            (a+ib)(c+id) = (ac-bd)+i(ad+cb)
        \end{equation*}
    \end{enumerate}
    
    \settag{5.1.2}
    \para{Definitions.} Let $z = a+bi \in \complex$. The real part of z is $Re(z) = a$, 
    while the imaginary part of $z$ is $Im(z) = b$.
    
    \settag{5.1.4}
    \para{Definition of Field.} A field is a set $\mathbb{F}$ with two operations $+$ and 
    $\cdot$. In addition, it has to satisfy the following axioms:
    \begin{enumerate}
        \item \textbf{Commutativity of Addition:} 
        \begin{equation*} 
            \forall x, y \in \mathbb{F}, x+y = y+x
        \end{equation*}
        \item \textbf{Associativity of Addition:} 
        \begin{equation*} 
            \forall x, y , z \in \mathbb{F}, (x+y)+z = x +(y+z)
        \end{equation*}
        \item \textbf{Existence of Additive Identity:} 
        \begin{equation*} 
            \exists!\, 0 \in \mathbb{F}, \forall x\in \mathbb{F}, 0+x = x
        \end{equation*}
        \item \textbf{Existence of Additive Inverse:} 
        \begin{equation*}
            \forall x \in \mathbb{F}, \exists! -x \in \mathbb{F}, x+(-x) = 0
        \end{equation*}
        \item \textbf{Commutativity of Multiplication:} 
        \begin{equation*}
            \forall x, y \in \mathbb{F}, xy = yx
        \end{equation*}
        \item \textbf{Associativity of Multiplication:} 
        \begin{equation*}
            \forall x,y,z \in \mathbb{F}, (xy)z = x(yz)
        \end{equation*}
        \item \textbf{Distributivity:}
        \begin{equation*} 
            \forall x,y,z \in \mathbb{F}, (x+y)z = xz+yz \land x(y+z) = xy+xz
        \end{equation*}
        \item \textbf{Existence of Multiplicative Identity:} 
        \begin{equation*} 
            \exists!\, 1\in \mathbb{F}, \forall x\in \mathbb{F}, 1\cdot x = x 
        \end{equation*}
        \item \textbf{Existence of Multiplicative Inverse:} 
        \begin{equation*}
            \forall x \in \mathbb{F}, x\neq 0\implies \exists!\,1/x \in \mathbb{F}, x \cdot 1/x = 1
        \end{equation*}
        Note that these are very different axioms from those that we encountered when defining 
        a vector space. For example, multiplication wasn't even defined for vectors! Also, there are nine axioms here rather than eight that we used to have.
    \end{enumerate}
    
    \settag{5.1.4*}
    \para{Examples and Counter Examples of Fields.}
    First, some examples that are not fields.
    \begin{enumerate}
        \item First let's examine our good old friend: the space of polynomials $P_n(\R)$. 
        Note that if a take a polynomial and try to construct a multiplicative inverse for 
        it, the constructed object will no longer be within the field, (a rational function actually) 
        so the set of all polynomials fails to be a field.
        \item The set of integers, $\mathbb{Z}$, is another counter example. Notice that 
        this set fails for exactly the same reason as the previous one, there is no multiplicative 
        inverse for each element.
    \end{enumerate}
    Next, some examples for fields
    \begin{enumerate}
        \item One should recognize rather quickly that the set $\R$ is a field! In fact, 
        field is an abstraction originated form the real numbers.
        \item Similarly, the set of algebraic numbers is a field.
        \begin{equation*}
            \text{Algebraic Numbers}:= \{x\in \R|\, p(x) = 0~~\text{for some }p\in P(\R)\}
        \end{equation*}
        We like this set, because it is closed.
    \end{enumerate}
    
    \settag{5.1.5}
    \para{Proposition: Complex Numbers form a Field} The set of complex numbers is a field with the operations 
    of addition and scalar multiplication as defined previously. \newline
    \proof \newline
    All of the axioms except for possibly the axiom (i) are trivial to show to be true, so will neglect 
    them here. Now, if $a + ib\neq 0$, then necessarily $a \neq 0 \wedge b \neq 0$. We then can deduce that $a^2 + b^2 \neq 0$. Consider any real number $a + ib$, let
    \begin{equation*}
        1/(a+ib) := \frac{a-ib}{a^2+b^2} \in \complex
    \end{equation*}
    It is easy to show that $(a+ib)\cdot\frac{a-ib}{a^2+b^2} = \frac{a^2+b^2}{a^2+b^2} = 1$, so indeed we have a 
    multiplicative inverse for any complex number. \qed
    
    \settag{5.1.7}
    \para{Proposition on Uniqueness.} Note that, as indicated in Definition (5.1.4), all of the additive identity in a field, 
    the additive inverse of an element of a field, the multiplicative identity of a field and the multiplicative inverse of a 
    non-zero element of a field are unique.

    \settag{5.1.8}
    \para{Definition of Absolute Value and Arugument.} The absolute value of the complex number $z = a + ib\in \complex$
    is the non-negative real number defined as
    \begin{equation*}
        r := |z| = \sqrt{a^2 + b^2}
    \end{equation*}
    and the argument of a the complex number is the angle between the two components, namely $a$ and $b$
    on the complex plain. Then we have the following identity
    \begin{equation*}
        z = |z|(\cos \theta + i\sin \theta)
    \end{equation*}
    \paragraph{Perodicity of the above identity.} Notice that if we replace $\theta$ with $\theta \pm
    2\pi k, k\in \mathbb{Z}$, we defined exactly the same cpmplex number $z$. 

    \settag{5.1.9}
    \para{Statement of De Moivre's Theorem.}\footnote{This should be familiar from FM}
    \begin{equation*}
        \forall x\in \R, n\in \mathbb{Z}, \left(\cos(x) + i\sin(x)\right)^n = \cos(nx) + i\sin(nx)
    \end{equation*}
    We can also reformulate this into the familiar notation that we used above, denoting the absolute
    value, or length, of the complex number, we have
    \begin{equation*}
        z^n = r^n\left(\cos(n\theta) + i\sin(n\theta)\right)
    \end{equation*}

    \settag{5.1.9*}
    \para{Relation w/ Euler's Formula.} First, we recall the Euler Formula as below
    \begin{equation*}
        e^{i\theta} = \cos \theta + i\sin \theta
    \end{equation*}
    Notice that in a special case of $\theta = \pi$, the above identity is a.k.a Euler's 
    Identity\footnote{$e^{i\pi}+1=0$}. Considering any $z\in \complex$, to derive the above identity, we have the following
    \begin{align*}
        z^n &= |z|^n\left(e^{i\pi}\right)^n\\
        &= |z|^ne^{i\pi n} \\
        &= r^n\left(\cos n\theta + i\sin n \theta\right)
    \end{align*}
    notice that we can now interchange, as we please, $\cos \theta + i\sin \theta$ with $e^{i\theta}$. \qed

    \settag{5.1.11}
    \para{Definition of Algebraic Closure.} A field $\mathbb{F}$ is called algebraically closed if
    \begin{equation*}
        p(z) = a_nz^n + ... + a_1z + a_0\in P_n(\complex)~~~\text{where}~a_i\in \mathbb{F}~\text{and}~a_n\neq 0
    \end{equation*}
    has $n$ roots counted with multiplicity in $\mathbb{F}$.

    \settag{5.1.12}
    \para{Theorem: Fundamental Theorem of Algebra.} The set $\complex$ is closed and it is the smallest algebraically closed field 
    containing $\real$

    \settag{5.1.*}
    \para{Roots of Unity.} We shall demonstrate this using the following example: suppose that we want to solve $i^{1/5}$.
    Then, by breaking this down using the Euler's Formula, we have
    \begin{equation*}
        i^{1/5} = \left(e^{i\frac{\pi}{2} + k2\pi}\right)^{1/5} = e^{i\frac{\pi}{10} + \frac{2\pi}{5}k}, 
        k \in \mathbb{Z}^{\geq 0}
    \end{equation*}
    By stustituting appropriate values of $k$, we have
    \begin{align*}
        \text{when}~k&=0, \text{sol} = \cos \frac{\pi}{10} + i\sin \frac{\pi}{10} \\
        \text{when}~k&=1, \text{sol} = \cos \frac{\pi}{2} + i\sin \frac{\pi}{2}  = i\\
        \text{when}~k&=2, \text{sol} = \cos \frac{9\pi}{10} + i\sin \frac{9\pi}{10} \\
        &\ldots
    \end{align*}
    by continuing see that the solutions cycles, since we have only 5 unique solutions. \qed\newline
    \textbf{General result.} Now we consider the problem of solving the equality $z^n = 1$, we call this the
    $n$-th root of unity.
    \begin{equation*}
        z = \left(e^{2\pi ik}\right)^\frac{1}{n} = e^{\frac{2\pi ik}{n}}, k\in \mathbb{Z}^{\geq 0}
    \end{equation*}
\end{enumerate}

\section{(Field) Vector Spaces}
\begin{enumerate}
    \settag{5.2.1}
    \para{Definition of a Vector Space Over Field}. In Chapter One, we defined what is called a 
    real vector space, we shall extend that idea into a more general one here.
    A vector space over a field $\field$ is a set $V$ whose elements we call vectors together with, 
    as usual, closure properties under the two operations 
    \begin{enumerate}
        \item \textbf{Closure under vector addition:} an operation called vector addition, which for each pair of vectors $\mathbf{x}, \mathbf{y}\in V$ produces another vector in $V$ denoted $\mathbf{x} + \mathbf{y}$, (i.e. $\forall \mathbf{x}, \mathbf{y}\in V, \mathbf{x} + \mathbf{y} \in V$) and
        \item \textbf{Closure under scalar multiplication:} an operation called multiplication by a scalar (a field element), which for each vector $\mathbf{x}\in V$, an each scalar $c\in \mathbb{F}$ produces another vector in $V$ denoted $c\mathbf{x}$. (i.e. $\forall \mathbf{x}\in V, \forall c \in \mathbb{F}, c \mathbf{x} \in V$)
    \end{enumerate}
    and the eight axioms to satisfy
    \begin{enumerate}
        \item $\forall \mathbf{x}, \mathbf{y}, \mathbf{z} \in V, (\mathbf{x} + \mathbf{y}) + \mathbf{z} = \mathbf{x} + (\mathbf{y} + \mathbf{z})$
        \item $\forall \mathbf{v}, \mathbf{y} \in V, \mathbf{x} + \mathbf{y} = \mathbf{y} + \mathbf{x}$
        \item $\exists \vzero \in V s.t. \forall \mathbf{x} \in V, \mathbf{x} + \vzero = \mathbf{x}$ (Note that this property is a.k.a existence of additive identity)
        \item $\forall \mathbf{x} \in V, \exists (-\mathbf{x}) \in V~s.t.~\mathbf{x} + (-\mathbf{x}) = \vzero$ (Note that this property is a.k.a existence of additive inverse)
        \item $\forall \mathbf{x}, \mathbf{y} \in V, c \in \mathbb{F}, c(\mathbf{x} + \mathbf{y}) = c\mathbf{x} + c\mathbf{y}$
        \item $\forall \mathbf{x} \in V, c,d \in \mathbb{F}, (c + d)\mathbf{x} = c\mathbf{x} + d\mathbf{x}$
        \item $\forall \mathbf{x} \in V, c,d \in \mathbb{F}, (cd)\mathbf{x} = c(d\mathbf{x})$
        \item $\forall \mathbf{x} \in V, 1\mathbf{x} = \mathbf{x}$
    \end{enumerate}
\end{enumerate}

\section{Geometry in Complex Vector Spaces}
\begin{enumerate}
    \settag{5.3.1}
    \para{Definition of Hermitian Inner Product.} Let $V$ be a complex vector space. A Hermitian
    inner product is a complex valued function on pairs of vectors in $V$, denoted by $\langle \vu, \vv\rangle\in \complex, \forall \vv,\vu \in V$,
    which statisfies the following properties
    \begin{enumerate}
        \item $\forall \vu, \vv, \vw\in V, a, b\in \complex, \langle a\vu+b\vv,\vw\rangle = a\langle \vu,\vw\rangle + b\langle \vv,\vw\rangle$. In naive words, the Hermitian Inner Product requires linearity in the first slot.
        \item $\forall \vu, \vv\in V, \langle \vu, \vv \rangle = \overbar{\langle \vv, \vu \rangle}$, i.e., we need so called conjugate linear in the second slot. 
        \item $\forall \vv\in V,\langle \vv,\vv \rangle \geq 0 \land \langle \vv,\vv\rangle =0 \implies \vv = \vzero$
    \end{enumerate}
    \paragraph{Sesquilinearity.} We have already seen that in a hermitian inner product, the second slot is conjugate linear. We shall illustrate this further.
    Consider some complex vector space $V$, and let $\vv,\vu\in V, a\in \complex$, we have
    \begin{equation*}
        \langle \vu,a\vv \rangle = \overbar{\langle a\vv,\vu\rangle} = \overbar{a\langle \vv,\vu \rangle} = \overbar{a}\overbar{\langle \vv,\vu\rangle} = \overbar{a}\langle \vu,\vv\rangle
    \end{equation*}
    notice that in this case, we lose the bilinearity that we had in an ordinary euclidean space.

    \settag{5.3.3}
    \para{Definitions in HIPS.} Let $V$ be a complex vector space with with a Hermitian Inner Product. We define the following
    \begin{itemize}
        \item The norm of $\vv\in V$ is $\norm{\vv} = \sqrt{\langle \vv,\vv \rangle}$
        \item A set of \tit{non-zero} vectors $\{\vv_1,...,\vv_k\}\subset V$ is called orthogonal if $\langle \vv_i,\vv_j \rangle =0, \forall i\neq j$.
        \item If a set of vectors is orthogonal, and in addition, $\langle \vv_i,\vv_i \rangle = 1, \forall i$ in the set, the set is said to be orthonormal.
    \end{itemize}

    \settag{5.3.5}
    \para{Lemma.} If $\alpha = \{ \vv_1,...,\vv_n \}$ is an orthonormal basis for the Hermitian inner product space $V$ and \trans{T}{V}{V} is a linear mapping, then
    \begin{equation*}
        \left[ \map{T}{\alpha}{\alpha}\right]_{ij} = \langle T(\vv_j, \vv_i) \rangle
    \end{equation*}

    \settag{5.3.7}
    \para{Definition of Adjoint in HIPS.} Let $V$ be a finite dimensional Hermitian Inner Product Space and let
    $\alpha$ be an orthonormal basis for $V$. The adjoint of the linear transformation \trans{T}{V}{V} is the linear
    transformation $T^*$ whose matrix with respect to the orthonormal basis $\alpha$ is the matrix $\left(\map{\bar{T}}{\alpha}{\alpha}\right)^t$. That is
    \begin{equation*}
        \map{T^*}{\alpha}{\alpha} = \left(\map{\bar{T}}{\alpha}{\alpha}\right)^t
    \end{equation*}
    \paragraph{Derivation of the above formula.}\footnote{Based on Example (5.3.6)} Let $V$ be fin-dim HIPS, we want to find a analogue to the
    transpose that we had in an ordinary euclidean space. Using the Lemma (5.3.5), we want to construct the complex transpose of some \trans{T}{V}{V}.
    Recall the definition, we want a $T^*$ such that it satisfies the eqaution $\langle T(\vv),\vw \rangle = \langle \vv,T^*(\vw) \rangle, \forall \vv,\vw \in V$.
    It suffices to construct a matrix for $T^*$ w.r.t some basis for $V$. If $\alpha = \{\vv_1,...\vv_n\}$ is an orthonormal basis for $V$, the $ij$-th entry of
    \trans{T^*}{\alpha}{\alpha} is the complex number $ \langle T^*(\vv_j),\vv_i\rangle $. We can then simplify the expression
    \begin{equation*}
        \langle T^*(\vv_j),\vv_i \rangle = \overbar{\langle \vv_i,T^*(\vv_j)\rangle} = \overbar{\langle T(\vv_i), \vv_j\rangle}
    \end{equation*}
    writing the above statement in terms of matrix yields us the desired formula. \qed

    \settag{5.3.8}
    \para{Definition.} Let $V$ be a finite dimensional HIPS. The adjoint of \trans{T}{V}{V} satisfies 
    $\langle T(\vv), \vw\rangle = \langle \vv,T^*(\vw)\rangle, \forall \vv,\vw\in V$.

    \settag{5.3.9}
    \para{Definition.} \trans{T}{V}{V} is called Hermitian or self-adjoint if $\langle T(\vu, \vv)\rangle =\langle \vu,T(\vv)\rangle,\forall \vw,\vv\in V$.
    Equivalently, $T$ is Hermitian or self-adjoint if $T = T^*$ or $\map{\bar{T}}{\alpha}{\alpha}^t = \map{T}{\alpha}{\alpha}$ for an orthonormal basis $\alpha$.
     An $n\times n$ complex matrix is called self-adjoint if $A = A^*$

    \settag{5.3.10}
    \para{Proposition.} If $\lambda$ is an eigenvalue of the self-adjoint linear transformation $T$, then $\lambda \in \real$. 
    In naive words, a self-adjoint transformation is similar to the ``symmetric" matrix in euclidean space, 
    where we have ``Symmetric matrices have real eigenvalues''.

    \settag{5.3.11}
    \para{Proposition.} If $\vu, \vv$ are eigenvectors, respectively, for the distinct eigenvalyes $\lambda, \mu$ 
    of \trans{T}{V}{V}, then $\vv, \vu$ are orthogonal, with $\langle \vv, \vu\rangle = 0$.

    \settag{5.3.12}
    \para{Theorem: Diagonalizability of HIPS.} Let \trans{T}{V}{V} be a self-adjoint transformation of a 
    complex vector space $V$ with Hermitian Inner Product. Then there is an orthonormal basis of $V$ 
    consisting of eigenvectors for $T$ and, in particular $T$ is diagonalizable.

    \settag{5.3.13}
    \para{Theorem: Spectral Decomposition of HIPS.} Let \trans{T}{V}{V} be self-adjoint transformation of a complex vector space 
    $V$ with hermitian inner product. Let $\lambda_1,...,\lambda_k\in \real$ be the distinct eigenvalues of 
    $T$, let $P_i$ be the orthogonal projections of $V$ onto the eigenspace $E_{\lambda_i}$, then
    \begin{enumerate}
        \item $T = \lambda_1P_1 + ... + \lambda_kP_k$, and
        \item $I = P_1 + ... + P_k$
    \end{enumerate}

\end{enumerate}

\chapter{Jordan Canonical Form}
\section{Triangular Form}
\begin{enumerate}
    \settag{6.1.2}
    \para{Definition of Invariant Subspace.} Let \trans{T}{V}{V} be a linear mappping. A subspace 
    $W\subset V$ is said to be stable or invariant under $T$ if $T(W)\subset W$

    \settag{6.1.4}
    \para{Propostion.} Let $V$ be a vector space, let \trans{T}{V}{V} be linear mapping and let 
    $\beta=\{\vx_1,...,\vx_n\}$ be a basis for $V$. Then \map{T}{\beta}{\beta} is upper-triangular 
    if and only if each of the subspaces $W_i = span\{ \vx_1,...,\vx_i \}$, that is, 
    the span of the first $i$ elements in the basis $\beta$ is invariant under $T$. One shall note that
    we have the following increasing sequence of inclusion
    \begin{equation*}
        \{\vzero\} \subset W_1 \subset W_2 \subset ... \subset W_n = V
    \end{equation*}

    \settag{6.1.5}
    \para{Definition.} We say that a linear mapping \trans{T}{V}{V} on a fin-dim vector space $V$ is triangularizable if there exists
    a basis $\beta$ such that \map{T}{\beta}{\beta} is upper-triangular. Notice that to show a linear mapping is triangularizable
    we need to show that there exists an incresing sequence of invariant subspaces
    \begin{equation*}
        \{\vzero\} \subset W_1 \subset W_2 \subset ... \subset W_n = V
    \end{equation*}

    \settag{6.1.6}
    \para{Proposition.} Let \trans{T}{V}{V}, and let $W \subset V$ be an invariant subspace. Then the characteristic polynomial
    of $T|_W$ divides the characteristic polynomial of $T$. \newline
    \proof \newline
    Let $\alpha = \{\vx_1,...,\vx_k\}$ be a basis for $W$, and extend $\alpha$ to the basis $\beta = \{ \vx_1,...,\vx_k, \vx_{k+1},...,\vx_k \}$ for V.
    Since $W$ is invariant under $T$, for each $i \leq k$, we have
    \begin{equation*}
        T(\vx_i) = a_{1i}\vx_1 + ... + a_{ki}\vx_k + 0 \vx_{k+1} + ... + 0\vx_m
    \end{equation*}
    hence the matrix of \map{T}{\beta}{\beta} has a block form composition
    \begin{equation*}
        \map{T}{\beta}{\beta} = 
        \begin{bmatrix}
            A & B \\
            0 & C
        \end{bmatrix}
    \end{equation*}
    where $A = \map{T|_W}{\alpha}{\alpha}$ is a $k \times k$ matrix and $B$ and $C$ include the coefficients in the expensions of
    $T(\vx_j)$, where $k+1 \leq j \leq n$. When we compute the characteristic polynomial of $T$ using this matrix, we have
    \begin{equation*}
        \det(T - \lambda I) = \det(A - \lambda I)\det(C - \lambda I)
    \end{equation*}
    where we notice that $\det(A - \lambda I)$ is just the characteristic polynomial of $T$. \qed

    \settag{6.1.8}
    \para{Theorem.} Let $V$ be a fin-dim vector space over a field $\field$, and let \trans{T}{V}{V} be alinear mapping. Then
    $T$ is triangularizable if and only if the characteristic polynomial of $T$ has $\dim(V)$ roots (counted with multiplicties) in
    the filed $\field$.

    \settag{6.1.9}
    \para{Important Remark: Superiority of Complex Field.} Notice that for the above theorem, the conclusion automatically holds 
    if we choose $\field = \complex$, since we can always find such mant solutions for the characteristic polynomial. Generally speaking,
    we have that
    \begin{equation*}
        \forall A \in M_{n\times n}(\complex),\,\,\text{A can be triangularized}
    \end{equation*}

    \settag{6.1.10}
    \para{Lemma.} Let \trans{T}{V}{V} be as in the theorem, and assume that the characteristic polynomial of $T$ has $n = \dim(V)$ 
    roots in $\field$. If $W \subsetneq V$ is an invariant subsspace under $T$, then there exists a vector $\vx \neq \vzero$ in $V$ such that 
    $\vx \neg \in W\wedge W + span\{\vx\}$ is also invariant under $T$.
\end{enumerate}


\end{document}
