% Meta data
\documentclass[oneside, 12pt]{book}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{url}
\usepackage{hyperref}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\qed}{\hfill $\mathcal{Q}.\mathcal{E}.\mathcal{D}.\dagger$}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\text{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand\aug{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
% Call settag{\ldots} first to initialize, and then \para{} for a new paragraph
\newcommand{\para}[1]{\item \tbf{#1}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\ve}{\mathbf{e}}
% For convenience, I am setting both of these to refer to the same thing.
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\be}{\mathbf{e}}


\pdfinfo{
   /Author (Tingfeng Xia)
   /Title  (MAT224 Linear Algebra: Definitions, Lemmas, Theorems, Corollaries and their related proofs)
   /CreationDate (D:20190107)
   /Subject (MAT224 Linear Algebra)
}

\title{%
  \textbf{MAT224 Linear Algebra}\\
  \large Definitions, Lemmas, Theorems, Corollaries \\
    and their related proofs}
\author{Tingfeng Xia}
\date{Winter 2019}

\begin{document}

\frontmatter
\maketitle
\newpage % make a new page for actual contents, possibly a content table
\mbox{}
\vfill
by Tingfeng Xia \\


Materials in this booklet are based heavily on Prof. Nicholas Hoell's lectures as well as A COURSE IN LINEAR ALGEBRA by David B. Damiano and John B. Little.\newline\newline
Items in this booklet are, in fact, very similar to those in the book and are intended to be used as a bullet point guide to (almost) all the knowledge points and should \tit{certainly not} be used as a substitute for the actual learning material. Please notice that I make\tit{ no promise} about the accuracy of statements appearing in these notes.\newline\newline
The course website could be found here:\newline \url{http://www.math.toronto.edu/nhoell/MAT224/}
\newpage

\tableofcontents

\mainmatter

\chapter{Vector Spaces}
\section{(Real) Vector Space}
\begin{enumerate}
    \settag{1.1.1}
    \item{\textbf{Definition of real vector space:}} A real vector space is a set $V$ together with
    \begin{enumerate}
        \item \textbf{Closure under vector addition:} an operation called vector addition, which for each pair of vectors $\mathbf{x}, \mathbf{y}\in V$ produces another vector in $V$ denoted $\mathbf{x} + \mathbf{y}$, (i.e. $\forall \mathbf{x}, \mathbf{y}\in V, \mathbf{x} + \mathbf{y} \in V$) and
        \item \textbf{Closure under scalar multiplication:} an operation called multiplication by a scalar (a real number), which for each vector $\mathbf{x}\in V$, an each scalar $c\in \mathbb{R}$ produces another vector in $V$ denoted $c\mathbf{x}$. (i.e. $\forall \mathbf{x}\in V, \forall c \in \mathbb{R}, c \mathbf{x} \in V$)
    \end{enumerate}
    Furthermore, the two operations must satisfy the following axioms:
    %\begin{enumerate}
        %\item For all vectors \textbf{x, y}, and \textbf{z} $\in V$, %(\textbf{x}+\textbf{y})+\textbf{x} 
    %\end{enumerate}
    \begin{enumerate}
        \item $\forall \mathbf{x}, \mathbf{y}, \mathbf{z} \in V, (\mathbf{x} + \mathbf{y}) + \mathbf{z} = \mathbf{x} + (\mathbf{y} + \mathbf{z})$
        \item $\forall \mathbf{v}, \mathbf{y} \in V, \mathbf{x} + \mathbf{y} = \mathbf{y} + \mathbf{x}$
        \item $\exists \mathbf{0} \in V s.t. \forall \mathbf{x} \in V, \mathbf{x} + \mathbf{0} = \mathbf{x}$ (Note that this property is a.k.a existence of additive identity)
        \item $\forall \mathbf{x} \in V, \exists (-\mathbf{x}) \in V~s.t.~\mathbf{x} + (-\mathbf{x}) = \mathbf{0}$ (Note that this property is a.k.a existence of additive inverse)
        \item $\forall \mathbf{x}, \mathbf{y} \in V, c \in \mathbb{R}, c(\mathbf{x} + \mathbf{y}) = c\mathbf{x} + c\mathbf{y}$
        \item $\forall \mathbf{x} \in V, c,d \in \mathbb{R}, (c + d)\mathbf{x} = c\mathbf{x} + d\mathbf{x}$
        \item $\forall \mathbf{x} \in V, c,d \in \mathbb{R}, (cd)\mathbf{x} = c(d\mathbf{x})$
        \item $\forall \mathbf{x} \in V, 1\mathbf{x} = \mathbf{x}$
    \end{enumerate}
    \tbf{Remark.} Note that here we are not explicitly defining a vector space to be non-empty, however, if a set ever fails to have a larger than zero cardinality, it must not be a vector space. This is a consequence of axiom $(c)$, the existence of the one and only zero vector in the space.
    
    \settag{1.1.6}
    \para{Propositions for a R-v.s.} Let $V$ be a vector space. Then 
    \begin{enumerate}
        \item The zero vector is unique. Note that it might not necessarily be actually the zero vector in $\mathbb{R}^n$ that we are somewhat used to use.\newline
        \proof \newline
        Suppose, for the sake of contradiction, that $\mathbf{a},\mathbf{b}$ are two \tit{different} zero vectors of the vector space $V$. Then, by the definition of zero vector, we have 
        \begin{equation*}
            \forall \mathbf{x}\in V,\mathbf{x} +\mathbf{a} = \mathbf{x} \land \mathbf{x} + \mathbf{b} = \mathbf{x}
        \end{equation*}
        Simple algebraic manipulation yields us $\mathbf{x} + \mathbf{a} = \mathbf{a} + \mathbf{x} \implies \mathbf{a} = \mathbf{b}$ $\longrightarrow\!\longleftarrow$ Contradiction! \qed
        
        \item $\forall \mathbf{x} \in V, 0\mathbf{x}=0$ \newline
        \proof \newline 
        We have $0\vx = (0+0)\vx =0\vx +0\vx$, by axiom 6. By axiom $(d)$, I know there exists a additive inverse of $0\vx$, so I subtract on both sides of the equation such additive inverse. This yields us $\mathbf{0} = 0\vx$ as wanted. \qed
        
        \item $\forall \mathbf{x} \in V, $ the additive inverse is unique. Note that it might not necessarily be actually just $(-1)$ times the vector in $\mathbb{R}^n$ that we are somewhat used to use. \newline
        \proof \newline
        Let $\vx\in V$, and let $(-\vx),(-\vx)'$ be two additive inverse of $\vx$. Then, on one hand, by axioms 1, 4 and 3 we have 
        \begin{align*}
            \vx + (-\vx) + (-\vx)'
            &= (\vx + (-\vx)) + (-\vx)' \\
            &= \mathbf{0} + (-\vx)' \\
            &= (-\vx)'
        \end{align*}
        On the other hand, by axiom 2, we have 
        \begin{align*}
            \vx + (-\vx) +(-\vx)' 
            &= \vx + (-\vx)' + (-\vx) \\
            &= (\vx + (-\vx)') + (-\vx) \\
            &= \mathbf{0} + (-\vx) \\
            &= -\vx
        \end{align*}
        Hence we conclude that $(-\vx) = (-\vx)'$, and this completes the proof. \qed
        
        \item $\forall \mathbf{x} \in V,~\forall c\in \mathbb{R}, ~(-c)\mathbf{x}=~-(c\mathbf{x})$ \newline
        \proof \newline
        We have
        \begin{align*}
            c\vx + (-c)\vx &= (c +~-c)\vx \\
            &= \mathbf{0}\vx \\
            &= \mathbf{0}
        \end{align*}
        Then we notice that equating the fist and the last of the equations above completes the proof. \qed
        
    \end{enumerate}
\end{enumerate}

\section{Sub-spaces}
    \begin{enumerate}
        \settag{1.2.4}
        \item \textbf{Lemma on functions in $C^0(\mathbb{R})$}. Note that by $C^n(\cdot)$ we mean the function in this set are all of $Class-n$. Let $f,g \in C^0(\mathbb{R)}, \text{let} ~c\in \mathbb{R}$. Then, 
        \begin{enumerate}
            \item $f+g\in C^0(\R)$, and
            \item $cf \in C^0(\R)$
        \end{enumerate}
        The proof of this lemma relies on limit theorems of calculus.\newline
        
        \settag{1.2.6}
        \item \textbf{(Intuitive) definition of (vector) subspace.} Let $V$ be a vector space and let $W\subseteq V$ be a subset. Then $W$ is a (vector) subspace if $W$ is a vector subspace itself under the operations of vector sum and scalar multiplication from $V$.
        
        \settag{1.2.8}
        \item \textbf{Quick check rule for a subspace.} Let $V$ be a vector subspace, and let $W$ be a \textbf{non empty} subset of $V$. Then $W$ is a subspace of $V$ \tit{if and only if} 
        \begin{equation*}
            \forall \mathbf{x}, \mathbf{y}\in W,~\forall c\in \R, \text{ we have } c\mathbf{x}+\mathbf{y}\in W
        \end{equation*}
        Notice that the \tit{if and only if} makes this theorem extremely powerful in the sense that it will save you a lot of time to come up with a dis-proofing counter example. However, do keep in mind that there are cases that this theorem is very hard to check, for example, Exercise 1.2, Question 3(c).
        
        \settag{1.2.9}
        \item \textbf{Remark on the necessary condition of non-emptiness of subspace.} According to the definition of vector space that we gave in 1.1.1, a vector space must contain an additive identity element, hence it is necessary that we ensure $W\subseteq V$(from 1.2.6) is not an empty set.
        
        \settag{1.2.13}
        \item \textbf{Theorem: Intersection of sub-spaces is a subspace.} Let $V$ be a vector space. Then the intersection of any collection of sub-spaces of $V$ is a subspace of $V$.
        
        \settag{1.2.14}
        \item \textbf{Corollary: Hyper planes in $\R^n$ are sub-spaces of $\R^n$.} Let $a_{ij}(1\leq i\leq m)$, let $W_i = \{(x_1, \ldots, x_n)\in \R^n|a_{i1}x_1 + \ldots + a_{in}x_n = 0,~\forall 1 \leq i \leq m\}$. Then $W$ is a subspace of $\R^n$.
        
    \end{enumerate}
    
\section{Linear Combinations}
    \begin{enumerate}
        \settag{1.3.1}
        \item \textbf{Definitions regarding L.C. and derived spans.} Let $S$ be a subset of a vector space $V$, that is $S\subseteq V$.
        \begin{enumerate}
            \item a \textit{linear combination} of vectors in $S$ is any sum $a_1\mathbf{x}_1 + \ldots + a_n\mathbf{x}_n$, where the $a_i \in \R$, and the $x_i \in S$.
            \item we define the $Span$ of a set of vectors as follows to consider the special case of $S\stackrel{?}{=}\emptyset \in V$.
            \newline \underline{\tit{Case1: $S\neq \emptyset$:}} In this case, we define $Span(S)$ to be all possible linear combinations using vectors in $S$.\newline
            \underline{\tit{Case2: $S= \emptyset$:}} In this case, we define $Span(S = \emptyset)=\{\mathbf{0}\}$. We call this the zero-space.
            
            \item If $W=Span(S)$, we say $S$ \textit{spans(\text{or} generates)} $W$.
        \end{enumerate}
        
        \settag{1.3.4}
        \item \textbf{Span of a  subset of a vector space is a subspace.} Let $V$ be a vector space and let $S$ be any subset of $V$. Then $Span(S)$ is a subspace of $V$.
        
        \settag{1.3.5}
        \item \textbf{Sum of sets(with application to subs-paces).} Let $W_1~\text{and}~W_2$ be sub-spaces of a vector space $V$. The sum of $W_1$ and $W_2$ is the set
        \begin{center}
            $W_1+W_2 :=\{\mathbf{x}\in V |\mathbf{x}=\mathbf{x_1}+\mathbf{x_2}, \text{for some } \mathbf{x_1}\in W_1, \mathbf{x_2} \in W_2\}$
        \end{center}
        We think of the sum of the two sub-spaces(the two sets) as the set of vectors that can be built up from the vectors in $W_1$ and $W_2$ by linear combinations. Conversely, the vectors in the set $W_1+W_2$ are precisely the vectors that can be broken down into the sum of a vector in $W_1$ and a vector in $W_2$. One may find it helpful to view this as an analogue to a Cartesian product of the two set with a new constraint on the result.
        
        \settag{1.3.6}
        \item \textbf{Example.} If $W_1 = \{(a_1, a_2)\in \R^2|a_2 = 0\}$ and $W_2 \{(a_1, a_2)\in \R^2|a_1 = 0\}$, then $W_1 + W_2= \R^2$, since every vector in $\R^2$ can be written as the sum of vector in $W_1$ and a vector in $W_2$. For instance, we have $(5, -6)=(0, 5)+(0, -6)$, and $(5, 0)\in W_1 \land (0, -6)\in W_2$
        
        
        \settag{1.3.8} 
        \item \textbf{Proposition: The sum of spans of sets is the span of the union of the sets.} Let $W_1 = Span(S_1)$ and $W_2 = Span(S_2)$ be sub-spaces of a\textit{(the same)} vector space $V$. Then $W_1 + W_2 = Span(S_1 \cup S_2)$. Notice that the proof of this gave the important idea of mutual inclusion in proving sets are equal to each other.
        
        \settag{1.3.9}
        \item \textbf{The sum of sub-spaces is also a subspace.} Let $W_1$ and $W_2$ be sub-spaces of a vector space $V$. Then $W_1 + W_2$ is also a subspace of $V$.\newline
        \underline{\textit{Proof:}}\newline
        It is clear that $W_1 + W_2$ is non-empty, since neither $W_1$ nor $W_2$ is empty. Let $\mathbf{x}, \mathbf{y}$ be two vectors in $W_1+W_2$, let $c\in \R$. By our choice of $\mathbf{x}\text{and } \mathbf{y}$, we have
        \begin{align*}
            c\mathbf{x} + \mathbf{y} & = c(\mathbf{x}_1 + \mathbf{x}_2) + (\mathbf{y_1} + \mathbf{y_2}) \\
            & = (c\mathbf{x}_1 + \mathbf{y}_1) + (c\mathbf{x}_2 + \mathbf{y}_2) \\
            &\in W_1+W_2
        \end{align*}
        Since $W_1$ and $W_2$ are sub-spaces of $V$, we have $(c\mathbf{x}_1 + \mathbf{y}_1) \in W_1\land (c\mathbf{x}_2 + \mathbf{y}_2)\in W_2$. Then by (1.2.8), we see that indeed $W_1 + W_2$ is a subspace of $V$. \qed
        
        \settag{1.3.10}
        \item \textbf{Remark.} In general, if $W_1$ and $W_2$ are sub-spaces of $V$, then $W_1 \cup W_2$ will not be a subspace of $V$. For example, consider the two sub-spaces of $\R^2$ given in example (1.3.6). In that case $W_1\cup W_2$ is the union of two lines through the origin in $\R^2$. 
        
        \settag{1.3.11}
        \item \textbf{Proposition.} Let $W_1$ and $W_2$ be sub-spaces of vector space $V$ and let $W$ be a subspace of $V$ such that $W\supseteq W_1 \cup W_2$, then $W\supseteq W_1 + W_2$. Informally speaking, this proposition saying: "$W_1+W_2$ is the smallest subspace containing $W_1\cup W_2$", i.e., Any subspace that contains $W_1\cup W_2$ must be a super set of $W_1 + W_2$. \newline
        \proof\newline
        We want to show: $W\supseteq W_1\cup W_2\Longrightarrow W\supseteq W_1 + W_2$\newline
            \begin{align*}
                \text{Assume that } \text{$W\supseteq W_1 \cup W_2$. Let $w_1\in W_1,~w_2\in W_2$.}\\
                \text{We notice that } w_1,~w_2\in W_1\cup W_2 \subseteq W \\
                \implies& w_1, w_2\in W \\
                \text{(Since $W$ is a subspace, so it is closed under addition)}\\
                \implies& w_1 + w_2 \in W\\
                \implies W_1 + W_2 \subseteq W \iff& W \supseteq W_1 + W_2 
            \end{align*}
            \qed
        
    \end{enumerate}

\section{Linear (In)dependence}
    \begin{enumerate}
        \settag{1.4.2}
        \item \textbf{Algebraic definition of linear dependence.} Let $V$ be a vector space, and let $S\subseteq V$.
            \begin{enumerate}
                \item A \textit{linear dependence} among the vectors of $S$ is an equation $a_1\mathbf{x} + \ldots + a_n\mathbf{x}_n = \mathbf{0}$ where the $x_i\in S$, and the $a_i\in \R$ are not all zero(i.e., at least one of the $a_i\neq 0$). In familiar\footnote{Familiar from MAT223, Prof. Jason Siefken's IBL(Inquiry Based Learning) notes.} words, there exists a non-trivial solution to the equation mentioned above.
                \item the set $S$ is said to be \textit{linearly dependent} if there exists a linear dependence among the vectors in $S$.
            \end{enumerate}
            \tbf{Remark: }It can be shown that the geometric\footnote{A set of vectors is said to be dependent of each other there exists a vector in this set, that it is in the Span of all other vectors in the set. I.e., There is some vectors in this set that are "redundant", it's position can be taken by some linear combination of the other vectors in the set.} definition and this are, in-fact, equivalent to each other. I will now produce the proof. \newline
        \tit{\underline{Proof of equivalence of definitions:}} \newline
        Let $V$ be a vector space, and let $S\subseteq V$. Consider the following equation:
        \begin{align*}
            a_1\mathbf{x}_1 + a_2\mathbf{x}_2 + \ldots + a_n\mathbf{x}_n &= 0\text{, where } \exists a_i\neq 0 \\
            \text{(WLOG, assume that } a_n &\neq \mathbf{0}\text{)}\\
            \implies \frac{a_1\mathbf{x}_1 + a_2\mathbf{x}_2 + \ldots + a_n\mathbf{x}_n}{a_n}&=\mathbf{0}\\
            \implies \mathbf{x}_n &= -\sum_{i=1}^{n-1}a_i\mathbf{x}_i
        \end{align*}
        Notice that the result $\mathbf{x}_n$ is in terms of all the other $(n-1)$ vectors in the set, hence a linear combination of those vectors, and this completes the proof.
        \qed\newline
        \textbf{Re-Remark: }We can also use this proof as an argument towards the following problem: Show that at least one of the vectors in a linearly dependent set is redundant. We could take take a similar proof and argue that the linear combination could be written without at least one of the vectors.
        
        \settag{1.4.4}
        \item \textbf{Algebraic definition of linear independence.} Let $V$ be a vector space, and $S\subseteq V$. Then $S$ is \textit{linearly independent} if whenever we have $a_i\in \R$ and $x_i\in S$ such that $a_1\mathbf{x}_1+\ldots+a_n\mathbf{x}_n = \mathbf{0}$, then $a_i = 0,~\forall i$. A more conceivable way to understand this is if the aforementioned equation exists and only exists a set of trivial solution then the vectors involved in the equation are \textit{linearly independent}. \newline \tbf{Remark: }A set of vector is linearly independent \tit{if and only if} it is not linearly dependent.
        
        \settag{1.4.7}
        \item \textbf{Propositions regarding linear (in)dependency.}
            \begin{enumerate}
                \item Let $S$ be a linearly dependent subset of a vector space $V$, and let $S'$ be another subset of $V$ that contains $S$. Then $S'$ is also linearly dependent.
                \item Let $S$ be a linearly independent subset of vector space $V$ and let $S'$ be another subset of $V$ that is contained in $S$. Then $S'$ is also linearly independent.
            \end{enumerate}
        \underline{\tit{Proof of (a):}}
        Since $S$ is linearly dependent, there exists a linear dependence among the vectors in $S$, say, $a_1\mathbf{x}_1 + \ldots+ a_n\mathbf{x}_n = \mathbf{0}$. Since $S$ is contained in $S'$, this is also a linear dependence among the vectors in $S'$. Hence $S'$ is linear dependent.\qed\newline
        \underline{\tit{Proof of (b):}}
        Consider any equation $a_1\mathbf{x}_1 + \ldots+ a_n\mathbf{x}_n = \mathbf{0}$, where the $a_i \in \R,~\mathbf{x}_i \in S'$. Since $S'$ is contained in $S$, we can also view this as a potential linear dependence among vectors in $S$. However, $S$ is linearly independent, so it follows that all the $a_i = 0\in \R$. Hence $S'$ is also linearly independent.\qed
        
        \settag{1.4.*}
        \item \tbf{Example of showing linear independence.(1)} Show that the set $\{1,x,x^2,x^3,\ldots,x^n\}$ is linearly independent. We consider the following equation:
        \begin{equation*}
            0 = a_0 + a_1x + \ldots + a_{n-1}x^{n-1}+a_n x^n~~~~~~(*)
        \end{equation*}
        Then we take the derivative:
        \begin{equation*}
            \frac{d^n}{dx^n}(*)= 0 = n!\cdot a_n
        \end{equation*}
        Then since $n! \neq 0$, we know $a_n=0$. We repeat the same process, taking derivatives $(n-1)$ times w.r.t. $x$ we get $a_{n-1}=0$ and so on. As a last step we have:
        \begin{equation*}
            \frac{d}{dx}(*) = 0 = 1!\cdot a_1 \implies a_1 = 0
        \end{equation*}
        Then we have $0 = 0 + 0 +\ldots + a_0 \implies a_0 = 0$. So the equation has and only has a trivial solution, thus the set $\{1,x,x^2,x^3,\ldots,x^n\}$ is linearly independent. \qed
        
        \settag{1.4.*}
        \item \tbf{Example of showing linear independence.(2)} Show that the set $\{e^x, e^{2x}\}$ is linearly independent. We consider the following equation:
        \begin{equation*}
            0 = ae^x + be^{2x}~~~~~(1)
        \end{equation*}
        We take derivative on both sides of $(1)$ w.r.t. $x$ and we have:
        \begin{equation*}
            0 = ae^x + 2be^{2x}~~~~(2)
        \end{equation*}
        We subtract $(2)$ from $(1)$ to get:
        \begin{equation*}
            0 = be^{2x} \implies b = 0 \implies 0 = ae^x + 0 \implies a = 0
        \end{equation*}
        Since $a=b=0$ is the one and only solution to $(1)$, we claim they are linearly independent. \qed
    \end{enumerate}
    
\section{Interlude on solving SLEs}
    \begin{enumerate}
        \settag{1.5.*}
        \item \tbf{Note Aside: }This section of the book is covered, although not rigorously but completely, in MAT223. Hence the vast majority of definitions and corollary in this section were omitted. Consult the book for more detail on this.
        
        \settag{1.5.1}
        \item \tbf{Definition of (homogeneous) SLEs\footnote{System of Linear Equations}} A system of $m$ equations in $n$ unknowns $x_1,\ldots,x_n$ of the form:
        \begin{align*}
            a_{11}\mathbf{x_1}+\ldots+a_{1n}\mathbf{x_n} &= b_1 \\
            a_{21}\mathbf{x_1}+\ldots+a_{2n}\mathbf{x_n} &= b_2 \\
            &\mbox{\ldots} \\
            a_{m_1}\mathbf{x_1}+\ldots+a_{mn}\mathbf{x_n} &= b_m
        \end{align*}
        where the $a_{ij},~b_i\in \R$, is called a \tit{system of linear equations.} The scalars $a_{ij}$ are called coefficients or \tit{weights} of the equations. We call this system \textbf{homogeneous} if and only if all the $b_i$ are 0's.
        
        \settag{1.5.2}
        \item \tbf{Definition of equivalent SLEs} Two systems of linear equations are said to be equivalent if their sets of solutions are the same(i.e., mutual inclusion of the two solution sets)
        
        \settag{1.5.3}
        \item \tbf{Propositions on operations on SLEs\footnote{This should be familiar from MAT223, operations involved in row reducing an augmented matrix for a system of linear equations}}
        \begin{enumerate}
            \item The system obtained buy adding any multiple of any one equation to any second equation, while leaving the other other equations unchanged, is an equivalent system.
            \item The system obtained by multiplying any one equation by a non-zero scalar and leaving the other equations unchanged is an equivalent system.
            \item The system obtained by interchanging any two equations is an equivalent system.
        \end{enumerate}
        
        \settag{1.5.13}
        \item \tbf{Corollary} If $m<n$, every homogeneous system of $m$ linear equations in $n$ unknowns has a non-trivial solution.
    \end{enumerate}
    

\section{Bases and Dimension}
    \begin{enumerate}
        \settag{1.6.1}
        \item \tbf{Definition of basis.} A subset $S$ of a vector space $V$ is called a basis if $V=Span(S)$ and the set $S$ is linearly independent.
        
        \settag{1.6.3}
        \item \tbf{Theorem. }Let $V$ be a vector space, and let $S$ be a non-empty subset of $V$. Then $S$ is a basis of $V$ if and only if $\forall \mathbf{x} \in V, \mathbf{x}$ can be written \tit{uniquely} as a linear combination of the vectors in $S$.
        
        \settag{1.6.6}
        \item \tbf{Theorem. }Let $V$ be a vector space that has a finite spanning set, and let $S$ be a linearly independent subset of $V$. Then there exists a basis $S'$ of $V$, such that $S\subseteq S'.$ Note that this theorem is can be summarized as follows: Every linearly independent set of vectors could be extended to a basis. We do so by adding yet another vector that is linearly independent to all the vectors already in the set, but notice that this process should possibly be repeated but \tit{not} infinite.
        
        \settag{1.6.8}
        \item \tbf{Lemma on linear independence towards a set and a vector.} Let $S$ be a linearly independent subset of $V$ and let $\mathbf{x}\in V$, but $\mathbf{x} \notin S$. Then $S\cup \{\mathbf{x} \}$ is linearly independent if and only if $\mathbf{x} \notin Span(S)$.
        
        \settag{1.6.10}
        \item \tbf{Theorem on cardinality of linearly independent set.} Let $V$ be a vector space and let $S$ be a spanning set for $V$, which has $m$ elements. Then no linearly independent set in $V$ can have more than $m$ elements.\newline
        \proof\newline
        To show that there are no linearly independent set in $V$ that can have more than $m$ elements in it, it suffices to show that every set in $V$ that has more than $m$ elements in it is linearly dependent. Let $S=\{y_1,\ldots,y_m\}$ and $S'=\{y_1,\ldots,y_n\}\subset V$ where $n>m$. Now we consider the following equation:
        \begin{equation}
            a_1\mathbf{x}_1 + \ldots + a_n\mathbf{x}_n = \mathbf{0}~~~~\text{    where }\mathbf{x}_i \in S',a_i\in \R
        \end{equation}
        Since $S$ is a spanning set for $V$, then $\exists b_{ij}\in \R,~s.t.~\forall 1\leq i \leq m$:
        \begin{equation*}
            \mathbf{x}_i = b_{i1}\mathbf{y}_1 + \ldots + b_{im}\mathbf{y}_m = \sum_{j=1}^{m}b_{ij}\mathbf{y}_j
        \end{equation*}
        Then we substitute the $\mathbf{x}_i$'s into equation (1.1):
        \begin{equation}
            a_1\left(\sum_{j=1}^{m}b_{1j}\mathbf{y}_j\right) + \ldots +  a_n\left(\sum_{j=1}^{m}b_{nj}\mathbf{y}_j\right) = \mathbf{0}
        \end{equation}
        Rearranging (1.2), we have:
        \begin{equation}
            \left(\sum_{j=1}^{n}b_{1j}a_{j}\right)\mathbf{y}_1 + \ldots +  \left(\sum_{j=1}^{n}b_{mj}a_{j}\right)\mathbf{y}_m = \mathbf{0}
        \end{equation}
        Now we consider the following SLE(The coefficients of (1.3)):
        \begin{center}
            $
            \begin{cases}
            b_{11}a_1 + \ldots + b_{1n}a_n &= 0 \\
            b_{21}a_1 + \ldots + b_{2n}a_n &= 0 \\
            &\vdots \\
            b_{m1}a_1 + \ldots + b_{mn}a_n &= 0
            \end{cases}
            $
        \end{center}
        If we can find a set of $a_1, \ldots, a_n$ that are not all zero that solves the above SLE, then those scalars will give us a linear dependence among the vectors in $S'$ in (1.1). But the above SLE is a system of homogeneous linear equations in the $n$ unknowns $a_i$'s, hence since $m < n$, by Corollary (1.5.13), there exists a non-trivial solution. Then $S'$ is linearly dependent and this completes the proof. \qed
        
        
        \settag{1.6.11}
        \item \tbf{Bases of a vector space shall have same cardinality.} Let $V$ be a vector space and let $S$ and $S'$ be two bases of  with $m$ and $m'$ elements, respectively. Then $m=m;$. \newline
        \tit{\underline{Proof: (Euclid's style)}} \newline
        Let $S$ and $S'$ with properties given as above. Since they are bases, we know: (By Theorem 1.6.10)
        \begin{enumerate}
            \item $S$ is a spanning set and $S'$ is a linearly independent set \newline $\Longrightarrow$ $m'=|S'| \leq |S|=m$
            \item $S'$ is a spanning set and $S$ is a linearly independent set \newline $\Longrightarrow$ $m=|S| \leq |S'|=m'$
        \end{enumerate}
        Then, $m'=m$. \qed
        
        \settag{1.6.12}
        \item \tbf{Definitions of (in)finite-dimension}
        \begin{enumerate}
            \item If $V$ is a vector space with some finite basis (possibly empty), we sat $V$ is \tit{finite-dimensional}. We say the vector space is \tit{infinite-dimensional} otherwise.
            \item Let $V$ be a \tit{finite-dimensional} vector space. The dimension of $V$, denoted as dim($V$), is the number of elements in a (hence any) basis of $V$.
            \item If $V=\{\mathbf{0}\}$, we define dim($V$)$=0$. \tbf{Remark: }Please note that is consistent with our definition of span of a empty set. We defined the span of empty set to be $\{\mathbf{0}\}$, and hence $\{\mathbf{0}\}$ is the resulting space of all the possibly linear combination of vectors in the empty set(where there is none). Then the cardinality of the empty set(which is zero) is here defined as the dimension of the zero vector space.
        \end{enumerate}
        
        \settag{1.6.13}
        \item \tbf{Examples on dimensions of vector spaces. }If we have a basis of $V$, then computing dim($V$) is simply a matter of counting the number of vectors in a (hence any) basis for the vectors space. We explore this by looking at the following examples
        \begin{enumerate}
            \item For each $n$, dim($\R^n$) $=n$. This is a consequence of the standard basis for $\R^n$ is of the form: $\{\mathbf{e}_1, \ldots, \mathbf{e}_n\}$ has $n$ elements in it.
            \item dim($P_n(\R)$) $=n+1$, since $|\{1, x^1, x^2, \ldots, x^n\}|=n+1$.\footnote{The absolute value marks around a set returns the cardinality of the set.}
            \item The vector spaces $P(\R)$\footnote{Here $P(\R)$ means the vector space of all polynomials that are $\R \xrightarrow{} \R$.}$,~C^n(\R)$, where $n\in \mathbb{N}^{\geq 0}$, are not of finite dimension, and hence are called \tit{infinite dimensional}.
        \end{enumerate}
        
        \settag{1.6.14}
        \item \tbf{Corollary. }Let $W$ be a subspace of a finite-dimensional vector space $V$. Then dim($W$) $\leq$ dim($V$). Furthermore, dim($W$) $=$ dim($V$) if and only if $W=V$.
        
        \settag{1.6.15}
        \item \tbf{Corollary. }Let $W$ be a subspace of $\R^n$ defined by a system of homogeneous linear equations. The dim($W$) is equal to the number of free variables in the corresponding echelon form of the equations.\newline
        \tbf{Generalization.} It is also worth pointing out that by setting the free variables equal to one in turn\footnote{By in turn we mean set one of them to one and all others zero, one at a time}, we can always generate a basis for the subspace. Referring to example $(1.6.16)$ in the book, we see that we have two free variables, and the two (linearly independent) vectors obtained from the method described above that form a 2-dimensional subspace of $\R^5$. 
        
        
        \settag{1.6.18}
        \item \tbf{Inclusion-Exclusion principle of dimensions.} Let $W_1$ and $W_2$ be finite dimensional sub-spaces of a vector space $V$. Then,
        \begin{center}
            dim($W_1+W_2$) = dim($W_1$) $+$ dim($W_2$) $-$ dim($W_1\cap W_2$)
        \end{center}
        \tbf{Remark:} This theorem could not be generalized to higher dimensions as does the Inclusion-Exclusion Principle in set theory. (Consequence of the challenge problem of Tutorial 2), more information can be found \href{https://www.jstor.org/stable/24337937?seq=1#metadata_info_tab_contents}{\underline{here}}, on a paper of generalizing this formula. 
        
    \end{enumerate}
    
\chapter{Linear Transformations}
\section{Linear Transformations}

    \begin{enumerate}
        \settag{2.1.1}
        \item \tbf{Definition of linear transformation.\footnote{Familiar from MAT223}} A function $T:V\xrightarrow{}W$ is called a \tit{linear mapping} or a \tit{linear transformation} if it satisfies:
        \begin{enumerate}
            \item $\forall \mathbf{u},\mathbf{v}\in V,~T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{u})$
            \item $\forall \alpha \in \R, \mathbf{v} \in V,~ T(\alpha\mathbf{v}) = \alpha T(\mathbf{v})$
        \end{enumerate}
        
        \settag{2.1.2}
        \item \tbf{Proposition: alternative definition of L.T.s} A function $T:V\xrightarrow{}W$ is a linear transformation if and only if 
        \begin{equation*}
            \forall \alpha,\beta \in \R, \forall \mathbf{u}, \mathbf{v} \in V, T(\alpha \mathbf{u}+\beta \mathbf{v})=\alpha T(\mathbf{u}) + \beta T(\mathbf{v}).
        \end{equation*}
        \tit{\underline{Proof($\implies$):}}\newline
        Assuming that $T$ is a linear transformation, then the definition in (2.1.1) must satisfy. Let $\alpha,\beta \in \R, \mathbf{u},\mathbf{v}\in V$, then: 
        \begin{align*}
            T(\alpha\mathbf{u} + \beta\mathbf{v}) &= T(\alpha\mathbf{u}) + T(\beta\mathbf{u})~~~~\text{// by (a)} \\
            &= \alpha T(\mathbf{u}) + \beta T(\mathbf{v})~~~~\text{// by (b)}
        \end{align*}
        \tit{\underline{Proof($\impliedby$):}}\newline
        Assuming the alternative definition, we want to show the definition given in (2.1.1). Since the quantifier is $\forall$, we can take $\alpha = \beta  =1, \text{(arbitrary)}~\mathbf{u},\mathbf{v}\in V$ in the alternative definition which directly yields us (a) of (2.1.1). Then we take $\alpha \in \R, \beta =0, \mathbf{u}, \mathbf{v}\in V$. In this case, we want to show that $T(\alpha \mathbf{u}) = \alpha T(\mathbf{u})$. We proceed as follows: 
        \begin{align*}
            T(\alpha\mathbf{u} + \beta\mathbf{v}) &= T(\alpha\mathbf{u} + 0\mathbf{v}) = T(\alpha\mathbf{u})\\
            &= \alpha T(\mathbf{u}) + 0T(\mathbf{u}) \\
            &= \alpha T(\mathbf{u})
        \end{align*}
        Since $\alpha\in \R, \mathbf{u}\in V$ are arbitrary, this completes the proof. \qed
        
        \settag{2.1.3}
        \item \tbf{Corollary.} A function $T:V\xrightarrow{}W$ is a linear transformation if and only if
        \begin{equation}
            \forall a_1,\ldots,a_k\in \R, \forall \mathbf{v}_1, \ldots,\mathbf{v}_k\in V:~ 
            T\left(\sum_{i=1}^{k}a_i\mathbf{v}_i\right) = \sum_{i=1}^{k}a_i T\left(\mathbf{v}_i\right)
        \end{equation}
        \proof To show this if and only if relationship, we have to consider the implication of both directions. Since $(2.1.2)$ is just a special case of $(2.1.3)$ then we are done in proving $(2.1.3)\implies (2.1.2)$. We will prove the other direction by mathematical induction (on $k$) to generalize the case of $k=2$ to arbitrary $k\in \double{N}^{\geq 2}$. \newline
        Define predicate $P(k): \double{N}^{\geq 2} \rightarrow{} \{0, 1\}~\text{as ``}(2.1)~\text{holds"}$. \newline
        Claim that $\forall k \in \double{N}^{\geq 2}, P(k)$.\newline
        \tbf{\underline{BASIS CASE:}} $k = 2$. In this case the wanted equality is the same as the one proved in $(2.1.2)$, so $P(2)$. \newline
        \tbf{\underline{INDUCTIVE STEP:}} Let $k\in \double{N}^{\geq 2}$, assume $P(k-1)$, we want to show that $P(k)$ follows.\newline
        \begin{align*}
            T\left(\sum_{i=1}^{k}a_i\mathbf{v}_i\right) &= T\left(a_k\mathbf{v}_k + \sum_{i=1}^{k-1}a_i\mathbf{v}_i\right) \\
            &= a_k T(\mathbf{v}_k) + T\left(\sum_{i=1}^{k-1}a_i\mathbf{v}_i\right)~~~~\text{// By $(2.1.1)$} \\
            &= a_k T(\mathbf{v}_k) + \sum_{i=1}^{k-1}a_i T\left(\mathbf{v}_i\right)~~~~~~\text{// By $P(k-1)$}\\
            &= \sum_{i=1}^{k}a_i T\left(\mathbf{v}_i\right)
        \end{align*}
        So $P(k)$ follows in this case.
        \qed
        
        \settag{2.1.9}
        \item \tbf{Angle between two vectors.} If $\mathbf{0}\neq \mathbf{a}\in \R^2$ and $\mathbf{0}\neq \mathbf{b}\in \R^2$, then the angle $\theta$ between them must be\footnote{The angle brackets here denotes the inner product of vectors}
        \begin{equation*}
            \theta = \arccos\left( \frac{\left<\mathbf{a},\mathbf{b}\right>}{\lVert\mathbf{a}\rVert \cdot \lVert\mathbf{b}\rVert}\right)
        \end{equation*}
        \tbf{Remark.} Notice that this definition could also be extended to $\R^3$.
        
        \settag{2.1.10}
        \item \tbf{Corollary on orthogonality of vectors.} If $\R^2\ni \mathbf{a}\neq \mathbf{0}$ and $\R^2\ni \mathbf{b}\neq \mathbf{0}$, then the angle $\theta$ between them is a right angle if and only if $\left<\mathbf{a},\mathbf{b}\right>=0$.\newline
        \tbf{Remark.} Note this definition of orthogonality could be extended to all euclidean vector spaces. The orthogonal complement of a subspace is the space of all vectors that are orthogonal to every vector in the subspace. In a three-dimensional Euclidean vector space, the orthogonal complement of a line through the origin is the plane through the origin perpendicular to it, and vice versa. In four-dimensional Euclidean space, for example, the orthogonal complement of a line is a hyper-plane and vice versa, and that of a plane is a plane.
        
        \settag{2.1.14}
        \item \tbf{Proposition.} If $T:V\xrightarrow{}W$ is a linear transformation and $V$ is finite-dimensional, then $T$ is uniquely determined by its values on the members of a basis of $V$. To make this proposition more clear, we present the proof below. We will show that if $S$ and $T$ are linear transformations that take the same values on each member of a basis for $V$, then in fact $S=T$.\newline
        \proof \newline
        Let $\{ \mathbf{v}_1, \ldots, \mathbf{v}_k\}$ be a basis for $V$, and let $S$ and $T$ be two linear transformations that satisfy $T(\mathbf{v}_1)=S(\mathbf{v}_1), \forall i \in \{1,\ldots,k\}$. If $\mathbf{v}\in V, ~\mathbf{v} = a_1\mathbf{v}_1 + \ldots + a_k\mathbf{k}$, then
        \begin{align*}
            T(\mathbf{v}) &= T(a_1\mathbf{v}_1 + \ldots + a_k\mathbf{k}) \\
            &= a_1T(\mathbf{v}_1) + \ldots + a_k T(\mathbf{v}_k)~~~~\text{// Since $T$ is linear}\\
            &= a_1S(\mathbf{v}_1) + \ldots + a_k S(\mathbf{v}_k) \\
            &= S(a_1\mathbf{v}_1 + \ldots + a_k\mathbf{k})~~~~~~~~~~\text{//  Since $S$ is linear} \\
            &=S(\mathbf{v})
        \end{align*}
        Hence $S$ and $T$ are equal as mappings from $V$ to $W$. \qed
        
    \end{enumerate}
    
\section{Linear Transformations between finite dimensional vector spaces}
    \begin{enumerate}
        \settag{2.2.1}
        \item \tbf{Proposition.} Let $T:V\xrightarrow{}W$ be a linear transformation between the finite dimensional vector spaces $V$ and $W$. If $\{\mathbf{v}_1,\ldots,\mathbf{v}_k\}$ is a basis for $V$ and $\{\mathbf{w}_1,\ldots,\mathbf{w}_l\}$ is a basis for $W$, then $T:V\xrightarrow{}W$ is uniquely determined by the $l\times k$ scalars used to express $T(\mathbf{v}_j)$, where $j\in \{1,\ldots,k\}$, in terms of $\mathbf{w}_1,\ldots,\mathbf{w}_l$.
        
        \settag{2.2.6}
        \item \tbf{Matrix of a transformation w.r.t.$\alpha,\beta$.} Let $T:V\xrightarrow{}W$ be a linear transformation between finite-dimensional vector spaces $V$ and $W$, and let $\alpha = \{\mathbf{v}_1,\ldots,\mathbf{v}_k \}$ and $\beta = \{\mathbf{w}_1,\ldots,\mathbf{w}_l\}$, respectively, be any basis for $V$ and $W$. 
        Let $a_{ij}$, where $1\leq i \leq l$ and $1\leq j \leq k$ be the $l\cdot k$ scalars that determined $T$ with respect to the bases $\alpha$ and $\beta$. The matrix whole entries are the scalars $a_{ij}$, given above, is called the \tit{matrix of the linear transformation $T$ with respect to the bases $\alpha$ for $V$ and $\beta$ for $W$}. We denote such matrix with following notation: $\left[T\right]^\beta_\alpha$. \newline 
        \tbf{Notice that}, again, $\alpha$ is a basis for $V$, the domain of the transformation and $\beta$ is a basis for $W$, the co-domain of the transformation.
        \newline
        \tbf{Remark.} This should be familiar from MAT223, where we perform transformation on each and every element in a basis for a space, and transcribe them, in terms of another basis, into a standard matrix for the transformation. In terms of standardized formulaic calculations, we have
        \begin{equation*}
            \left[T\right]_\alpha^\beta =
            \begin{bmatrix}
                \left[T(\alpha_1) \right]_\beta & \ldots & \left[T(\alpha_n) \right]_\beta
            \end{bmatrix}
        \end{equation*}
        where the $\alpha_i$'s denotes the $i$-th element of the $\alpha$ basis, and is itself a vector.
        
        \settag{2.2.15}
        \item \tbf{Proposition: Linear Transformation on alternative basis.} Let $T:V\xrightarrow{} W$ be a linear transformation between vector spaces $V$ of dimension $k$ and $W$ of dimension $l$. Let $\alpha = \{\mathbf{v}_1,\ldots,\mathbf{v}_k\}$ be a basis for $V$, and let $\beta = \{\mathbf{w}_1,\ldots,\mathbf{w}_l\}$ be a basis for $W$. Then for each $\mathbf{v} \in V$, we have the following:
        \begin{equation*}
            \left[T(\mathbf{v})\right]_\beta = \left[T\right]^\beta_\alpha\left[\mathbf{v}\right]_\alpha
        \end{equation*}
        One can think about this in the sense that the same transformation could have been accomplished under another basis. We just have to convert the original problem into some easy to solve basis, perform the transformation under that basis, and then convert the result back. We now present the proof for this proposition.\newline
        \proof Let $\mathbf{v}=x_1\mathbf{v}_1 + \ldots + x_k\mathbf{v}_k\in V$. Then if $T(\mathbf{v}_j)=a_{1j}\mathbf{w}_1 + \ldots + a_{lj}\mathbf{w}_l$, 
        \begin{align*}
            T(\mathbf{v}) &= \sum_{j=1}^k x_j T(\mathbf{v}_j) \\
            &= \sum_{j=1}^k x_j \left(\sum_{i=1}^{l}a_{ij}\mathbf{w}_i\right) \\
            &= \sum_{i=1}^l\left(\sum_{j=1}^k x_j a_{ij} \right)\mathbf{w}_i
        \end{align*}
        Thus the $i$-th coefficient of $T(\mathbf{v})$ in terms of $\beta$ is $\sum_{j=1}^k x_j a_{ij}$, and
        \begin{equation*}
            \left[T(\mathbf{v})\right]_\beta = 
            \begin{bmatrix}\sum_{j=1}^kx_ja_{1j} 
            \\ \dots 
            \\ \dots 
            \\ \dots 
            \\ \sum_{j=1}^kx_ja_{lj}
            \end{bmatrix}
            =
            \begin{bmatrix}
            a_{11} & \cdots & a_{1k} \\
            \vdots & \ddots & \vdots \\
            a_{l1} & \cdots & a_{lk}
            \end{bmatrix}
            \begin{bmatrix}
            a_1 \\ \vdots \\ a_k
            \end{bmatrix}
            =\left[T\right]_\alpha^\beta\left[\vv\right]_\alpha
        \end{equation*}
        \qed
        
        \settag{2.2.18}
        \item \tbf{Proposition: Property of matrix \tit{times} L.C. of vectors.} Note that in this proposition, we assume the vectors and the matrix are compatible\footnote{i.e., They can \tit{always} perform the operations that we want}. Let $A$ be an $l\times k$ matrix and $\mathbf{u}$ and $\mathbf{v}$ be column vectors with $k$ entries. Then,
        \begin{equation*}
            \forall~ \text{pairs of}~a\in\R, b\in\R, A(a\mathbf{u} + b\mathbf{v}) = aA\mathbf{u} + bA\vv
        \end{equation*}
        \proof Since $\mathbf{u}, \mathbf{v}\in \R^k$, let them be $\mathbf{u} = (u_1,\ldots,u_k)^T$ and $\mathbf{v} = (v_1,\ldots,v_k)^T$.\newline 
        Fix $a,b\in \R$. Then,
        \begin{align*}
            A\left(a\mathbf{u} + b\mathbf{v}\right) &=  A\left(a\left(u_1,\ldots,u_k\right)^T + b\left(v_1,\ldots,v_k\right)^T\right) \\
            &=  A\left(\left(a\cdot u_1,\ldots,a\cdot u_k\right)^T + \left(b \cdot v_1,\ldots,b\cdot v_k\right)^T\right) \\
            &=  A\left(a\cdot u_1 + b \cdot v_1,\ldots,a\cdot u_k + b\cdot v_k \right)^T \\
            &\text{\%TODO: change this vector notation and finish the proof..}
        \end{align*}
        
        \settag{2.2.19}
        \item \tbf{Proposition.} Let $\alpha = \{\vv_1,\ldots,\vv_k\}$ be a basis for $V$ and $\beta= \{\vw_1,\ldots,\vw_l\}$ be a basis for $W$, and let $\mathbf{v}=x_1\vv_1+\ldots+x_k\vv_k\in V$.
        \begin{enumerate}
            \item If $A$ is an $l \times k$ matrix, then the function $T(\mathbf{v}) = \mathbf{w}$, where $\left[\vw\right]_\beta A\left[\vv\right]_\alpha$ is a linear transformation.
            \item If $A = [S]_\alpha^\beta$ is the matrix of a transformation $S:V\xrightarrow{} W$, then the transformation $T$ constructed from $[S]_\alpha^\beta$ is equal to $S$.
            \item If $T$ is the transformation of (a) constructed from $A$, then $[T]_\alpha^\beta = A$.
        \end{enumerate}
        
        \settag{2.2.20}
        \item \tbf{Proposition.} Let $V$ and $W$ be finite-dimensional vector spaces. Let $\alpha$ be a basis for $V$ and $\beta$ a basis for $W$. Then the assignment of a matrix to a linear transformation from $V$ to $W$ given by $T$ goes to $[T]_\alpha^\beta$ is bijective\footnote{Injective and surjective}.
    \end{enumerate}
    
\section{Kernel and image}
    \begin{enumerate}
        \settag{2.3.1}
        \item \tbf{Definition of Kernel.} The \tit{kernel} of $T$, denoted Ker($T$), is the subset of $V$ consisting of all vectors $\vv\in V$ such that $T(\vv) = \mathbf{0}$. Writing in familiar set builder notation:
        \begin{equation*}
            \text{Ker}(T) := \{\vv \in V | T(\vv) = \mathbf{0}\}
        \end{equation*}
        One should notice the difference between the familiar Null Space of a transformation and the Kernel here. Kernel is defined for all vector spaces, however, null-spaces are for $\R^n$ only.
        
        \settag{2.3.2}
        \item \tbf{Proposition: Kernel is a subspace.} Let $T: V\xrightarrow{} W$ be a linear transformation. Ker($T$) is a subspace of $V$. \newline
        \proof \newline
        Since Ker($T$) $\subset V$, it suffices to show that Ker($T$) is closed under addition and scalar multiplication. Since $T$ is linear, $\forall \vu, \vv \in \text{Ker}(T)~\text{and}~a\in \R$, we have
        \begin{equation*}
            T(\vu + a\vv) = T(\vu) + aT(\vv) = \mathbf{0} + a\mathbf{0} \implies \vu + a\vv \in \text{Ker}(T)
        \end{equation*}
        \qed
        
        \settag{2.3.7}
        \para{Proposition.} Let $T:V\rightarrow{} W$ be a linear transformation of finite-dimensional vector spaces, and let $\alpha, \beta$ be bases for $V,W$ respectively. Then $\bx \in \text{Ker}(T)$ if and only if the coordinate vector of $\bx$, $[\bx]_\alpha$, satisfies the system of equations
        \begin{center}
            $\begin{cases}
            a_{11}x_1 + \ldots + a_{1k}x_k &= 0 \\
            &\vdots \\
            a_{l1}x_1 + \ldots + a_{lk}x_k &= 0
            \end{cases}$
        \end{center}
        where the coefficient $a_{ij}$ are the entries of the matrix $[T]_\alpha^\beta$
        % \begin{center}
        %     $
        %     \begin{cases}
        %     b_{11}a_1 + \ldots + b_{1n}a_n &= 0 \\
        %     b_{21}a_1 + \ldots + b_{2n}a_n &= 0 \\
        %     &\vdots \\
        %     b_{m1}a_1 + \ldots + b_{mn}a_n &= 0
        %     \end{cases}
        %     $
        % \end{center}
        
        \settag{2.3.8}
        \item \tbf{Independence is basis-independent.} If $\alpha = \{v_1,\ldots,v_k\}$ is a basis for $V$, then $\vx_1,\ldots,\vx_m\in V$ are independent if and only if $[\bx_1]_\alpha,\ldots,[\bx_m]_\alpha$ are independent.
        
        \settag{2.3.10}
        \para{Definition of Image.} The subset of $W$ constsing of vectors $\vw\in W$ for which there exists a $\vv\in V$ such that $T(\vv) = \vw$ is called the \tit{image} of $T$ and is denoted by Im$(T)$. In set builder notation we have
        \begin{equation*}
            \text{Im}(T) := \{\vw\in W|T(\vv) = \vw~\text{for some}~\vv \in V\}~~\text{where}~T:V\rightarrow{} W
        \end{equation*}
        
        \settag{2.3.11}
        \para{Proposition: Image is subspace.} Let $T:V\rightarrow{} W$ be a linear transformation. The image of $T$ is a subspace of $W$, the co-domain.\newline
        \proof\newline
        Let, $\vw_1,\vw_2\in\text{Im}(T)$, and let $a\in \R$. Since $\vw_1$ and $\vw_2\in V$ with $T(\vv_1) = \vw_1$ and $T(\vv_2) = \vw_2$. Then we have
        \begin{align*}
            a\vw_1 + \vw_2 
            &= aT(\vv_1) + T(\vv_2) \\
            &= T(a\vv_1 + \vv_2) 
            \implies a\vw_1 +\vw_2 \in \text{Im}(T)\text{ by linearity}
        \end{align*}
        Hence, by the ``quick check rule", we know that image is a subspace of the co-domain. \qed
        
        \settag{2.3.12}
        \para{Proposition.} If $\{\vv_1,\ldots,\vv_m\}$ is ant set that spans $V$ (in particular, it could be a basis of $V$), then $\{T(\vv_1),\ldots,T(\vv_m)\}$ spans\footnote{By ``spans" we mean equal to each other} Im$(T)$. \newline
        \underline{\tit{Proof($\supseteq$):}} \newline
        Let $\vw\in \text{Im}(T)$, then $\exists \vv\in V$ with $T(\vv) = \vw$. Since $Span\{\vv_1,\ldots,\vv_m\} = V$, then $\exists a_1,\ldots,a_m$ s.t. $a_1\vv_1 + \ldots + a_m\vv_m = \vv$. Then, 
        \begin{align*}
            \vw &= T(\vv) \\
            &= T\left(\sum_{i=1}^m a_i\vv_i \right) \\
            &= \sum_{i=1}^m a_iT(\vv_i) ~~~~~~\mbox{// by linearity}
        \end{align*}
        Therefore, Im($T$) is contained in $Span\{T(\vv_1),\ldots,T(\vv_m)\}$. \newline
        \proofsubset \newline
        Let $\vw\in Span\{T(\vv_1,\ldots,\vv_m)\}$, then (reversing what we did previously) we have
        \begin{align*}
            \vw &= \sum_{i=1}^m a_iT(\vv_i) \\
            &= T\left(\sum_{i=1}^m a_i\vv_i \right) ~~~~~~\mbox{// by linearity}\\
            &= T(\vv)\in \text{Im}(T)
        \end{align*}
        Hence mutual inclusion yields us the wanted result, and this completes the proof. \qed
        
        \settag{2.3.13}
        \para{Corollary.} If $\alpha = \{\vv_1,\ldots,\vv_k\}$ is a basis for $V$, and $\beta = \{\vw_1,\ldots,\vw_l\}$ is a basis for $W$, then the vectors in $W$ whose coordinate vectors (in terms of $\beta$) are the column of $[T]_\alpha^\beta$ span Im($T$).
        
        \settag{2.3.17}
        \para{Rank-Nullity Theorem.\footnote{Known as The Dimension Theorem in book}} If $V$ is a finite-dimensional vector space and $T:V\rightarrow{}W$ is a linear transformation, then 
        \begin{equation*}
            \text{dim(Ker($T$))} + \text{dim(Im($T$))} = \text{dim}(V)
        \end{equation*}
        Or equivalently\footnote{also, dim(Im($T$)) = dim(Rol($T$)) = dim(Col($T$)) = \#pivot in r.r.e.f},
        \begin{equation*}
            \text{dim(Ker($T$))} + \text{Rank($T$)} = \text{dim}(V)
        \end{equation*}
    \end{enumerate}
    
\section{Applications of Rank-Nullity Theorem}
    \begin{enumerate}
        \settag{2.4.2}
        \para{Proposition.} A linear transformation $T:V\rightarrow{} W$ is injective if and only of $\text{dim(Ker}(T\text{))}=0$. Informally speaking, we can think of this as ``No information is lost during the linear transformation". \newline
        \proofforward \newline
            If $T$ is injective, then by definition, there $\exists!\vv \in V$ with $T(\vv) = \mathbf{0}$. Since we know that $T(\mathbf{0}) = 0, \forall~\text{linear mappings}$, the zero vector is the unique vector $\vv$ satisfying $T(\vv) = \mathbf{0}$. Thus the kernel of $T$ consists of only the zero vector. Therefore, dim(Ker($T$)) = 0. \newline
        \proofback \newline
            Conversely, assume that dim(Ker($T$)) = 0. Let $\vv_1, \vv_2 \in V$ with $T(\vv_1) = T(\vv_2)$. We want to show that $\vv_1 = \vv_2$. Since $T(\vv_1) = T(\vv_2)$, $T(\vv_1 - \vv_2) = \mathbf{0}$, so $\vv_1 - \vv_2 \in \text{Ker}(T)$. But if dim(Ker($T$)) = 0, it follows that Ker($T$) = $\{\mathbf{0}\}$. It follows that $\vv_1 - \vv_2 = \mathbf{0} \implies \vv_1 = \vv_2$ as wanted. Thus, $T$ is injective. \qed
            
        \settag{2.4.3}
        \para{Corollary.} A linear mapping $T:V\rightarrow{} W$ on a finite-dimensional vector space $V$ is injective if and only if $\text{dim(Im(}T\text{)) = dim(}V\text{)}$
        
        \settag{2.4.4}
        \para{Corollary.} If dim($W$) $<$ dim($V$) and \trans{T}{V}{W} is a linear mapping, then $T$ is not injective.
        
        \settag{2.4.5}
        \para{Corollary.} If $V$ and $W$ are finite dimensional, then a linear mapping \trans{T}{V}{W} can be injective only if $\dime{W} \geq \dime{V}$
        
        \settag{2.4.7}
        \para{Proposition.} If $W$ is finite-dimensional, then a linear mapping \trans{T}{V}{W} is surjective if and only if dim(Im($T$))$ =\dime{W}$. \newline
        \textbf{Remark.} Since dim(Im($T$)) $\leq$ dim($V$) by the theorem, if dim($V$) $<$ dim($W$), then we have dim(Im($T$)) $<$ dim($W$), hence $T$ is not surjective.
        
        \settag{2.4.8}
        \para{Corollary.} If $V$ and $W$ are finite-dimensional, with $\dime{V} < \dime(W)$, then there is no surjective linear mapping \trans{T}{V}{W}.
        
        \settag{2.4.9}
        \para{Corollary.} A linear mapping \trans{T}{V}{W} can be surjective only if $\dime{V} \geq \dime{W}$.
        
        \settag{2.4.10}
        \para{Proposition.} Let $\dime{V} = \dime{W}$. A linear transformation \trans{T}{V}{W} is injective if and only if it is surjective. \newline
        \proofforward \newline
        If $T$ is injective, then dim(Ker($T$)) = 0 by proposition (2.4.2). By Theorem (2.3.17), we have dim(Im($T$)) = dim($V$). Therefore, by proposition (2.4.7), $T$ is injective \newline
        \proofback \newline
        If $T$ is surjective, then by Proposition (2.4.7), dim(Im($T$)) = dim($W$) = dim($V$). Therefore, by Theorem (2.3.17), dim(Ker($T$)) = 0. Hence, by proposition (2.4.2), $T$ is injective. \qed
        
        \settag{2.4.11}
        \para{Proposition.} Let \trans{T}{V}{W} be a linear transformation, and let $\vw \in \text{Im}(T)$. Let $\vv_1$ be any fixed vector with $T(\vv_1) = \vw$. Then every vector $\vv_2\in T^{-1}\left(\{\vw\}\right)$ can be written uniquely as $\vv_2 = \vv_1 + \vu$, where $\vu \in \text{ker}(T)$
        
        \settag{2.4.15}
        \para{Corollary.} Let \trans{T}{V}{W} be a linear transformation of finite-dimensional vector spaces, and let $\vw\in W$. Then $\exists! \vv \in V~s.t.~T(\vv) = \vw$ if and only if
        \begin{enumerate}
            \item $\vw \in \text{Im}(T)$, and
            \item dim(Ker($T$)) = $0$
        \end{enumerate}
        
        \settag{2.4.16}
        \para{Proposition.}
        \begin{enumerate}
            \item The set of solutions of the system of linear equations $A\vx = \vb$ is the subset $T^{-1}\left(\{\vb\}\right)$ of $V = \R^n$
            \item The set of solutions of the system of linear equations $A\vx = \vb$ is a subspace of $V$ if and only if the system is homogeneous, in which case the set of solutions is Ker($T$).
        \end{enumerate}
        
        \settag{2.4.17}
        \para{Corollary.} 
        \begin{enumerate}
            \item The number of free variables in the homogeneous system $A\vx = \mathbf{0}$ (or its echelon form equivalent) is equal to dim(Ker($T$)).
            \item The number of basic variables of the system is equal to dim(Im($T$)).
        \end{enumerate}
        Now, if the system is \tit{not homogeneous}, then from Proposition (2.4.16) we see that $A\vx = \vb$ has a solution if and only if $\vb \in \text{Im}(T)$. Let us assume then that $\vb \in \text{Im}(T)$, then this yields us the following terminology:
        
        \settag{2.4.18}
        \para{Definition of particular solution of a SLE (in-homo).} Given an in-homogeneous system of equations, $A\vx = \vb$, any single vector $\vx$ satisfying the system (necessarily $\vx \neq \mathbf{0}$) is called a \tit{particular solution} of the system of equations.
        
        \settag{2.4.19}
        \para{Proposition.} Let $\vx_p$ be a particular solution of the system $A\vx = \vb$. Then every other solution to $A\vx = \vb$ is of the form $\vx = \vx_p + \vx_h$, where $\vx_h$ is a solution of the corresponding homogeneous system of equations $A\vx = \mathbf{0}$. Furthermore, given $\vx_p$ and $\vx$, there is a unique $\vx_h$ such that $\vx = \vx_p + \vx_h$.
        
        \settag{2.4.20}
        \para{Corollary.} The syetem $A\vx = \vb$ has a unique solution if and only if $\vb \in \text{Im}(T)$ and the only solution to $A\vx = \mathbf{0}$ is the zero vector.
    \end{enumerate}
    
\section{Composition of Linear Transformations}
\begin{enumerate}
    \settag{2.5.1}
    \para{Proposition.} If \trans{S}{U}{V} and \trans{T}{V}{W} are linear transformations, then so is $TS$. \newline
    \proof \newline
    Let $\alpha ,\beta \in \R$ and let $\vu_1,\vu_2 \in U$. We must show that $TS$ satisfies Proposition (2.1.2).
    \begin{align*}
        TS(\alpha \vu_1 + \beta \vu_2) &= T(S(\alpha \vu_1 + \beta \vu_2)) ~~~~~~~~~~~~~\text{// by definition of $TS$}\\
        &= T(\alpha S(\vu_1) + \beta S(\vu_2))~~~~~~~~~\text{// by linearity of $S$}  \\
        &= \alpha T(S(\vu_1)) + \beta T(S(\vu_2))~~~~\text{// by linearity of $T$} \\
        &= \alpha TS(\vu_1) + \beta TS(\vu_2)
    \end{align*}
    \qed
    
    \settag{2.5.4}
    \para{Propositions.} 
    \begin{enumerate}
        \item \textbf{Associativity.} Let \trans{R}{U}{V}, and \trans{T}{W}{X} be linear transformation of the vectors spaces $W,V,W$, and $X$ as indicated, then 
        \begin{equation*}
            T(SR) = (TS)R
        \end{equation*}
        \item \textbf{Distributivity I.} Let \trans{R}{U}{V}, \trans{S}{U}{V} and \trans{T}{V}{W} be linear transformations of the vectors spaces $U,V$ and $W$ as indicated, then
        \begin{equation*}
            T(R+S) = TR+ TS
        \end{equation*}
        \item \textbf{Distributivity II.} Let \trans{R}{U}{V}, \trans{S}{V}{W} and \trans{T}{V}{W} be linear transformations of the vectors spaces $U, W$ and $W$ as indicated, then
        \begin{equation*}
            (T+S)R = TR + SR
        \end{equation*}
    \end{enumerate}
    
    \settag{2.5.6}
    \para{Proposition.} Let \trans{S}{U}{V} and \trans{T}{V}{W} be linear transformation, then
    \begin{enumerate}
        \item $\text{Ker}(S)\subset \text{Ker}(TS)$
        \item $\text{Im}(TS)\subset \text{Im}(T)$
    \end{enumerate}
    \proof \newline
    We will prove (a) here. If $\vu \in \text{Ker}(S)$, $S(\vu) = \mathbf{0}$. Then 
    \begin{equation*}
        TS(\vu) = T(\mathbf{0}) = \mathbf{0}
    \end{equation*}
    where we notice that the first and last term tells us $\vu \in \text{Ker}(TS)$ and this completes the proof. \qed
    
    \settag{2.5.7}
    \para{Corollary.} Let \trans{S}{U}{V} and \trans{T}{V}{W} be linear transformations of fin-dim vector spaces, then
    \begin{enumerate}
        \item dim(Ker($S$)) $\leq$ dim(Ker($TS$))
        \item dim(Im($TS$)) $\leq$ dim(Im($T$))
    \end{enumerate}
    \tbf{Remark.} In naive words, the statement (a) is saying that no transformation can bring what has been smashed to zero back, and thus the composition result of two linear transformations must have a larger kernel. The statement (b) is saying that each time we perform a linear transformation, the ``Image Range" of the final output would be restricted, and thus a composition would only result in a tighter restriction.
    
    \settag{2.5.9}
    \para{Proposition: Compatible Matrix Product.} If $\left[S\right]_\alpha^\beta$ has entries $a_{ij}$, where $i\in \{1, \ldots, n\}$ and $j\in \{1, \ldots, m\}$ and $\left[T\right]_\beta^\gamma$ has entries $b_{kl}$, where $k\in \{1, \ldots, p\}$ and $l\in \{1, \ldots, n\}$, then the entries of $\left[TS\right]_\alpha^\gamma$ are 
    \begin{equation*}
        \sum_{l=1}^nb_{kl}a_{lj}
    \end{equation*}
    
    \settag{2.5.13}
    \para{Proposition: Composition of Transformations Induced By Matrices.} Let \trans{S}{U}{V} and \trans{T}{V}{W} be linear transformation between fin-dim vector spaces. Let $\alpha, \beta, \gamma$ be bases for $U, V$ and $W$, respectively. Then
    \begin{equation*}
        \left[TS\right]_\alpha^\gamma = \left[T\right]_\beta^\gamma \left[S\right]_\alpha^\beta
    \end{equation*}
    One can comprehend this as: the standard matrix of a transformation that is a composition of two transformations induced by matrices is the matrix product of those matrices. Also note that matrix product \tit{does not} commute.
    
    \settag{2.5.14}
    \para{Propositions.}
    \begin{enumerate}
        \item \tbf{Associativity.} Let $A\in M_{m\times n}(\R), B\in M_{n\times p}(\R), C\in M_{p\times r}(\R)$, then
        \begin{equation*}
            (AB)C = A(BC)
        \end{equation*}
        
        \item \tbf{Distributivity I.} Let $A\in M_{m\times n}(\R)$ and $B, C\in M_{n\times p}(\R)$, then
        \begin{equation*}
            A(B+C) = AB + AC
        \end{equation*}
        
        \item \tbf{Distributivity II.} Let $A, B\in \mat{m}{n}, C\in \mat{n}{p}$, then
        \begin{equation*}
            (A+B)C = AC + BC
        \end{equation*}
    \end{enumerate}
\end{enumerate}

\section{The Inverse of A Linear Transformation}
\begin{enumerate}
    \settag{2.6.1}
    \para{Proposition.} Let \trans{T}{V}{W} be bijective, then the inverse function \trans{S}{W}{V} is a linear transformation.
    
    \settag{2.6.2}
    \para{Proposition.} A linear transformation \trans{T}{V}{W} has an inverse linear transformation $S$ \tit{if and only if} $T$ is injective and surjective.
    
    \settag{2.6.3}
    \para{Definition of Inverse and Invertible.} If \trans{T}{V}{W} is a linear transformation that has an inverse transformation \trans{S}{W}{V}, we say that $T$ is \tit{invertible}, and we denote the inverse of $T$ here $T^{-1} := S$ 
    
    \settag{2.6.4}
    \para{Definition of isomorphism.} If \trans{T}{V}{W} is an invertible linear transformation, $T$ is called an \tit{isomorphism}, and we say $V$ and $W$ are \tit{isomorphic} vector spaces.
    
    \settag{2.6.5}
    \para{Remark on the $T^{-1}$ notation.} Please note that the inverse here that is acting on a vector is denotes the inverse transformation. For example we have \trans{T}{V}{W}, then $w\in W, T^{-1}(w)\in V$ denotes the inverse transformation. We \textit{must} differentiate this from the other notation we encountered earlier which acts on a set rather than on a vector. As an example, we have \trans{T}{V}{W} and $\mathbf{0}\in W$, then
    \begin{equation*}
        T^{-1}\left(\{\mathbf{0}\}\right) \equiv \{\vv\in V, T(\vv) = \mathbf{0}\}
    \end{equation*}
    Notice that this special example is a.k.a the Kernel of the transformation by definition.
    
    \settag{2.6.7}
    \para{Proposition.} If $V$ and $W$ are finite-dimensional vector spaces, then there is an isomorphism \trans{T}{V}{W} \tit{if and only if} dim($V$) $=$ dim($W$). \newline
    \proofforward \newline
    If $T$ is an isomorphism, then we know that $T$ is bijective. So,
    dim(Ker($T$)) = 0 and dim(Im($T$)) = dim($W$). Then by the Rank-Nullity Theorem, we conclude that dim($V$) = dim($W$).\newline
    \proofback \newline
    If dim($V$) = dim($W$), we must produce an isomorphism \trans{T}{V}{W}. Let $\alpha = \{\vv_1, \ldots, \vv_n\}$ be a basis for $B$ and $\beta = \{\vw_1,\ldots,\vw_n\}$ be a basis for $W$. Define $T$ to be the linear transformation from $V$ to $W$ with $T(\vv_i) = w_i, i = 1,\ldots,n$. By Proposition (2.1.14), $T$ is uniquely determined by tis choice of values on $\alpha$. To see that $T$ is injective, notice that if
    \begin{equation*}
        T(a_1\vv_1 + \ldots + a_n\vv_n) = \mathbf{0}
    \end{equation*}
    then we have
    \begin{equation*}
        a_1\vw_1 + \ldots + a_n\vw_n = \mathbf{0}
    \end{equation*}
    Since the $\vw$'s are a basis, we know immediately that $a_1 = \ldots = a_n = 0$. Then Ker($T$) = $\{\mathbf{0}\}$ and $T$ is injective. By proposition (2.4.10) $T$ is also surjective, and then by Proposition (2.6.2) it is an isomorphism.
    
    \settag{2.6.9}
    \para{The Gauss-Jordan Method of Inverse\footnote{Extended based on this example}.} Suppose we have the following matrix:
    \begin{equation*}
    A = \left[T\right]_\alpha^\beta = 
        \begin{bmatrix}
            a_{11} & \hdots & a_{1n} \\
            \vdots & \ddots & \vdots \\
            a_{n1} & \ldots & a_{nn}
        \end{bmatrix}
    \end{equation*}
    and we want to find the matrix inverse $B = \left[T^{-1}\right]_\beta^\alpha$. To achieve this, we augment this matrix with the compatible identity map $I_{n\times n}$.
    \begin{equation}
        \begin{bmatrix}
            a_{11} & \hdots & a_{1n} &\aug & 1 & \ldots & 0 \\
            \vdots & \ddots & \vdots &\aug & \vdots & \ddots & \vdots \\
            a_{n1} & \ldots & a_{nn} &\aug & 0 & \ldots & 1
        \end{bmatrix} 
        = \left[A|I\right]
    \end{equation}
    We row reduce (2.2) w.r.t the left half matrix with a result of the following form:
    \begin{equation*}
        \begin{bmatrix}
            1 & \ldots & 0 &\aug & b_{11} & \hdots & b_{1n}  \\
            \vdots & \ddots & \vdots &\aug & \vdots & \ddots & \vdots \\
            0 & \ldots & 1 &\aug & b_{n_1} & \ldots & b_{nn}
        \end{bmatrix} 
        = \left[I|B\right] = \left[I|A^{-1}\right]
    \end{equation*}
    We then record the $n\times n$ coefficients ($b_{ij}$) on the right hand side which is our end result of the inverse.
    
    \settag{2.6.10}
    \para{Definition of invertible matrices.} An $n \times n $ matrix is called invertible if there exists an $n\times n$ matrix $B$ so that $AB = BA = I$. Where the $I$ denotes the compatible identity map and $B$ is called an inverse of $A$ and $A^{-1} := B$
    
    \settag{2.6.11}
    \para{Proposition.} Let \trans{T}{V}{W} be an isomorphism of finite-dimensional vector spaces. Then for any choice of bases $\alpha$ for $V$ and $\beta$ for $W$, we have
    \begin{equation*}
        \left[T^{-1}\right]_\beta^\alpha = \left[\left[T\right]^\beta_\alpha\right]^{-1}
    \end{equation*}
    \proof \newline
    To show the above identity, it suffices to show that $\left[T^{-1}\right]_\beta^\alpha$ is the matrix inverse of $\left[T\right]_\alpha^\beta$. On one hand we have
    \begin{equation*}
        \left[T^{-1}\right]_\beta^\alpha \left[T\right]^\beta_\alpha = \left[T^{-1}T\right]_\alpha^\alpha = \left[I_{n \times n}\right]_\alpha^\alpha
    \end{equation*}
    On the other hand, we have
    \begin{equation*}
        \left[T\right]^\beta_\alpha \left[T^{-1}\right]_\beta^\alpha = \left[TT^{-1}\right]_\beta^\beta = \left[I_{n \times n}\right]_\beta^\beta
    \end{equation*}
    Then, since the $n\times n$ identity map is unique, $\left[I_{n \times n}\right]_\beta^\beta = \left[I_{n \times n}\right]_\alpha^\alpha$, we can equate the above two equations, which, by definition tells us that $\left[T^{-1}\right]_\beta^\alpha$ is the matrix inverse of $\left[T\right]_\alpha^\beta$. We conclude that the above identity is true.\qed
\end{enumerate}

\section{Change of Basis}
\begin{enumerate}
    \settag{2.7.2}
    \para{Remark on the notation.} Note that the here the $[I]_\alpha^{\alpha'}$ is \textit{not} the identity map since $\alpha \neq \alpha'$. This notation will come up in the following chapter naturally but it is important to differentiate this notation with the identity map.
    
    \settag{2.7.3}
    \para{Proposition.} Let $V$ be a finite-dimensional vector space, and let $\alpha, \alpha'$ be bases for $V$. Let $\vv \in V$. Then the coordinate vector $[\vv]_{\alpha'}$ of $\vv$ in the basis $\alpha'$ is related to the coordinate vector $[\vv]_\alpha$ of $\vv$ in the basis $\alpha$ by
    \begin{equation*}
        [I]_\alpha^{\alpha'}[\vv]_\alpha = [\vv]_{\alpha'}
    \end{equation*}
    
    \settag{2.7.5}
    \para{Theorem.} Let \trans{T}{V}{W} be a linear transformation between fin-dim vector spaces $V$ and $W$. Let \trans{I_V}{V}{V} and \trans{I_W}{W}{W} be respective identity transformations of $V$ and $W$. Let $\alpha, \alpha'$ be two bases for $V$ and let $\beta, \beta'$ be two bases for $W$, then
    \begin{equation*}
        \map{T}{\alpha'}{\beta'} = \map{I_W}{\beta}{\beta'}\cdot \map{T}{\alpha}{\beta}\cdot \map{I_V}{\alpha'}{\alpha}
    \end{equation*}
    \textbf{Remark.} In naive words, in order to produce a transformation from bases $\alpha'$ to $\beta'$ we first make the change of basis from $\alpha'$ to $\alpha$, then we perform the transformation from $\alpha$ basis to $\beta$ basis and lastly we change the basis to the wanted $\beta'$ through a ``cross basis identity map".
    
    \settag{2.7.6}
    \para{Definition of Similar Matrices.} Let $A, B$ be $n\times n$ matrices. $A$ and $B$ are said to be \textit{similar} if there exists an invertible matrix $Q$ such that:
    \begin{equation*}
        B = Q^{-1}AQ
    \end{equation*}
    Applying this new definition, we can see the follows: Let \trans{T}{V}{V} is a linear transformation and $\alpha, \alpha'$ are two bases for $V$, then $A = \map{T}{\alpha}{\alpha}$ is similar to $B = \map{T}{\alpha'}{\alpha'}$, and the invertible matrix $Q$ in the definition is the matrix $Q = \map{I_V}{\alpha'}{\alpha}$
\end{enumerate}

\chapter{The Determinant Function}
\section{The determinant as area}
\begin{enumerate}
    \settag{3.1.1}
    \para{Propositions: Geometry of a parallelogram in $\R^2$} 
    \begin{enumerate}
        \item The area of the parallelogram with vertices $\mathbf{0}, \va_1, \va_2$, and $\va_1 + \va_2$ is $\pm\left(a_{11}a_{22} - a_{12}a_{21}\right)$
        \item The area is not zero if and only if the vectors $\va_1$ and $\va_2$ are linearly independent.
    \end{enumerate}
    
    \settag{3.1.2}
    \para{Corollary.} Let $V=\R^2$, \trans{T}{V}{V} is an isomorphism if and only if the area of the parallelogram constructed previously is non-zero
    
    \settag{3.1.3}
    \para{Proposition.} The function $Area(\va_1, \va_2)$ has the following properties for $\va_1,\va_2, \va_1'$, and $\va_2'\in \R^2$
    \begin{enumerate}
        \item $Area(b\va_1 + c\va_1', \va_2) = b~Area(\va_1,\va_2) + c~Area(\va_1', \va_2)~\text{for $b, c \in \R$}$
        \item \footnote{This essentially the same as (a)}$Area(\va_1, b\va_2 + c\va_2') = b~Area(\va_1,\va_2) + c~Area(\va_1, \va_2')~\text{for $b, c \in \R$}$
        \item $Area(\va_1, \va_2) = -Area(\va_2, \va_1)$, and
        \item $Area((1, 0), (0, 1) = 1$.
    \end{enumerate}
    \textbf{Remark.} Recall that (d) is the \textit{n-cube} in $\R^2$, where n-cube is defined as follows: (with $\ve_i$'s as components of $\xi$ basis for $\R^n$ Euclidean Space)
    \begin{equation*}
        C_n := \left\{\vx \in \R^n| \vx = \sum_{i = 1}^n \alpha_i \ve_i~\text{for some $\alpha_i\in \left[0, 1\right]$}\right\} \equiv \left[0, 1\right]^n
    \end{equation*}
    
    \settag{3.1.4}
    \para{Proposition.} If $B(\va_1, \va_2)$ is any real-valued function of $\va_1, \va_2\in \R^2$ that satisfies Properties (a), (c) and (d) of Proposition (3.1.3), then $B$ is equal to the area function.
    
    \settag{3.1.5}
    \para{Definition of \textit{determinant} of a $2 \times 2$ matrix.}
    \begin{enumerate}
        \item $\det(b\va_1 + c\va_1', \va_2) = b\det(\va_1, \va_2) + c\det(\va_1', \va_2)$ for $b, c\in \R$
        \item $\det(\va_1, \va_2) = -\det(\va_2, \va_1)$, and
        \item $\det(\ve_1, \ve_2) = 1$
    \end{enumerate}
    As a consequence of (3.1.4), $\det(A)$ is given by
    \begin{equation*}
        \det(A)\equiv a_{11}a_{22} - a_{12}a_{21}
    \end{equation*}
    where $a_{ij}$ refers to the $i$-th row, $j$-th column of the matrix $A$.
    
    \settag{3.1.6}
    \para{Propositions: Determinant in relations.}
    \begin{enumerate}
        \item A $2 \times 2$ matrix $A$ is invertible if and only if $\det(A)\neq 0$
        \item If \trans{T}{V}{V} is linear transformation of a two-dimensional vector space $V$, then $T$ is an isomorphism if and only if $\det(\map{T}{\alpha}{\alpha}) \neq 0$
    \end{enumerate}
        
\end{enumerate}



\end{document}
