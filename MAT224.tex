% Meta data
\documentclass[oneside, 12pt]{book}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{url}
\usepackage{hyperref}

% Environment, new commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\qed}{\hfill $\mathcal{Q}.\mathcal{E}.\mathcal{D}.$}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}

\title{%
  \textbf{MAT224 Linear Algebra}\\
  \large Definitions, Lemmas, Theorems, Corollaries \\
    and their related proofs}
\author{Tingfeng Xia}
\date{Winter 2019}

\begin{document}
\maketitle
\newpage % make a new page for actual contents, possibly a content table
\mbox{}
\vfill
by Tingfeng Xia \\


Materials in this booklet are based heavily on Prof. Nicolas Hoell's lectures as well as A COURSE IN LINEAR ALGEBRA by David B. Damiano and John B. Little.\newline\newline
Items in this booklet are, in fact, very similar to those in the book and are intended to be used as a bullet point guide to (almost) all the knowledge points and should \tit{certainly not} be used as a substitute for the actual learning material. Please notice that I make\tit{ no promise} about the accuracy of statements appearing in these notes.\newline\newline
The course website could be found here:\newline \url{http://www.math.toronto.edu/nhoell/MAT224/}
\newpage
\tableofcontents

\chapter{Vector Spaces}
\section{(Real) Vector Space}
\settag{1.1.1}
\begin{enumerate}
    \item{\textbf{Definition of real vector space:}} A real vector space is a set $V$ together with scalar
    \begin{enumerate}
        \item \textbf{Closure under vector addition:} an operation called vector addition, which for each pair of vectors $\vec{x}, \vec{y}\in V$ produces another vector in $V$ denoted $\vec{x} + \vec{y}$, (i.e. $\forall \vec{x}, \vec{y}\in V, \vec{x} + \vec{y} \in V$) and
        \item \textbf{Closure under scalar multiplication:} an operation called multiplication by a scalar (a real number), which for each vector $\vec{x}\in V$, an each scalar $c\in \mathbb{R}$ produces another vector in $V$ denoted $c\vec{x}$. (i.e. $\forall \vec{x}\in V, \forall c \in \mathbb{R}, c \vec{x} \in V$)
    \end{enumerate}
    Furthermore, the two operations must satisfy the following axioms:(important)
    %\begin{enumerate}
        %\item For all vectors \textbf{x, y}, and \textbf{z} $\in V$, %(\textbf{x}+\textbf{y})+\textbf{x} 
    %\end{enumerate}
    \begin{enumerate}
        \item $\forall \vec{x}, \vec{y}, \vec{z} \in V, (\vec{x} + \vec{y}) + \vec{z} = \vec{x} + (\vec{y} + \vec{z})$
        \item $\forall \vec{v}, \vec{y} \in V, \vec{x} + \vec{y} = \vec{y} + \vec{x}$
        \item $\exists \vec{0} \in V s.t. \forall \vec{x} \in V, \vec{x} + \vec{0} = \vec{x}$ (Note that this property is a.k.a existence of additive identity)
        \item $\forall \vec{x} \in V, \exists (-\vec{x}) \in V~s.t.~\vec{x} + (-\vec{x}) = \vec{0}$ (Note that this property is a.k.a existence of additive inverse)
        \item $\forall \vec{x}, \vec{y} \in V, c \in \mathbb{R}, c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}$
        \item $\forall \vec{x} \in V, c,d \in \mathbb{R}, (c + d)\vec{x} = c\vec{x} + d\vec{x}$
        \item $\forall \vec{x} \in V, c,d \in \mathbb{R}, (cd)\vec{x} = c(d\vec{x})$
        \item $\forall \vec{x} \in V, 1\vec{x} = \vec{x}$
    \end{enumerate}
    \settag{1.1.6}
    \item \textbf{Propositions for a R-v.s.} Let $V$ be a vector space. Then 
    \begin{enumerate}
        \item The zero vector is unique. Note that it might not necessarily be actually the zero vector in $\mathbb{R}^n$ that we are somewhat used to use.
        \item $\forall \vec{x} \in V, 0\vec{x}=0$
        \item $\forall \vec{x} \in V, $ the additive inverse is unique. Note that it might not necessarily be actually just $(-1)$ times the vector in $\mathbb{R}^n$ that we are somewhat used to use.
        \item $\forall \vec{x} \in V,~\forall c\in \mathbb{R}, ~(-c)\vec{x}=~-(c\vec{x})$
    \end{enumerate}
\end{enumerate}

\section{Sub-spaces}
    \begin{enumerate}
        \settag{1.2.4}
        \item \textbf{Usual definition of subspace applied to functions in $C^0(\mathbb{R})$}. Note that by $C^n(\cdot)$ we mean the function in this set are all of $Class-n$. Let $f,g \in C^0(\mathbb{R)}, let c\in \mathbb{R}$. Then, 
        \begin{enumerate}
            \item $f+g\in C^0(\R)$, and
            \item $cf \in C(\R)$
        \end{enumerate}
        The proof of this lemma relies on limit theorems of calculus.
        
        \settag{1.2.6}
        \item \textbf{(Intuitive) definition of (vector) subspace} Let $V$ be a vector space and let $W\subseteq V$ be a subset. Then $W$ is a (vector) subspace if $W$ is a vector subspace itself under the operations of vector sum and scalar multiplication from $V$.
        
        \settag{1.2.8}
        \item \textbf{Quick check rule for a subspace.} Let $V$ be a vector subspace, and let $W$ be a \textbf{non empty} subset of $V$. Then $W$ is a subspace of $V$ if and only if $\forall \vec{x}, \vec{y}\in W,~\forall c\in \R$, we have $c\vec{x}+\vec{y}\in W$.
        \settag{1.2.9}
        \item \textbf{Remark on the necessary condition of non-emptiness of subspace.} According to the definition of vector space that we gave in 1.1.1, a vector space must contain an additive identity element, hence it is necessary that we ensure $W\subseteq V$(from 1.2.6) is not an empty set.
        
        \settag{1.2.13}
        \item \textbf{Theorem: Intersection of sub-spaces is a subspace.} Let $V$ be a vector space. Then the intersection of any collection of sub-spaces of $V$ is a subspace of $V$.
        
        \settag{1.2.14}
        \item \textbf{Corollary: Hyper planes in $\R^n$ are sub-spaces of $\R^n$.} Let $a_{ij}(1\leq i\leq m)$, let $W_i = \{(x_1, ..., x_n)\in \R^n|a_{i1}x_1 + ... + a_{in}x_n = 0,~\forall 1 \leq i \leq m\}$. Then $W$ is a subspace of $\R^n$.
        
    \end{enumerate}
    
\section{Linear Combinations}
    \begin{enumerate}
        \settag{1.3.1}
        \item \textbf{Definitions regarding L.C. and derived spans.} Let $S$ be a subset of a vector space $V$, that is $S\subseteq V$.
        \begin{enumerate}
            \item a \textit{linear combination} of vectors in $S$ is any sum $a_1\vec{x}_1 + ... + a_n\vec{x}_n$, where the $a_i \in \R$, and the $x_i \in S$.
            \item we define the $Span$ of a set of vectors as follows to consider the special case of $S\stackrel{?}{=}\emptyset \in V$.
            \newline \underline{\tit{Case1: $S\neq \emptyset$:}} In this case, we define $Span(S)$ to be all possible linear combinations using vectors in $S$.\newline
            \underline{\tit{Case2: $S= \emptyset$:}} In this case, we define $Span(S = \emptyset)=\{\vec{0}\}$. We call this the zero-space.
            
            \item If $W=Span(S)$, we say $S$ \textit{spans(\text{or} generates)} $W$.
        \end{enumerate}
        
        \settag{1.3.4}
        \item \textbf{Span of a  subset of a vector space is a subspace.} Let $V$ be a vector space and let $S$ be any subset of $V$. Then $Span(S)$ is a subspace of $V$.
        
        \settag{1.3.5}
        \item \textbf{Sum of sets(with application to subs-paces).} Let $W_1\wedge W_2$ be sub spaces of a vector space $V$. The sum of $W_1$ and $W_2$ is the set
        \begin{center}
            $W_1+W_2 :=\{\vec{x}\in V |\vec{x}=\vec{x_1}+\vec{x_2}, \text{for some } \vec{x_1}\in W_1, \vec{x_2} \in W_2\}$
        \end{center}
        We think of the sum of the two sub-spaces(the two sets) as the set of vectors that can be built up from the vectors in $W_1$ and $W_2$ by linear combinations. Conversely, the vectors in the set $W_1+W_2$ are precisely the vectors that can be broken down into the sum of a vector in $W_1$ and a vector in $W_2$. One may find it helpful to view this as an analogue to a Cartesian product of the two set with a new constraint on the result.
        
        \settag{1.3.6}
        \item \textbf{Example.} If $W_1 = \{(a_1, a_2)\in \R^2|a_2 = 0\}$ and $W_2 \{(a_1, a_2)\in \R^2|a_1 = 0\}$, then $W_1 + W_2= \R^2$, since every vector in $\R^2$ can be written as the sum of vector in $W_1$ and a vector in $W_2$. For instance, we have $(5, -6)=(0, 5)+(0, -6)$, and $(5, 0)\in W_1 \wedge (0, -6)\in W_2$
        
        
        \settag{1.3.8} 
        \item \textbf{Proposition: The sum of spans of sets is the span of the union of the sets.} Let $W_1 = Span(S_1)$ and $W_2 = Span(S_2)$ be sub-spaces of a\textit{(the same)} vector space $V$. Then $W_1 + W_2 = Span(S_1 \cup S_2)$. Notice that the proof of this gave the important idea of mutual inclusion in proving sets are equal to each other.
        
        \settag{1.3.9}
        \item \textbf{The sum of sub-spaces is also a subspace.} Let $W_1$ and $W_2$ be sub-spaces of a vector space $V$. Then $W_1 + W_2$ is also a subspace of $V$.\newline
        \underline{\textit{Proof:}}\newline
        It is clear that $W_1 + W_2$ is non-empty, since neither $W_1$ nor $W_2$ is empty. Let $\vec{X}, \vec{y}$ be two vectors in $W_1+W_2$, let $c\in \R$. By our choice of $\vec{x}\text{and } \vec{y}$, we have
        \begin{align*}
            c\vec{x} + \vec{y} & = c(\vec{x}_1 + \vec{x}_2) + (\vec{y_1} + \vec{y_2}) \\
            & = (c\vec{x}_1 + \vec{y}_1) + (c\vec{x}_2 + \vec{y}_2) \in W_1+W_2
        \end{align*}
        Since $W_1$ and $W_2$ are sub-spaces of $V$, we have $(c\vec{x}_1 + \vec{y}_1) \in W_1\wedge (c\vec{x}_2 + \vec{y}_2)\in W_2$. Then by (1.2.8), we see that indeed $W_1 + W_2$ is a subspace of $V$. \qed
        
        \settag{1.3.10}
        \item \textbf{Remark.} In general =, if $W_1$ and $W_2$ are sub-spaces of $V$, then $W_1 \cup W_2$ will not be a subspace of $V$. For example, consider the two sub-spaces of $\R^2$ given in example (1.3.6). In that case $W_1\cup W_2$ is the union of two lines through the origin in $\R^2$. 
        
        \settag{1.3.11}
        \item \textbf{Proposition.} Let $W_1$ and $W_2$ be sub-spaces of vector space $V$ and let $W$ be a subspace of $V$ such that $W\supseteq W_1 \cup W_2$, then $W\supseteq W_1 + W_2$. Informally speaking, this proposition saying: "$W_1+W_2$ is the smallest subspace containing $W_1\cup W_2$", i.e., Any subspace that contains $W_1\cup W_2$ must be a super set of $W_1 + W_2$. \newline
        \tit{\underline{Proof:}}\newline
        We want to show: $W\supseteq W_1\cup W_2\Longrightarrow W\supseteq W_1 + W_2$\newline
            \begin{align*}
                \text{Assume that } \text{$W\supseteq W_1 \cup W_2$. Let $w_1\in W_1,~w_2\in W_2$.}\\
                \text{We notice that } w_1,~w_2\in W_1\cup W_2 \subseteq W \\
                \implies& w_1, w_2\in W \\
                \text{(Since $W$ is a subspace, so it is closed under addition)}\\
                \implies& w_1 + w_2 \in W\\
                \implies W_1 + W_2 \subseteq W \iff& W \supseteq W_1 + W_2 
            \end{align*}
            \qed
        
    \end{enumerate}

\section{Linear (In)dependence}
    \begin{enumerate}
        \settag{1.4.2}
        \item \textbf{Algebraic definition of linear dependence.} Let $V$ be a vector space, and let $S\subseteq V$.
            \begin{enumerate}
                \item A \textit{linear dependence} among the vectors of $S$ is an equation $a_1\vec{x} + ... + a_n\vec{x}_n = \vec{0}$ where the $x_i\in S$, and the $a_i\in \R$ are not all zero(i.e., at least one of the $a_i\neq 0$). In familiar\footnote{Familiar from MAT223, Prof. Jason Siefken's IBL(Inquiry Based Learning) notes.} words, there exists a non-trivial solution to the equation mentioned above.
                \item the set $S$ is said to be \textit{linearly dependent} if there exists a linear dependence among the vectors in $S$.
            \end{enumerate}
            \tbf{Remark: }It can be shown that the geometric\footnote{A set of vectors is said to be dependent of each other there exists a vector in this set, that it is in the Span of all other vectors in the set. I.e., There is some vectors in this set that are "redundant", it's position can be taken by some linear combination of the other vectors in the set.} definition and this are, in-fact, equivalent to each other. I will now produce the proof. \newline
        \tit{\underline{Proof of equivalence of definitions:}} \newline
        Let $V$ be a vector space, and let $S\subseteq V$. Consider the following equation:
        \begin{align*}
            a_1\vec{x}_1 + a_2\vec{x}_2 + ... + a_n\vec{x}_n &= 0\text{, where } \exists a_i\neq 0 \\
            \text{(WLOG, assume that } a_n &\neq \vec{0}\text{)}\\
            \implies \frac{a_1\vec{x}_1 + a_2\vec{x}_2 + ... + a_n\vec{x}_n}{a_n}&=\vec{0}\\
            \implies \vec{x}_n &= -\sum_{i=1}^{n-1}a_i\vec{x}_i
        \end{align*}
        Notice that the result $\vec{x}_n$ is in terms of all the other $(n-1)$ vectors in the set, hence a linear combination of those vectors, and this completes the proof.
        \qed\newline
        \textbf{Re-Remark: }We can also use this proof as an argument towards the following problem: Show that at least one of the vectors in a linearly dependent set is redundant. We could take take a similar proof and argue that the linear combination could be written without at least one of the vectors.
        
        \settag{1.4.4}
        \item \textbf{Algebraic definition of linear independence.} Let $V$ be a vector space, and $S\subseteq V$. Then $S$ is \textit{linearly independent} if whenever we have $a_i\in \R$ and $x_i\in S$ such that $a_1\vec{x}_1+...+a_n\vec{x}_n = \vec{0}$, then $a_i = 0,~\forall i$. A more conceivable way to understand this is if the aforementioned equation exists and only exists a set of trivial solution then the vectors involved in the equation are \textit{linearly independent}. \newline \tbf{Remark: }A set of vector is linearly independent \tit{if and only if} it is not linearly dependent.
        
        \settag{1.4.7}
        \item \textbf{Propositions regarding linear (in)dependency.}
            \begin{enumerate}
                \item Let $S$ be a linearly dependent subset of a vector space $V$, and let $S'$ be another subset of $V$ that contains $S$. Then $S'$ is also linearly dependent.
                \item Let $S$ be a linearly independent subset of vector space $V$ and let $S'$ be another subset of $V$ that is contained in $S$. Then $S'$ is also linearly independent.
            \end{enumerate}
        \underline{\tit{Proof of (a):}}
        Since $S$ is linearly dependent, there exists a linear dependence among the vectors in $S$, say, $a_1\vec{x}_1 + ...+ a_n\vec{x}_n = \vec{0}$. Since $S$ is contained in $S'$, this is also a linear dependence among the vectors in $S'$. Hence $S'$ is linear dependent.\qed\newline
        \underline{\tit{Proof of (b):}}
        Consider any equation $a_1\vec{x}_1 + ...+ a_n\vec{x}_n = \vec{0}$, where the $a_i \in \R,~\vec{x}_i \in S'$. Since $S'$ is contained in $S$, we can also view this as a potential linear dependence among vectors in $S$. However, $S$ is linearly independent, so it follows that all the $a_i = 0\in \R$. Hence $S'$ is also linearly independent.\qed
        
        \settag{1.4.*}
        \item \tbf{Example of showing linear independence.(1)} Show that the set $\{1,x,x^2,x^3,...,x^n\}$ is linearly independent. We consider the following equation:
        \begin{equation*}
            0 = a_0 + a_1x + ... + a_{n-1}x^{n-1}+a_n x^n~~~~~~(*)
        \end{equation*}
        Then we take the derivative:
        \begin{equation*}
            \frac{d^n}{dx^n}(*)= 0 = n!\cdot a_n
        \end{equation*}
        Then since $n! \neq 0$, we know $a_n=0$. We repeat the same process, taking derivatives $(n-1)$ times w.r.t. $x$ we get $a_{n-1}=0$ and so on. As a last step we have:
        \begin{equation*}
            \frac{d}{dx}(*) = 0 = 1!\cdot a_1 \implies a_1 = 0
        \end{equation*}
        Then we have $0 = 0 + 0 +... + a_0 \implies a_0 = 0$. So the equation has and only has a trivial solution, thus the set $\{1,x,x^2,x^3,...,x^n\}$ is linearly independent. \qed
        
        \settag{1.4.*}
        \item \tbf{Example of showing linear independence.(2)} Show that the set $\{e^x, e^{2x}\}$ is linearly independent. We consider the following equation:
        \begin{equation*}
            0 = ae^x + be^{2x}~~~~~(1)
        \end{equation*}
        We take derivative on both sides of $(1)$ w.r.t. $x$ and we have:
        \begin{equation*}
            0 = ae^x + 2be^{2x}~~~~(2)
        \end{equation*}
        We subtract $(2)$ from $(1)$ to get:
        \begin{equation*}
            0 = be^{2x} \implies b = 0 \implies 0 = ae^x + 0 \implies a = 0
        \end{equation*}
        Since $a=b=0$ is the one and only solution to $(1)$, we claim they are linearly independent. \qed
    \end{enumerate}
    
\section{Interlude on solving SLEs}
    \begin{enumerate}
        \settag{1.5.*}
        \item \tbf{Note Aside: }This section of the book is covered, although not rigorously but completely, in MAT223. Hence the vast majority of definitions and corollary in this section were omitted. Consult the book for more detail on this.
        
        \settag{1.5.1}
        \item \tbf{Definition of (homogeneous) SLEs\footnote{System of Linear Equations}} A system of $m$ equations in $n$ unknowns $x_1,...,x_n$ of the form:
        \begin{align*}
            a_{11}\vec{x_1}+...+a_{1n}\vec{x_n} &= b_1 \\
            a_{21}\vec{x_1}+...+a_{2n}\vec{x_n} &= b_2 \\
            &\mbox{...} \\
            a_{m_1}\vec{x_1}+...+a_{mn}\vec{x_n} &= b_m
        \end{align*}
        where the $a_{ij},~b_i\in \R$, is called a \tit{system of linear equations.} The scalars $a_{ij}$ are called coefficients or \tit{weights} of the equations. We call this system \textbf{homogeneous} if and only if all the $b_i$ are 0's.
        
        \settag{1.5.2}
        \item \tbf{Definition of equivalent SLEs} Two systems of linear equations are said to be equivalent if their sets of solutions are the same(i.e., mutual inclusion of the two solution sets)
        
        \settag{1.5.3}
        \item \tbf{Propositions on operations on SLEs\footnote{This should be familiar from MAT223, operations involved in row reducing an augmented matrix for a system of linear equations}}
        \begin{enumerate}
            \item The system obtained buy adding any multiple of any one equation to any second equation, while leaving the other other equations unchanged, is an equivalent system.
            \item The system obtained by multiplying any one equation by a non-zero scalar and leaving the other equations unchanged is an equivalent system.
            \item The system obtained by interchanging any two equations is an equivalent system.
        \end{enumerate}
        
        \settag{1.5.13}
        \item \tbf{Corollary} If $m<n$, every homogeneous system of $m$ linear equations in $n$ unknowns has a non-trivial solution.
    \end{enumerate}
    

\section{Bases and Dimension}
    \begin{enumerate}
        \settag{1.6.1}
        \item \tbf{Definition of basis.} A subset $S$ of a vector space $V$ is called a basis if $V=Span(S)$ and the set $S$ is linearly independent.
        
        \settag{1.6.3}
        \item \tbf{Theorem. }Let $V$ be a vector space, and let $S$ be a non-empty subset of $V$. Then $S$ is a basis of $V$ if and only if $\forall \vec{x} \in V, \vec{x}$ can be written \tit{uniquely} as a linear combination of the vectors in $S$.
        
        \settag{1.6.6}
        \item \tbf{Theorem. }Let $V$ be a vector space that has a finite spanning set, and let $S$ be a linearly independent subset of $V$. Then there exists a basis $S'$ of $V$, such that $S\subseteq S'.$ Note that this theorem is can be summarized as follows: Every linearly independent set of vectors could be extended to a basis. We do so by adding yet another vector that is linearly independent to all the vectors already in the set, but notice that this process should possibly be repeated but \tit{not} infinite.
        
        \settag{1.6.8}
        \item \tbf{Lemma on linear independence towards a set and a vector.} Let $S$ be a linearly independent subset of $V$ and let $\vec{x}\in V$, but $\vec{x} \notin S$. Then $S\cup \{\vec{x} \}$ is linearly independent if and only if $\vec{x} \notin Span(S)$.
        
        \settag{1.6.10}
        \item \tbf{Theorem on cardinality of linearly independent set.} Let $V$ be a vector space and let $S$ be a spanning set for $V$, which has $m$ elements. Then no linearly independent set in $V$ can have more than $m$ elements.\newline
        \tit{\underline{Proof:}}\newline
        To show that there are no linearly independent set in $V$ that can have more than $m$ elements in it, it suffices to show that every set in $V$ that has more than $m$ elements in it is linearly dependent. Let $S=\{y_1,...,y_m\}$ and $S'=\{y_1,...,y_n\}\subset V$ where $n>m$. Now we consider the following equation:
        \begin{equation}
            a_1\vec{x}_1 + ... + a_n\vec{x}_n = \vec{0}~~~~\text{    where }\vec{x}_i \in S',a_i\in \R
        \end{equation}
        Since $S$ is a spanning set for $V$, then $\exists b_{ij}\in \R,~s.t.~\forall 1\leq i \leq m$:
        \begin{equation*}
            \vec{x}_i = b_{i1}\vec{y}_1 + ... + b_{im}\vec{y}_m = \sum_{j=1}^{m}b_{ij}\vec{y}_j
        \end{equation*}
        Then we substitute the $\vec{x}_i$'s into equation (1.1):
        \begin{equation}
            a_1\left(\sum_{j=1}^{m}b_{1j}\vec{y}_j\right) + ... +  a_n\left(\sum_{j=1}^{m}b_{nj}\vec{y}_j\right) = \vec{0}
        \end{equation}
        Rearranging (1.2), we have:
        \begin{equation}
            \left(\sum_{j=1}^{n}b_{1j}a_{j}\right)\vec{y}_1 + ... +  \left(\sum_{j=1}^{n}b_{mj}a_{j}\right)\vec{y}_m = \vec{0}
        \end{equation}
        Now we consider the following SLE(The coefficients of (1.3)):
        \begin{center}
            $
            \begin{cases}
            b_{11}a_1 + ... + b_{1n}a_n &= 0 \\
            b_{21}a_1 + ... + b_{2n}a_n &= 0 \\
            &\vdots \\
            b_{m1}a_1 + ... + b_{mn}a_n &= 0
            \end{cases}
            $
        \end{center}
        If we can find a set of $a_1, ..., a_n$ that are not all zero that solves the above SLE, then those scalars will give us a linear dependence among the vectors in $S'$ in (1.1). But the above SLE is a system of homogeneous linear equations in the $n$ unknowns $a_i$'s, hence since $m < n$, by Corollary (1.5.13), there exists a non-trivial solution. Then $S'$ is linearly dependent and this completes the proof. \qed
        
        
        \settag{1.6.11}
        \item \tbf{Bases of a vector space should shall have same cardinality.} Let $V$ be a vector space and let $S$ and $S'$ be two bases of  with $m$ and $m'$ elements, respectively. Then $m=m;$. \newline
        \tit{\underline{Proof: (Euclid's style)}} \newline
        Let $S$ and $S'$ with properties given as above. Since they are bases, we know: (By Theorem 1.6.10)
        \begin{enumerate}
            \item $S$ is a spanning set and $S'$ is a linearly independent set \newline $\Longrightarrow$ $m'=|S'| \leq |S|=m$
            \item $S'$ is a spanning set and $S$ is a linearly independent set \newline $\Longrightarrow$ $m=|S| \leq |S'|=m'$
        \end{enumerate}
        Then, $m'=m$. \qed
        
        \settag{1.6.12}
        \item \tbf{Definitions of (in)finite-dimension}
        \begin{enumerate}
            \item If $V$ is a vector space with some finite basis (possibly empty), we sat $V$ is \tit{finite-dimensional}. We say the vector space is \tit{infinite-dimensional} otherwise.
            \item Let $V$ be a \tit{finite-dimensional} vector space. The dimension of $V$, denoted as dim($V$), is the number of elements in a (hence any) basis of $V$.
            \item If $V=\{\vec{0}\}$, we define dim($V$)$=0$. \tbf{Remark: }Please note that is consistent with our definition of span of a empty set. We defined the span of empty set to be $\{\vec{0}\}$, and hence $\{\vec{0}\}$ is the resulting space of all the possibly linear combination of vectors in the empty set(where there is none). Then the cardinality of the empty set(which is zero) is here defined as the dimension of the zero vector space.
        \end{enumerate}
        
        \settag{1.6.13}
        \item \tbf{Examples on dimensions of vector spaces. }If we have a basis of $V$, then computing dim($V$) is simply a matter of counting the number of vectors in a (hence any) basis for the vectors space. We explore this by looking at the following examples
        \begin{enumerate}
            \item For each $n$, dim($\R^n$) $=n$. This is a consequence of the standard basis for $\R^n$ is of the form: $\{\vec{e}_1, ..., \vec{e}_n\}$ has $n$ elements in it.
            \item dim($P_n(\R)$) $=n+1$, since $|\{1, x^1, x^2, ..., x^n\}|=n+1$.\footnote{The absolute value marks around a set returns the cardinality of the set.}
            \item The vector spaces $P(\R)$\footnote{Here $P(\R)$ means the vector space of all polynomials that are $\R \xrightarrow{} \R$.}$,~C^n(\R)$, where $n\in \mathbb{N}^{\geq 0}$, are not of finite dimension, and hence are called \tit{infinite dimensional}.
        \end{enumerate}
        
        \settag{1.6.14}
        \item \tbf{Corollary. }Let $W$ be a subspace of a finite-dimensional vector space $V$. Then dim($W$) $\leq$ dim($V$). Furthermore, dim($W$) $=$ dim($V$) if and only if $W=V$.
        
        \settag{1.6.15}
        \item \tbf{Corollary. }Let $W$ be a subspace of $\R^n$ defined by a system of homogeneous linear equations. The dim($W$) is equal to the number of free variables in the corresponding echelon form of the equations.
        
        \settag{1.6.18}
        \item \tbf{Inclusion-Exclusion principle of dimensions.} Let $W_1$ and $W_2$ be finite dimensional sub-spaces of a vector space $V$. Then,
        \begin{center}
            dim($W_1+W_2$) = dim($W_1$) $+$ dim($W_2$) $-$ dim($W_1\cap W_2$)
        \end{center}
        
    \end{enumerate}
    
\chapter{Linear Transformations}
\section{Linear Transformations}

    \begin{enumerate}
        \settag{2.1.1}
        \item \tbf{Definition of linear transformation.\footnote{Familiar from MAT223}} A function $T:V\xrightarrow{}W$ is called a \tit{linear mapping} or a \tit{linear transformation} if it satisfies:
        \begin{enumerate}
            \item $\forall \vec{u},\vec{v}\in V,~T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{u})$
            \item $\forall \alpha \in \R, \vec{v} \in V,~ T(\alpha\vec{v}) = \alpha T(\vec{v})$
        \end{enumerate}
        
        \settag{2.1.2}
        \item \tbf{Proposition: alternative definition of L.T.s} A function $T:V\xrightarrow{}W$ is a linear transformation if and only if $\forall \alpha,\beta \in \R, \forall \vec{u}, \vec{v} \in V, T(\alpha \vec{u}+\beta \vec{v})=\alpha T(\vec{u}) + \beta T(\vec{v})$.\newline
        \tit{\underline{Proof($\implies$):}}\newline
        Assuming that $T$ is a linear transformation, then the definition in (2.1.1) must satisfy. Let $\alpha,\beta \in \R, \vec{u},\vec{v}\in V$, then: 
        \begin{align*}
            T(\alpha\vec{u} + \beta\vec{v}) &= T(\alpha\vec{u}) + T(\beta\vec{u})~~~~\text{// by (a)} \\
            &= \alpha T(\vec{u}) + \beta T(\vec{v})~~~~\text{// by (b)}
        \end{align*}
        \tit{\underline{Proof($\impliedby$):}}\newline
        Assuming the alternative definition, we want to show the definition given in (2.1.1). Since the quantifier is $\forall$, we can take $\alpha = \beta  =1, \text{(arbitrary)}~\vec{u},\vec{v}\in V$ in the alternative definition which directly yields us (a) of (2.1.1). Then we take $\alpha \in \R, \beta =0, \vec{u}, \vec{v}\in V$. In this case, 
        \begin{align*}
            T(\alpha\vec{u} + \beta\vec{v}) &= T(\alpha\vec{u} + 0\vec{v}) \\
            &= \alpha T(\vec{u}) + 0T(\vec{u}) \\
            &= \alpha T(\vec{u})
        \end{align*}
        Since $\alpha\in \R, \vec{u}\in V$ are arbitrary, this completes the proof. \qed
        
        \settag{2.1.3}
        \item \tbf{Corollary.} A function $T:V\xrightarrow{}W$ is a linear transformation if and only if
        \begin{equation*}
            \forall a_1,...,a_k\in \R, \forall \vec{v}_1, ...,\vec{k}\in V:~ 
            T\left(\sum_{i=1}^{k}a_i\vec{v}_i\right) = \sum_{i=1}^{k}a_i T\left(\vec{v}_i\right)
        \end{equation*}
        \tit{\underline{Proof:}} %todo. I think this question could use some help from simple induction to formalize a little more.
        
        \settag{2.1.9}
        \item \tbf{Angle between two vectors.} If $\vec{0}\neq \vec{a}\in \R^2$ and $\vec{0}\neq \vec{b}\in \R^2$, then the angle $\theta$ between them must be\footnote{The angle brackets here denotes the inner product of vectors}
        \begin{equation*}
            \theta = \arccos\left( \frac{\left<\vec{a},\vec{b}\right>}{\lVert\vec{a}\rVert \cdot \lVert\vec{b}\rVert}\right)
        \end{equation*}
        \tbf{Remark.} Notice that this definition could also be extended to $\R^3$.
        
        \settag{2.1.10}
        \item \tbf{Corollary on orthogonality of vectors.} If $\R^2\ni \vec{a}\neq \vec{0}$ and $\R^2\ni \vec{b}\neq \vec{0}$, then the angle $\theta$ between them is a right angle if and only if $\left<\vec{a},\vec{b}\right>=0$.\newline
        \tbf{Remark.} Note this definition of orthogonality could be extended to all euclidean vector spaces. The orthogonal complement of a subspace is the space of all vectors that are orthogonal to every vector in the subspace. In a three-dimensional Euclidean vector space, the orthogonal complement of a line through the origin is the plane through the origin perpendicular to it, and vice versa. In four-dimensional Euclidean space, for example, the orthogonal complement of a line is a hyper-plane and vice versa, and that of a plane is a plane.
        
        \settag{2.1.14}
        \item \tbf{Proposition.} If $T:V\xrightarrow{}W$ is a linear transformation and $V$ is finite-dimensional, then $T$ is uniquely determined by its values on the members of a basis of $V$. To make this proposition more clear, we present the proof below. We will show that if $S$ and $T$ are linear transformations that take the same values on each member of a basis for $V$, then in fact $S=T$.\newline
        \tit{\underline{Proof:}} \newline
        Let $\{ \vec{v}_1, ..., \vec{v}_k\}$ be a basis for $V$, and let $S$ and $T$ be two linear transformations that satisfy $T(\vec{v}_1)=S(\vec{v}_1), \forall i \in \{1,...,k\}$. If $\vec{v}\in V, ~\vec{v} = a_1\vec{v}_1 + ... + a_k\vec{k}$, then
        \begin{align*}
            T(\vec{v}) &= T(a_1\vec{v}_1 + ... + a_k\vec{k}) \\
            &= a_1T(\vec{v}_1) + ... + a_k T(\vec{v}_k)~~~~\text{// Since $T$ is linear}\\
            &= a_1S(\vec{v}_1) + ... + a_k S(\vec{v}_k) \\
            &= S(a_1\vec{v}_1 + ... + a_k\vec{k})~~~~~~~~~~\text{// Since $S$ is linear} \\
            &=S(\vec{v})
        \end{align*}
        Hence $S$ and $T$ are equal as mappings from $V$ to $W$. \qed
        
    \end{enumerate}
    
\section{L.Ts between finite dimensional v.s's}
    \begin{enumerate}
        \settag{2.2.1}
        \item \tbf{Proposition.} Let $T:V\xrightarrow{}W$ be a linear transformation between the finite dimensional vector spaces $V$ and $W$. If $\{\vec{v}_1,...,\vec{v}_k\}$ is a basis for $V$ and $\{\vec{w}_1,...,\vec{w}_l\}$ is a basis for $W$, then $T:V\xrightarrow{}W$ is uniquely determined by the $l\cdot k$ scalars used to express $T(\vec{v}_j)$, where $j\in \{1,...,k\}$, in terms of $\vec{w}_1,...,\vec{w}_l$.
        
        \settag{}
    \end{enumerate}
    
\end{document}