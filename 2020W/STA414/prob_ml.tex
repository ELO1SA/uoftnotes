\documentclass[11pt]{article}
\usepackage[margin=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{xargs}
\usepackage[
    pdftex, 
    dvipsnames
]{xcolor}
\usepackage[
    colorlinks=true,
    linkcolor=black,
    urlcolor=Thistle,
    citecolor=DarkOrchid
]{hyperref}
\usepackage{fancyhdr}
\usepackage{datetime}
\usepackage{ccicons}
\usepackage{tikz}
\usepackage{cancel}
\usepackage{mdframed}
%\usepackage[outputdir=.texpadtmp]{minted}

% ==== License =====
\usepackage[
    type={CC}, 
    modifier={by-nc-sa}, 
    version={4.0},
]{doclicense}

% ==== todo notes ====
\usepackage[
    colorinlistoftodos,
    prependcaption,
    textsize=tiny
]{todonotes}
\newcommandx{\note}[2][1=]{\todo[linecolor=Thistle,backgroundcolor=Thistle!25,bordercolor=Thistle,#1]{#2}}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}

% General
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\argmax}{\arg\max}
\newcommand{\kl}{\mathrm{KL}}

\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}

% Math Bold Font, Vector Notations
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bzero}{\mathbf{0}}

% Proofs, Structures
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\qed}{\hfill $\blacksquare$}

% Number Spaces, Vector Space
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\text{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\independent}{\perp}

\usepackage{chngcntr}
\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{equation}{section}

\author{\ccLogo \,\,by Xia, Tingfeng}
\title{\textsc{Statistical Methods for Machine Learning II}}
\date{2020 winter term}

\begin{document}
\maketitle
\doclicenseThis

\section*{Preface} 
This document is consist of notes from lectures and the online course notes that I, personally, find interesting/important. You can find the online course notes on the course website here: \url{https://probmlcourse.github.io/sta414/}

\tableofcontents

\newpage

\section{Lecture 2 - Introduction to Probabilistic Models}
\subsection{Overview}
We have a random vector in the form $X = (X_1,\dots,X_m)$ which can be \textit{\textbf{either observed or unobserved}}. To approach this in a generative way, we make the so called generative assumption. which is that $X\sim P_{true}(X)$, i.e. there is some true distribution that is behind the scene and our data is from such distribution. 

\paragraph{Goal} Model a parametric joint distribution $P_\theta (X)$ by learning the parameters. The learning here means we want to find a/the ``close''/``best'' estimation to our parameter $\theta$. In this course we will investigate the following three problems,
\begin{itemize}
    \item How to specify the joint, $P_\theta(X)$?
    \item What does ``best''/``close'' mean? In some sense we want to find $P_\theta \approx P_{true}$, however $P_{true}$ might also be unknown.
    \item How to find the best $\theta$? In this course we will generally rely on gradient methods, so $\nabla_\theta$...
\end{itemize}

\subsection{Probabilistic Perspective on ML}
With this perspective, we can think about common machine learning tasks differently, where random variables represent:
\begin{itemize}
    \item $X$: (high dimensional) input data
    \item $C$: discrete label
    \item $Y$: continuous target
\end{itemize}
If we assume our knowledge of the joint of the above three, i.e. we know $P(X,C,Y)$, then we can write our familiar tasks in the following way
\begin{itemize}
    \item \textit{\textbf{Regression}}:
        \begin{equation}
            p(Y|X) = \frac{p(X,Y)}{P(X)} = \frac{p(X,Y)}{\int p(X,Y)dY}
        \end{equation}
    \item \textit{\textbf{Classification/Clustering}}:
        \begin{equation}
            p(C|X) = \frac{p(X,C)}{\sum_{C'}p(X,C')}
        \end{equation}
\end{itemize}

\subsubsection{(Example) Classification} Suppose we have data of the form $\mathcal{D} = \{(x,c)_i\}_i$. We assume that they came from a certain true distribution, i.e. $\{(x,c)_i\}_i \sim p(X,C)$. Then, the ultimate goal of the ML problem is converted into finding $p(C|X)$. Using Bayes Rule of total probability, we can expand the distribution of interest into
\begin{equation}
    p(C | X)= \frac{p(X,C)}{P(X)} =\frac{p(X, C)}{\sum_{C'} p(X, C')}
\end{equation}

\paragraph{Output Heuristics} After we acquire $p(C|X)$ as above, we are one step away from our goal of output the actual prediction $c^*$. There are three ways that we can do this, namely
\begin{itemize}
    \item \textit{\textbf{MLE Estimate}} is the most intuitive one, we simply choose
        \begin{equation}
            c^* = \argmax_c p(C = c|X)
        \end{equation}
    \item \textit{\textbf{Sample Learnt Dist}} is another approach which produces non-deterministic results, i.e. we sample $c^* \sim p(C|X)$.
    \item \textit{\textbf{Combined}} is usually a safe way of doing this. We output
        \begin{equation}
            (c^*, p(C=c^*|X)) \iff (\text{result}, \text{how sure?})
        \end{equation}
    As an example, we have a ML algorithm drives a car. In this case, we might want to make decision only when the machine learning model has a certain level of confidence. 
\end{itemize}

\subsection{Observed vs Unobserved Random Variables}
\subsubsection{Supervised Dataset}
\begin{equation}
    \left\{x_{i}, c_{i}\right\}_{i=1}^{N} \sim p(X, C)
\end{equation}
In such case, the class labels are observed and finding the conditional distribution $p(C|X)$ satisfies the supervised classification problem. 

\subsubsection{Unsupervised Dataset}
\begin{equation}
    \left\{x_{i}\right\}_{i=1}^{N} \sim p(X, C)
\end{equation}
Still under the generative assumption, where we assume that there is some underlying distribution for our dataset. Further, we assume that the distribution of data is related to the class labels for the data points even though the class labels are never observed. \textbf{A common way to refer to an unobserved discrete class label is ``cluster''}. However, in this case, our final goal of classification is still $p(C|X)$\footnote{Might be helpful to think of Gaussian Mixture Models}. 

\subsubsection{Latent Variables} 
\begin{quote}
    Further, like clusters, introducing assumptions about unobserved variables is a powerful modelling tool. We will make use of this by modelling variables which are never observed in the dataset, called latent or hidden variables. By introducing and modelling latent variables, we will be able to naturally describe and capture abstract features of our input data.
\end{quote}

\subsection{Operations on Probabilistic Models}
\begin{itemize}
    \item \textit{\textbf{Generate Data}}: Sample from the model.
    \item \textit{\textbf{Estimate Likelihood}}: When all variables are either observed or marginalized, we produce the result which is a single real number that describes the `probability' of the all variables taking on those specific values.
    \item \textit{\textbf{Inference}}: Compute the expected value of some variables given others which are either observed or marginalized. 
    \item \textit{\textbf{Learning}}: Set the parameters of the joint distribution given some observed data to maximize the probability of the observed data. 
\end{itemize}

\subsection{Desiderata of Probabilistic Models}
We have two desires for the joint distribution to learn, namely
\begin{itemize}
    \item The marginal and conditional distribution can be computed efficiently
    \item The representation of the joint distribution should be compact. This is especially important when we are dealing with joint distributions over many variables.
\end{itemize}
In general, total joint distribution are too large to specify and would require an insane amount of data to fit even we wanted to. Thus, we need modelling assumptions. 

\subsubsection{Fully Dependent Factorization (Chain Rule)}
Suppose we have sample space defined such that $|T| = 2$, $|W| = 3$, and $|M| = 4$. Then the total joint distribution could be expanding using the chain rule as (\textit{note: not unique})
\begin{equation}
    P_\theta(T,W,M) = P(T)P(W|T)P(M|T,W)
\end{equation} 
which requires\footnote{Here the bars mean ``cardinality'' rather than vector norm} $|\theta| = (2 - 1) + (3 - 1) \times 2 + (4 - 1)\times 2 \times 3 = 23$ parameters in total to specify. 

\subsubsection{Assumptions (Independence)}
Introducing assumptions results in
\begin{itemize}
    \item a less expressive model
    \item $|\theta|$ (usually, much) smaller
\end{itemize}
which can be bad sometimes (since the model is less expressive) and is a trade-off that we as modellers have to deal with. 

\paragraph{(Example) Fully Independent}
We assume that $T\independent W \independent M$, then by definition we know, for example, $P(W|T)= P(W)$. Then
\begin{align}
    P_\theta(T,W,M) &= P(T)P(W|T)P(M|T,W) \\
    &= P(T)P(W)P(M)
\end{align}
which requires only $|\theta| = (2-1) + (3-1) + (4-1) = 6$ to fit.

\subsection{Likelihood Function}
For some observed data $X$, the likelihood describes the likeliness of the data under then distribution with parameter $\theta$. 
\begin{equation}
    L(\theta) = p(X|\theta)
\end{equation}

In general, we prefer to deal with the log likelihood function, which is defined as
\begin{equation}
    \ell(\theta;X) = \log L(\theta) = \log p(X|\theta)
\end{equation}

\subsection{Maximum Likelihood Estimation}
The idea is to find
\begin{equation}
    \hat{\theta}_{MLE} := \argmax_\theta \ell(\theta; \mathcal{D})
\end{equation}
In the case of i.i.d, we can re-write as
\begin{align}
    \hat{\theta}_{MLE} &= \argmax_\theta \ell(\theta; \mathcal{D})\\
    &= \argmax_\theta \log \prod_{m} p\left(x^{(m)} | \theta\right) \\
    &= \argmax_\theta\sum_{m} \log p\left(x^{(m)} | \theta\right)
\end{align}

\subsection{Sufficient Statistics}
\paragraph{(Definition) Statistic}
A statistic is a possibly vector valued \textit{deterministic} function of a set of random variables.

\paragraph{(Definition) Sufficient Statistic} 
is a statistic that conveys exactly the same information about the data generating process that created the data as the entire data itself.\footnote{Could also be interpreted as ``summarize the data with respect to the likelihood''} In formal language, Sufficient Statistic ($T(X)$) for $X$ could be defined as
\begin{equation}
    T(X^{(1)}) = T(X^{(2)})~ \implies~ L(\theta; X^{(1)}) = L(\theta; X^{(2)}), ~~\forall \theta
\end{equation}
alternatively, we can define it as
\begin{equation}
    P(\theta | T(X)) = P(\theta | X); ~~~\text{\color{Gray} i.e. data doesn't give further info}
\end{equation}
\subsubsection{Fisher-Neyman Factorization Theorem}
If the probability function is $f_\theta(x)$, then $T$ is a sufficient statistic for $\theta$ if and only if non-negative functions $g$ and $h$ can be found such that
\begin{equation}
    P(\theta | T(X))=h(x, T(x)) g(T(x), \theta)
\end{equation}

\subsection{Exponential Family}
Factorizes as
\begin{align}
    p(x | \eta) 
    &=h(x) \exp \left\{\eta' T(x)-g(\eta)\right\} \\
    &=h(x) g(\eta) \exp \left\{\eta' T(x)\right\}
\end{align}
\subsubsection{(Example) 1-D Gaussian}
\begin{align}
    p(x|\theta) = \mathcal{N}(x|\mu, \sigma)
    &= \frac{1}{\sqrt{2\pi \sigma^2}}\exp \left(- \frac{1}{2\sigma^2} (x - \mu)^2 \right) \\
    &= \frac{1}{\sqrt{2\pi \sigma^2}}\exp \left(- \frac{1}{2\sigma^2} (x^2 - 2x\mu + \mu^2) \right) \\
    &= 
        \underbrace{\frac{1}{\sqrt{2\pi}\sigma}}_{h(x)} 
        \underbrace{\exp\left( \frac{-\mu^2}{2\sigma^2} \right)}_{g(\eta)}
    \exp\left(
    \underbrace{
        \begin{bmatrix}
            \frac{\mu}{\sigma^2} 
            & \frac{-1}{2\sigma^2}
        \end{bmatrix}}_{\eta}
    \underbrace{
        \begin{bmatrix}
            x \\
            x^2
        \end{bmatrix}}_{T(X)}
    \right)
\end{align}



\section{Lecture 3 - Directed Graphical Models}
\subsection{Decision Theory (Utility Theory)}
Let $a$ denote action, and $a^*$ be the optimal one. Also use $s$ to denote state and $V(\cdot)$\note{Often, it is very hard to find a/the good/best value function that quantifies everything using a numerical value. } be the value function. We have, in general
\begin{equation}
    a^* = \arg\min_a/\argmax_a \underbrace{{\mathbb{E}}_{p(s\vert a, knowledge)}\left[V(s)\right]}_{u(a)~ \overset{\Delta}{=} ~\text{utility of action $a$}}
\end{equation}

\subsection{Graphical Model Notation}
\subsubsection{Chain Rule Expansion} Given any joint probability of $N$ random variables, we can expand it as follows
\begin{equation}
    p\left(x_{1, \ldots, N}\right)=p\left(x_{1}\right) p\left(x_{2} | x_{1}\right) p\left(x_{3} | x_{2}, x_{1}\right) \ldots p\left(x_{n} | x_{n-1: 1}\right)
\end{equation}
Formally speaking, in the case of two random variables would simply
\begin{equation}
    p(x, y)=p(x | y) p(y)
\end{equation}
and the general case for $N$ random variables could be written as
\footnote{Note: when $k = 1$, $p\left(x_{k} | \cap_{j=1}^{k-1} x_{j}\right) = p(x_1)$}
\begin{equation}
    p\left(\bigcap_{i=1}^{N} x_{i}\right)
    = \prod_{j=1}^{N} p\left(x_{j} \left\vert\,\, \bigcap_{k=1}^{j-1} x_{k}\right)\right.
\end{equation}

\subsubsection{Graph Representation} 
\paragraph{(Example) Grouping Variables} Consider the model
\begin{equation}
    p\left(x_{i}, x_{\pi_{i}}\right)=p\left(x_{\pi_{i}}\right) p\left(x_{i} | x_{\pi_{i}}\right)
\end{equation}
which we can use the following graph to represent:
\begin{center}
    \begin{tikzpicture}[scale=0.2]
        \tikzstyle{every node}+=[inner sep=0pt]
        \draw [black] (25.7,-27.6) circle (3);
        \draw (25.7,-27.6) node {$x_{\pi_1}$};
        \draw [black] (41.1,-27.6) circle (3);
        \draw (41.1,-27.6) node {$x_1$};
        \draw [black] (28.7,-27.6) -- (38.1,-27.6);
        \fill [black] (38.1,-27.6) -- (37.3,-27.1) -- (37.3,-28.1);
    \end{tikzpicture}
\end{center}
where
\begin{itemize}
    \item \textit{\textbf{nodes}} represent random variables
    \item \textit{\textbf{arrows}} mean ``conditioned on'', e.g. ``$x_i$ is conditioned on $x_{\pi_i}$''
\end{itemize}
Notice that we can always group the variables together into one bigger variable, so in this example, $x_{\pi_i}$ might represent a group of variables in stead of just one. 


\paragraph{(Example) Fully Dependent 6 Nodes}
The total expansion of $p(x_{1:6})$ could be represented as
\begin{center}
    \begin{tikzpicture}[scale=0.2]
        \tikzstyle{every node}+=[inner sep=0pt]
        \draw [black] (19.3,-19.9) circle (3);
        \draw (19.3,-19.9) node {$x_1$};
        \draw [black] (36.4,-15.5) circle (3);
        \draw (36.4,-15.5) node {$x_2$};
        \draw [black] (53.6,-19.9) circle (3);
        \draw (53.6,-19.9) node {$x_3$};
        \draw [black] (53.6,-31.7) circle (3);
        \draw (53.6,-31.7) node {$x_4$};
        \draw [black] (19.3,-31.7) circle (3);
        \draw (19.3,-31.7) node {$x_6$};
        \draw [black] (36.4,-36.4) circle (3);
        \draw (36.4,-36.4) node {$x_5$};
        \draw [black] (22.21,-19.15) -- (33.49,-16.25);
        \fill [black] (33.49,-16.25) -- (32.6,-15.96) -- (32.84,-16.93);
        \draw [black] (22.3,-19.9) -- (50.6,-19.9);
        \fill [black] (50.6,-19.9) -- (49.8,-19.4) -- (49.8,-20.4);
        \draw [black] (22.14,-20.88) -- (50.76,-30.72);
        \fill [black] (50.76,-30.72) -- (50.17,-29.99) -- (49.84,-30.94);
        \draw [black] (21.46,-21.98) -- (34.24,-34.32);
        \fill [black] (34.24,-34.32) -- (34.01,-33.4) -- (33.32,-34.12);
        \draw [black] (19.3,-22.9) -- (19.3,-28.7);
        \fill [black] (19.3,-28.7) -- (19.8,-27.9) -- (18.8,-27.9);
        \draw [black] (39.31,-16.24) -- (50.69,-19.16);
        \fill [black] (50.69,-19.16) -- (50.04,-18.47) -- (49.79,-19.44);
        \draw [black] (38.58,-17.56) -- (51.42,-29.64);
        \fill [black] (51.42,-29.64) -- (51.18,-28.73) -- (50.49,-29.46);
        \draw [black] (36.4,-18.5) -- (36.4,-33.4);
        \fill [black] (36.4,-33.4) -- (36.9,-32.6) -- (35.9,-32.6);
        \draw [black] (34.22,-17.56) -- (21.48,-29.64);
        \fill [black] (21.48,-29.64) -- (22.4,-29.45) -- (21.71,-28.72);
        \draw [black] (53.6,-22.9) -- (53.6,-28.7);
        \fill [black] (53.6,-28.7) -- (54.1,-27.9) -- (53.1,-27.9);
        \draw [black] (51.44,-21.98) -- (38.56,-34.32);
        \fill [black] (38.56,-34.32) -- (39.49,-34.13) -- (38.8,-33.41);
        \draw [black] (50.76,-20.88) -- (22.14,-30.72);
        \fill [black] (22.14,-30.72) -- (23.06,-30.94) -- (22.73,-29.99);
        \draw [black] (50.71,-32.49) -- (39.29,-35.61);
        \fill [black] (39.29,-35.61) -- (40.2,-35.88) -- (39.93,-34.92);
        \draw [black] (50.6,-31.7) -- (22.3,-31.7);
        \fill [black] (22.3,-31.7) -- (23.1,-32.2) -- (23.1,-31.2);
        \draw [black] (33.51,-35.6) -- (22.19,-32.5);
        \fill [black] (22.19,-32.5) -- (22.83,-33.19) -- (23.1,-32.22);
    \end{tikzpicture}
\end{center}
This is the resultant graphical model if we make \textit{\textbf{absolutely no assumption}} on the independence. Notice that such model grows exponentially in complexity with respect to the number of parameters considered. We say such model ``scales poorly''.

\subsubsection{Conditional Independence} 
\paragraph{(Definition) Conditional Independence} Let $X$ be the set of nodes in our graph (the random variables of our model), then two sets of variables $X_A, X_B$ are said to be conditionally independent given a third set of variables $X_C$ if and only if either
\begin{equation}
    p\left(X_{A}, X_{B} | X_{C}\right)=p\left(X_{A} | X_{C}\right) p\left(X_{B} | X_{C}\right)
\end{equation}
or (\textit{\textbf{\#! Important!}})
\begin{align}
    &p\left(X_{A} | X_{B}, X_{C}\right)=p\left(X_{A} | X_{C}\right) \\
    \iff &p\left(X_{B} | X_{A}, X_{C}\right)=p\left(X_{B} | X_{C}\right)
\end{align}
and we denote the relation as $\left(X_{A} \independent X_{B} | X_{C}\right)$

\subsubsection{Plates}
In Bayesian methods, we treat parameters as random variables and hence we would like to include them in out graphical model. However, adding a node for each observation is quite cumbersome and thus we introduce plates, which denote replication of random variables. 

\paragraph{Nested Plates}
Plates could be nested, in which case their arrows get duplicated also, \textit{\textbf{according to the rule:}} draw an arrow from every copy of the source node to every copy of the destination node.

\paragraph{Crossing Plates}
Plates can also cross (intersect), in which case the nodes at the intersection have multiple indices and get duplicated a number of times equal to the product of the duplication numbers on all the plates containing them.


\subsection{Directed Acyclic Graphical Models (DAGM)}
A directed acyclic graphical model over $N$ random variables look like
\begin{equation}
    p\left(x_{1:N}\right)=\prod_{i}^{N} p\left(x_{i} | x_{\pi_{i}}\right)
\end{equation}
where $x_i$ is a random variable and $x_{\pi_i}$ denotes the parents of the node (which could be an empty set). This notion is more general than the fully dependence model that looked at above. Notice that here each node is only dependent on its parents rather than all other nodes. Thus, the complexity of such model reduces to exponential in fan-in of each node, instead of the total $N$. 

\subsubsection{Independence Assumption on DAGMs}
% TODO: Topological Sort?
Then, we have the independence relationship of\footnote{Requires topological ordering, to be added later. }
$
    x_i \independent x_{Ancestor({\pi_i})} \vert x_{\pi_i}
$
which expands into
\begin{equation}
    p\left(x_{1, \ldots, 6}\right)=p\left(x_{1}\right) p\left(x_{2} | x_{1}\right) p\left(x_{3} | x_{1}\right) p\left(x_{4} | x_{2}\right) p\left(x_{5} | x_{3}\right) p\left(x_{6} | x_{2}, x_{5}\right)
\end{equation}
with the following respective graph
\begin{center}
    \begin{tikzpicture}[scale=0.2]
        \tikzstyle{every node}+=[inner sep=0pt]
        \draw [black] (19.3,-19.9) circle (3);
        \draw (19.3,-19.9) node {$x_1$};
        \draw [black] (36.4,-15.5) circle (3);
        \draw (36.4,-15.5) node {$x_2$};
        \draw [black] (53.6,-19.9) circle (3);
        \draw (53.6,-19.9) node {$x_3$};
        \draw [black] (53.6,-31.7) circle (3);
        \draw (53.6,-31.7) node {$x_4$};
        \draw [black] (19.3,-31.7) circle (3);
        \draw (19.3,-31.7) node {$x_6$};
        \draw [black] (36.4,-36.4) circle (3);
        \draw (36.4,-36.4) node {$x_5$};
        \draw [black] (22.21,-19.15) -- (33.49,-16.25);
        \fill [black] (33.49,-16.25) -- (32.6,-15.96) -- (32.84,-16.93);
        \draw [black] (34.22,-17.56) -- (21.48,-29.64);
        \fill [black] (21.48,-29.64) -- (22.4,-29.45) -- (21.71,-28.72);
        \draw [black] (38.58,-17.56) -- (51.42,-29.64);
        \fill [black] (51.42,-29.64) -- (51.18,-28.73) -- (50.49,-29.46);
        \draw [black] (22.3,-19.9) -- (50.6,-19.9);
        \fill [black] (50.6,-19.9) -- (49.8,-19.4) -- (49.8,-20.4);
        \draw [black] (51.44,-21.98) -- (38.56,-34.32);
        \fill [black] (38.56,-34.32) -- (39.49,-34.13) -- (38.8,-33.41);
        \draw [black] (33.51,-35.6) -- (22.19,-32.5);
        \fill [black] (22.19,-32.5) -- (22.83,-33.19) -- (23.1,-32.22);
    \end{tikzpicture}
\end{center}
As we can see, the introduction of the assumption greatly reduced the complexity of the model. 
\subsubsection{(Example) Markov Chain}
The following example has independence relationships that satisfies the Markov Property.
\begin{equation}
    p(x) = p(x_1)p(x_2|x_1)p(x_3|x_2)p(x_4|x_3)...
\end{equation}
and could be represented, in graphical model, as
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (14.5,-18.4) circle (3);
\draw (14.5,-18.4) node {$x_1$};
\draw [black] (14.5,-18.4) circle (2.4);
\draw [black] (24.3,-18.4) circle (3);
\draw (24.3,-18.4) node {$x_2$};
\draw [black] (24.3,-18.4) circle (2.4);
\draw [black] (34.3,-18.4) circle (3);
\draw (34.3,-18.4) node {$x_3$};
\draw [black] (34.3,-18.4) circle (2.4);
\draw [black] (44.3,-18.4) circle (3);
\draw (44.3,-18.4) node {$x_4$};
\draw [black] (44.3,-18.4) circle (2.4);
%\draw [black] (54.1,-18.4) circle (3);
\draw (54.1,-18.4) node {$...$};
\draw [black] (17.5,-18.4) -- (21.3,-18.4);
\fill [black] (21.3,-18.4) -- (20.5,-17.9) -- (20.5,-18.9);
\draw [black] (27.3,-18.4) -- (31.3,-18.4);
\fill [black] (31.3,-18.4) -- (30.5,-17.9) -- (30.5,-18.9);
\draw [black] (37.3,-18.4) -- (41.3,-18.4);
\fill [black] (41.3,-18.4) -- (40.5,-17.9) -- (40.5,-18.9);
\draw [black] (47.3,-18.4) -- (51.1,-18.4);
\fill [black] (51.1,-18.4) -- (50.3,-17.9) -- (50.3,-18.9);
\end{tikzpicture}
\end{center}




\subsection{Directed - Separation}
\subsubsection{(Definition) D - Separation}
Directed-separation is a notion of connectedness in DAGs in which two (sets of) variables may or may not be connected conditioned on a third (set of) variable(s). D-connection implies conditional dependence and d-separation implies conditional independence.
\subsubsection{Float Rules Derivation}
\paragraph{Notation}
The double circles (or shaded circles) represent the notion of ``conditioned-on''.
\paragraph{Chain}
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (11.9,-26.7) circle (3);
\draw (11.9,-26.7) node {$X$};
\draw [black] (21.9,-26.7) circle (3);
\draw (21.9,-26.7) node {$Y$};
\draw [black] (31.8,-26.7) circle (3);
\draw (31.8,-26.7) node {$Z$};
\draw [black] (46.6,-26.7) circle (3);
\draw (46.6,-26.7) node {$X$};
\draw [black] (56.3,-26.7) circle (3);
\draw (56.3,-26.7) node {$Y$};
\draw [black] (56.3,-26.7) circle (2.4);
\draw [black] (65.9,-26.7) circle (3);
\draw (65.9,-26.7) node {$Z$};
\draw [black] (14.9,-26.7) -- (18.9,-26.7);
\fill [black] (18.9,-26.7) -- (18.1,-26.2) -- (18.1,-27.2);
\draw [black] (24.9,-26.7) -- (28.8,-26.7);
\fill [black] (28.8,-26.7) -- (28,-26.2) -- (28,-27.2);
\draw [black] (49.6,-26.7) -- (53.3,-26.7);
\fill [black] (53.3,-26.7) -- (52.5,-26.2) -- (52.5,-27.2);
\draw [black] (59.3,-26.7) -- (62.9,-26.7);
\fill [black] (62.9,-26.7) -- (62.1,-26.2) -- (62.1,-27.2);
\end{tikzpicture}
\end{center}
In this case, we are interested in knowing whether or not
\begin{equation}
    (X\independent Z)| Y
\end{equation}
From the arrows between the nodes, we know that
\begin{equation}
    P(X,Y,Z) = P(X)P(Y|X)P(Z|Y)
\end{equation}
Then, we have
\begin{align}
    P(X,Z|Y) &= \frac{P(X,Y,Z)}{P(Y)} \\
    &= \frac{P(X)P(Y|X)P(Z|Y)}{P(Y)} \\
    &= \frac{P(X,Y)P(Z|Y)}{P(Y)} \\
    &= P(X|Y)P(Z|Y)
\end{align}
and this completes the proof. \qed

\paragraph{Common Clause}
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (22.8,-21) circle (3);
\draw (22.8,-21) node {$Y$};
\draw [black] (17,-32.7) circle (3);
\draw (17,-32.7) node {$X$};
\draw [black] (29.1,-32.7) circle (3);
\draw (29.1,-32.7) node {$Z$};
\draw [black] (54.9,-21) circle (3);
\draw (54.9,-21) node {$Y$};
\draw [black] (54.9,-21) circle (2.4);
\draw [black] (49.6,-32.7) circle (3);
\draw (49.6,-32.7) node {$X$};
\draw [black] (61.7,-32.7) circle (3);
\draw (61.7,-32.7) node {$Z$};
\draw [black] (53.66,-23.73) -- (50.84,-29.97);
\fill [black] (50.84,-29.97) -- (51.62,-29.44) -- (50.71,-29.03);
\draw [black] (56.41,-23.59) -- (60.19,-30.11);
\fill [black] (60.19,-30.11) -- (60.22,-29.16) -- (59.36,-29.67);
\draw [black] (21.47,-23.69) -- (18.33,-30.01);
\fill [black] (18.33,-30.01) -- (19.14,-29.52) -- (18.24,-29.07);
\draw [black] (24.22,-23.64) -- (27.68,-30.06);
\fill [black] (27.68,-30.06) -- (27.74,-29.12) -- (26.86,-29.59);
\end{tikzpicture}
\end{center}
As previous, we are interested in knowing whether or not $(X \perp Z) | Y$. From the graph, we can know that
\begin{equation}
    P(X,Y,Z) = P(Y)P(X|Y)P(Z|Y)
\end{equation}
Then, we have
\begin{align}
    P(X,Z|Y) 
    &= \frac{P(X,Y,Z)}{P(Y)} \\
    &= \frac{P(Y)P(X|Y)P(Z|Y)}{P(Y)} \\
    &= P(X|Y)P(Z|Y)
\end{align}
and this completes the proof. \qed

\paragraph{Explaining Away (Berkson's Paradox)}
\begin{center}
\begin{tikzpicture}[scale=0.2]
\tikzstyle{every node}+=[inner sep=0pt]
\draw [black] (15.9,-22.2) circle (3);
\draw (15.9,-22.2) node {$X$};
\draw [black] (26.8,-21.8) circle (3);
\draw (26.8,-21.8) node {$Z$};
\draw [black] (21.7,-33.3) circle (3);
\draw (21.7,-33.3) node {$Y$};
\draw [black] (45.8,-21.8) circle (3);
\draw (45.8,-21.8) node {$X$};
\draw [black] (57.2,-21.8) circle (3);
\draw (57.2,-21.8) node {$Z$};
\draw [black] (51.5,-33.3) circle (3);
\draw (51.5,-33.3) node {$Y$};
\draw [black] (51.5,-33.3) circle (2.4);
\draw [black] (55.87,-24.49) -- (52.83,-30.61);
\fill [black] (52.83,-30.61) -- (53.64,-30.12) -- (52.74,-29.67);
\draw [black] (47.13,-24.49) -- (50.17,-30.61);
\fill [black] (50.17,-30.61) -- (50.26,-29.67) -- (49.36,-30.12);
\draw [black] (25.58,-24.54) -- (22.92,-30.56);
\fill [black] (22.92,-30.56) -- (23.7,-30.03) -- (22.78,-29.62);
\draw [black] (17.29,-24.86) -- (20.31,-30.64);
\fill [black] (20.31,-30.64) -- (20.38,-29.7) -- (19.5,-30.16);
\end{tikzpicture}
\end{center}
Notice from the graph that we have $P(X,Y,Z) = P(X)P(Z)P(Y|X,Z)$.
First, I will prove that, in this case, $(X \not\perp Z) | Y$.
\begin{align}
    P(Z|X,Y) &= \frac{P(X)P(Z)P(Y|X,Z)}{P(X)P(Y|X)} \\
    &= \frac{P(Z)P(Y|X,Z)}{P(Y|X)} \\
    &\neq P(Z|Y)
\end{align}
For marginal independence, i.e. $(X|Z)$, we want to show that $P(X,Z) = P(X)P(Z)$. 
\begin{align}
    P(X,Z) &= \sum_{Y'} P(X,{Y'},Z) \\
    &= \sum_{Y'} P(X)P(Z)P({Y'}|X,Z) \\
    &= P(X)P(Z)\sum_{Y'}P({Y'}|X,Z) \\
    &= P(X)P(Z)    
\end{align}
\qed

\subsubsection{The Bayes-Ball Algorithm}
To check if $x_A\perp x_B | x_C$, we will need
\begin{itemize}
    \item Shade all nodes that were conditioned on, i.e. all nodes of $x_C$
    \item Place balls at each node in $x_A$ (or $x_B$)
    \item Let the balls bounce around according to rules that are yet to be states below
    \begin{itemize}
        \item If any of the balls reach any of the nodes in $x_B$ from $x_A$ (or reach $x_A$ from $x_B$) then we declare $x_{A} \not \perp x_{B} | x_{C}$
        \item Otherwise, $x_{A} \perp x_{B} | x_{C}$
    \end{itemize}
\end{itemize}

\subsubsection{Bayes Ball Rules}
\paragraph{Chain}
\begin{center}
    \includegraphics[scale=0.25]{img/bayes_ball_chain}
\end{center}
\paragraph{Common Cause}
\begin{center}
    \includegraphics[scale=0.25]{img/bayes_ball_common_cause}
\end{center}
\paragraph{Explain Away}
\begin{center}
    \includegraphics[scale=0.25]{img/bayes_ball_explain_away}
\end{center}
\paragraph{Linear}
\begin{center}
    \includegraphics[scale=0.25]{img/bayes_ball_linear}
\end{center}

\subsection{Unobserved Variables}
Certain variables in our models may be unobserved , either some of the time or always, at training time or at test time. Graphically, we use shading to indicate observation.
\subsubsection{Partially Unobserved Variables}\footnote{An concrete example would be hospital data, where typically a large proportion of the data is missing. }
If variables are occasionally unobserved hen they are missing data, e.g., undefined inputs, missing class labels, erroneous target values. In this case, we can still model the joint distribution, but we marginalize the missing values
\begin{align}
    \ell(\theta ; \mathcal{D})&=\sum_{\text {complete }} \log p\left(x^{c}, y^{c} | \theta\right)+\sum_{\text {missing }} \log p\left(x^{m} | \theta\right) \\
    &=\sum_{\text {complete }} \log p\left(x^{c}, y^{c} | \theta\right)+\sum_{\text {missing }} \log \sum_{y} p\left(x^{m}, y | \theta\right)
\end{align}

\subsubsection{Latent Variables}
Above we discussed the case where some data are non-deterministically unobserved. Latent variables refers to those that \textit{\textbf{never observed}}. The handling of the latent variables depends on where it appears in our model,
\begin{itemize}
    \item If we never condition on it when computing the probability of the variables we do observe, then we can just forget about it and integrate it out. For example, given $y$, $x$ we fit the model
        \begin{equation}
            p(z, y | x)=p(z | y) p(y | x, w) p(w)
        \end{equation}
    \item If $z$ is not a leaf node, marginalizing over it will induce dependencies between its children. For example, given $y, x$ we can fit the model 
        \begin{equation}
            p(y | x)=\sum_{z} p(y | x, z) p(z)
        \end{equation}
\end{itemize}

\subsubsection{Mixture Models}
In this case, we are looking at data that has no input on class information. We can sum the labels out,
\begin{equation}
    p(x | \theta)=\sum_{k=1}^{K} p\left(z=k | \theta_{z}\right) p\left(x | z=k, \theta_{k}\right)
\end{equation}
where bayes rule comes in handy for calculating the posterior (class responsibilities) of the mixture component given some data, i.e., 
\begin{equation}
    p\left(z=k | x, \theta_{z}\right)=\frac{p\left(z=k | \theta_{z}\right) p_{k}\left(x | \theta_{k}\right)}{\sum_{j} p\left(z=j | \theta_{z}\right) p_{j}\left(x | \theta_{j}\right)}
\end{equation} 
%
%\paragraph{(Example) K-Gaussian Mixture Model}
%\begin{itemize}
%    \item The density model $p(x|\theta)$ is the marginal density. 
%\end{itemize}

\subsection{Examples}
\subsubsection{Second-order Markov Chain}
Consider the model
\begin{equation}
    p\left(\mathbf{x}_{1: T}\right)=p\left(x_{1}, x_{2}\right) p\left(x_{3} | x_{1}, x_{2}\right) p\left(x_{4} | x_{2}, x_{3}\right) \dots = p\left(x_{1}, x_{2}\right) \prod_{t=3}^{T} p\left(x_{t} | x_{t-1}, x_{t-2}\right)
\end{equation}
which has the graphical representation (double circles mean ``observed'' here)
\begin{center}
    \begin{tikzpicture}[scale=0.2]
    \tikzstyle{every node}+=[inner sep=0pt]
    \draw [black] (14.9,-18.4) circle (3);
    \draw (14.9,-18.4) node {$x_1$};
    \draw [black] (14.9,-18.4) circle (2.4);
    \draw [black] (23.7,-25.3) circle (3);
    \draw (23.7,-25.3) node {$x_2$};
    \draw [black] (23.7,-25.3) circle (2.4);
    \draw [black] (32.1,-18.4) circle (3);
    \draw (32.1,-18.4) node {$x_3$};
    \draw [black] (32.1,-18.4) circle (2.4);
    \draw [black] (41.2,-25.3) circle (3);
    \draw (41.2,-25.3) node {$x_4$};
    \draw [black] (41.2,-25.3) circle (2.4);
%    \draw [black] (49.2,-18.4) circle (3);
    \draw (49.2,-18.4) node {$...$};
    \draw [black] (17.26,-20.25) -- (21.34,-23.45);
    \fill [black] (21.34,-23.45) -- (21.02,-22.56) -- (20.4,-23.35);
    \draw [black] (26.02,-23.4) -- (29.78,-20.3);
    \fill [black] (29.78,-20.3) -- (28.85,-20.43) -- (29.48,-21.2);
    \draw [black] (34.49,-20.21) -- (38.81,-23.49);
    \fill [black] (38.81,-23.49) -- (38.47,-22.61) -- (37.87,-23.4);
    \draw [black] (43.47,-23.34) -- (46.93,-20.36);
    \fill [black] (46.93,-20.36) -- (46,-20.5) -- (46.65,-21.26);
    \draw [black] (17.9,-18.4) -- (29.1,-18.4);
    \fill [black] (29.1,-18.4) -- (28.3,-17.9) -- (28.3,-18.9);
    \draw [black] (26.7,-25.3) -- (38.2,-25.3);
    \fill [black] (38.2,-25.3) -- (37.4,-24.8) -- (37.4,-25.8);
    \end{tikzpicture}
\end{center}
we notice that this model essentially assumes ``the present depends on the past only through the current state as well as the last one.''

\subsubsection{Hidden Markov Models (HMMs)}
HMM is a statistical model in which a system being modelled is assumed to be a Markov Process\footnote{In continuous time, the Markov Chain model is known as Markov Process} with un-observed states. Here is the graphical model, where double circles mean ``observed'' here:
\begin{center}
    \begin{tikzpicture}[scale=0.2]
        \tikzstyle{every node}+=[inner sep=0pt]
        \draw [black] (13.9,-20.9) circle (3);
        \draw (13.9,-20.9) node {$z_1$};
        \draw [black] (24.7,-20.9) circle (3);
        \draw (24.7,-20.9) node {$z_2$};
        \draw [black] (35,-20.9) circle (3);
        \draw (35,-20.9) node {$z_3$};
        \draw [black] (45.7,-20.9) circle (3);
        \draw (45.7,-20.9) node {$z_4$};
        \draw [black] (56.5,-20.9) circle (3);
        \draw (56.5,-20.9) node {$z_5$};
%        \draw [black] (67,-20.9) circle (3);
        \draw (67,-20.9) node {$...$};
        \draw [black] (13.9,-31.5) circle (3);
        \draw (13.9,-31.5) node {$x_1$};
        \draw [black] (13.9,-31.5) circle (2.4);
        \draw [black] (24.7,-31.5) circle (3);
        \draw (24.7,-31.5) node {$x_2$};
        \draw [black] (24.7,-31.5) circle (2.4);
        \draw [black] (35,-31.5) circle (3);
        \draw (35,-31.5) node {$x_3$};
        \draw [black] (35,-31.5) circle (2.4);
        \draw [black] (45.7,-31.5) circle (3);
        \draw (45.7,-31.5) node {$x_4$};
        \draw [black] (45.7,-31.5) circle (2.4);
        \draw [black] (56.5,-31.5) circle (3);
        \draw (56.5,-31.5) node {$x_5$};
        \draw [black] (56.5,-31.5) circle (2.4);
        \draw [black] (16.9,-20.9) -- (21.7,-20.9);
        \fill [black] (21.7,-20.9) -- (20.9,-20.4) -- (20.9,-21.4);
        \draw [black] (27.7,-20.9) -- (32,-20.9);
        \fill [black] (32,-20.9) -- (31.2,-20.4) -- (31.2,-21.4);
        \draw [black] (38,-20.9) -- (42.7,-20.9);
        \fill [black] (42.7,-20.9) -- (41.9,-20.4) -- (41.9,-21.4);
        \draw [black] (48.7,-20.9) -- (53.5,-20.9);
        \fill [black] (53.5,-20.9) -- (52.7,-20.4) -- (52.7,-21.4);
        \draw [black] (59.5,-20.9) -- (64,-20.9);
        \fill [black] (64,-20.9) -- (63.2,-20.4) -- (63.2,-21.4);
        \draw [black] (13.9,-23.9) -- (13.9,-28.5);
        \fill [black] (13.9,-28.5) -- (14.4,-27.7) -- (13.4,-27.7);
        \draw [black] (24.7,-23.9) -- (24.7,-28.5);
        \fill [black] (24.7,-28.5) -- (25.2,-27.7) -- (24.2,-27.7);
        \draw [black] (35,-23.9) -- (35,-28.5);
        \fill [black] (35,-28.5) -- (35.5,-27.7) -- (34.5,-27.7);
        \draw [black] (45.7,-23.9) -- (45.7,-28.5);
        \fill [black] (45.7,-28.5) -- (46.2,-27.7) -- (45.2,-27.7);
        \draw [black] (56.5,-23.9) -- (56.5,-28.5);
        \fill [black] (56.5,-28.5) -- (57,-27.7) -- (56,-27.7);
    \end{tikzpicture}
\end{center}
where
\begin{itemize}
    \item $z_t$ are hidden states taking on one of $K$ discrete values
    \item $x_t$ are observed variables taking on values in any space.
\end{itemize}
The above graph factorizes into
\begin{equation}
    p\left(X_{1: T}, Z_{1: T}\right)=p\left(Z_{1: T}\right) p\left(X_{1: T} | Z_{1: T}\right)=p\left(Z_{1}\right) \prod_{t=2}^{T} p\left(Z_{t} | Z_{t-1}\right) \prod_{t=1}^{T} p\left(X_{t} | Z_{t}\right)
\end{equation}

\section{Lecture 4 - Exact Inference}
\subsection{Variable Elimination}
\subsubsection{(Simple Example) Chain}
The example that we will consider is the simple chain
\begin{equation}
    A \rightarrow B \rightarrow C \rightarrow D
\end{equation}
where we want to compute $P(D)$, with no observation\note{No observations for other variables means that we have no `evidence' for other variables. } for other variables. We have\note{$X_F$  is the set of variable that we are interested in, $X_E$ is evidence, and $X_R$ is the set of extraneous variables, i.e. what we marginalize out}
\begin{equation}
    X_{F}=\{D\}, X_{E}=\{\}, X_{R}=\{A, B, C\}
\end{equation}
The graphical model gives the factorization of
\begin{equation}
    p(A, B, C, D)=p(A) p(B | A) p(C | B) p(D | C)
\end{equation}
thus, if we want to find $p(D)$ we can marginalize over all other variables, i.e.
\begin{align}
    p(D)&=\sum_{A, B, C} p(A, B, C, D) \\
    &=\sum_{C} \sum_{B} \sum_{A} p(A) p(B | A) p(C | B) p(D | C)
\end{align}
If we marginalize the above the na\"ive way, then it cost exponentially $\mathcal{O}(k^n)$. Thus, we need to find a \textit{\textbf{elimination ordering}} that gives us a smaller runtime complexity. We can reduce the complexity by first computing terms that appear across the other marginalization sums, \note{Here $\phi$ means factor, an non-normalized ``pmf''}
\begin{align}
    \phi(D) 
    &= \sum_C p(D|C) \sum_B p(C|B) \sum_A p(A)p(B|A) \\
    &= \sum_C p(D|C) \sum_B p(C|B) \phi(B) \\
    &= \sum_C p(D|C) \phi(C)
\end{align}

\subsection{Sum-Product Inference}
Let $X = Z \cup Y \wedge Z \cap Y = \emptyset$ be a set of random variables. \note{Here $Y$ is the set that we want to do inference on and thus we are marginalizing over everything in $Z$}. Then, to compute $P(Y)$ for a directed and undirected \unsure{how to generalize to undirected graph?} model could be achieved by the sum product inference algorithm
\begin{equation}
    \tau(Y)=\sum_{z}\prod_{\phi \in \Phi} \phi\left(\mathrm{Scope}(\phi) \cap Z, \mathrm{Scope}(\phi) \cap Y\right)\quad \forall Y
\end{equation}
where the scope for each $\phi$ simply means the set of all random variables that have appeared in that specific $\phi$, and $\Phi$ denotes the set of all $\phi$'s.

\subsubsection{SP - Inference in  Directed Models}
In a directed model, the $\Phi$ is given by the conditional probability distributions for all variables, i.e.
\begin{equation}
    \Phi=\left\{\phi_{x_{i}}\right\}_{i=1}^{N}=\left\{p\left(x_{i} | \text { parents }\left(x_{i}\right)\right)\right\}_{i=1}^{N}
\end{equation}
The resulting term $\tau(Y)$ will be automatically normalized.

\subsubsection{SP - Inference in Undirected Models}
For undirected models, $\Phi$ is given by the set of un-normalized potentials, and we \textit{\textbf{must}} normalize the resulting $\tau(Y)$ by $\sum_Y \tau(y)$

\subsubsection{(Example) Directed Graph}
We have the following factorization of the joint distribution
\begin{equation}
    p(C, D, I, G, S, L, H, J)=p(C) p(D | C) p(I) p(G | D, I) p(L | G) P(S | I) p(J | S, L) p(H | J, G)
\end{equation}
and we will need the following potentials
\begin{equation}
    \Phi =\{\phi(C), \phi(C, D), \phi(I), \phi(G, D, I), \phi(L, G), \phi(S, I), \phi(J, S, L), \phi(H, J, G)\}
\end{equation}
consider the problem where we want to infer $P(J)$ with elimination ordering $\prec_{\{C, D, I, H, G, S, L\}}$.\footnote{Note that what comes first in the ordering is the what we want to sum over in the inner most summation} Then,
\begin{align}
    p(J) &= \sum_{L} \sum_{S} \phi(J, L, S) \sum_{G} \phi(L, G) \sum_{H} \phi(H, G, J) \sum_{I} \phi(S, I) \phi(I) \sum_{D} \phi(G, D, I) \sum_{C} \phi(C) \phi(C, D)\\
    &= ... \underbrace{\sum_{C} \phi(C) \phi(C, D)}_{\tau(D)} \\
    &= ... \underbrace{\sum_{D} \phi(G, D, I) \tau(D)}_{\tau(G, I)} \\
    &= ... \underbrace{\sum_{I} \phi(S, I) \phi(I) \tau(G, I)}_{\tau(S, G)} \\
    &= ... \underbrace{\tau(S, G)\sum_{H} \phi(H, G, J)}_{\tau(S, G)\tau(G, J)} ~~~~ \text{// Note that $\tau(S,G)$ doesn't contain $H$}\\
    &= ... \underbrace{\sum_{G} \phi(L, G) \tau(S, G) \tau(G, J)}_{\tau(J, L, S)} \\
    &= ... \underbrace{\sum_{S} \phi(J, L, S) \tau(J, L, S)}_{\tau(J, L)} \\
    &= ... \underbrace{\sum_{L} \tau(J, L)}_{\tau(J)}\\
    &= \tau(J)
\end{align}

\subsubsection{Complexity of VE}
The complexity of VE is 
\begin{equation}
    \mathcal{O}\left(m k^{N_{\max }}\right)
\end{equation}
where
\begin{itemize}
    \item $m = |\Phi|$ is the number of initial factors, i.e. the number of terms in the original factorization of the joint probability (given the graphical model)
    \item $k$ is the number of states each random variable takes (assumed to be equal here)
    \item $N_i$ is the number of random variables inside each sum $\sum_i$ \textit{at the time of marginalization}
    \item $N_{max} \triangleq \max_i N_i$ is the number of random variables inside the largest sum. (Inner sums are counted as one term all together)
\end{itemize}


\section{Lecture 5 - Message passing, Hidden Markov Models, and Sampling}
\subsection{Message Passing}
\subsubsection{Belief Propagation: Motivation and Definitions}
Our goal is to compute the marginal of every variable in graph $p(x_i), \forall x_i \in X$. Notice that in a tree, we have\info{Below, $T$ means the set of edges in the tree}
\begin{equation}
    P\left(X_{1: n}\right)=\frac{1}{Z} \prod_{k \in\{1,\dots,n\}} \phi\left(x_{k}\right) \prod_{(i, j) \in T} \phi_{i, j}\left(x_{i}, x_{j}\right)
\end{equation}
and thus, if we want to compute $P(X_1)$ we can marginalize out all other variables
\begin{align}
    P(X_1) = \sum_{x_2,\dots,x_n} P(X_{1:n}) 
    &= \sum_{x_2,\dots,x_n} \frac{1}{Z} \prod_k \phi\left(x_{k}\right) \prod_{(i, j) \in T} \phi_{i, j}\left(x_{i}, x_{j}\right) \\
    &\propto \sum_{x_2,\dots,x_n}  \prod_k \phi\left(x_{k}\right) \prod_{(i, j) \in T} \phi_{i, j}\left(x_{i}, x_{j}\right)
\end{align}
Now that we have the goal, we can group what we want to compute into a more structured representation, where we define \textit{\textbf{message-passing}} from variable $j$ to $i \in N(j)$ as
\begin{equation}
    m_{j \rightarrow i}\left(x_{i}\right)=\sum_{x_{j}} \phi_{j}\left(x_{j}\right) \phi_{i j}\left(x_{i}, x_{j}\right) \prod_{k \in N(j) \neq i} m_{k \rightarrow j}\left(x_{j}\right)
\end{equation}

\subsubsection{Belief Propagation Algorithm}
%\unsure{What is meant here in steps 2 and 3?}
\begin{enumerate}
    \item Choose an root $r$ arbitrarily, 
    \item Pass messages from leaves to $r$
    \item Pass messages from $r$ to leaves\info{Step 2 and 3 here so that we will compute all the $m_{i \rightarrow j}$, $\forall (i, j)$, $(j, i)\in T$, after which we can use them to calculate the marginals using formula in step 4}
    \item Compute
        \begin{equation}
            p\left(x_{i}\right) \propto \phi_{i}\left(x_{i}\right) \prod_{j \in \mathcal{N}(i)} m_{j \rightarrow i}\left(x_{i}\right), \forall_{i}
        \end{equation}
        \info{Notice that the idea is that we want to calculate \textit{\textbf{all}} of the marginals}
\end{enumerate}


\subsection{Inference in Hidden Markov Models}
There are the following four kinds of inference that we can do on a HMM, namely
\begin{itemize}
    \item \textit{\textbf{Filtering:}} Compute the belief state $p\left(z_{t} | \mathbf{x}_{1: t}\right)$ online. Notice that we call this inference operation filtering because this produces a result smoother than simply computing $p(z_t|\bx_{t})$.
    \item \textit{\textbf{Smoothing:}} Compute $p\left(z_{t} | \mathbf{x}_{1: T}\right)$ offline. Notice that this computation happens after all $\bx_{1:T}$ observations have been collected and hence is offline.\footnote{A very good intuitive example from KPM is that we have a detective investigating a crime scene, and as we gather more observation, the prediction that we make tends to be more and more certain and less `stochastic'.}\note{Definition in KPM is different from course note}
    \item \textit{\textbf{Fixed lag smoothing:}} Compute $p\left(z_{t-\ell} | \mathbf{x}_{1: t}\right)$, where $\ell>0$ is called the lag. Essentially, we want to do inference on the hidden state $z_\alpha$ and we wait until we observe all of $\bx_{1:\alpha + \ell}$ before we carry out the computation. 
    \item \textit{\textbf{Prediction:}} In this case, we want to compute $p\left(z_{t+h} | \mathbf{x}_{1: t}\right)$ for some $h > 0$. ($h$ is called the prediction horizon) Let's first take a look at the example where $h = 2$, then
    \begin{align}
        p(z_{t+2}|\bx_{1:t}) 
        &= \sum_{z_{t+1}}\sum_{z_t}p(z_t, z_{t+1}, z_{t+2}|\bx_{1:t}) \\
        &= \sum_{z_{t+1}}\sum_{z_t}p(z_t |\bx_{1:t})p(z_{t+1}|z_t, \bx_{1:t})p(z_{t+2}|z_{t+1}, z_{t}, \bx_{1:t}) \\
        &= \sum_{z_{t+1}} \sum_{z_{t}} p\left(z_{t+2} | z_{t+1}\right) p\left(z_{t+1} | z_{t}\right) p\left(z_{t} | \mathbf{x}_{1: t}\right)
    \end{align}
    where the last step of simplification could be easily justified using Bayes Ball rules. Above, we computed the prediction about the future hidden states, and it can be converted into prediction about the future observations using
    \begin{equation}
        p\left(\mathbf{x}_{t+h} | \mathbf{x}_{1: t}\right)=\sum_{z_{t+h}} p\left(\mathbf{x}_{t+h} | z_{t+h}\right) p\left(z_{t+h} | \mathbf{x}_{1: t}\right)
    \end{equation}
    which is called \textit{\textbf{Posterior Predictive Density}}.
\end{itemize}

\subsection{The Forward - Backward Algorithm}
Suppose that we want to find $p(z_t | x_{1:T}), \forall t$, then it can be broken down into two pieces
\begin{itemize}
    \item \textit{\textbf{Forward Filtering:}} Compute $p\left(z_{t}, x_{1: t}\right) \quad \forall t$, which effectively is the probability of the hidden state $z_t$ given all past observations, up to including $x_t$. 
    \item \textbf{\textit{Backward Filtering:}} Compute $p\left(x_{1+t: T} | z_{t}\right)\quad \forall t$, which computes the probability of \textit{all} future observations from $x_{t+1}$ up to including $x_T$ given the current hidden state, i.e. $z_t$.
\end{itemize}
Then, the complete smoothing $p(z_t | x_{1:T})$\note{This is called \textbf{Latent sequence given observations in HMM} in the course notes} could be computed as
\begin{align}
    p\left(z_{t} | x_{1: T}\right)
    &= p\left(x_{1:T}\right)p\left(z_{t}, x_{1: T}\right) \\
    &\propto p\left(z_{t}, x_{1: T}\right) \\
    &=p\left(z_{t}, x_{1: t}\right) p\left(x_{t+1: T} | z_{t}, x_{1: t}\right) \\
    &=p\left(z_{t}, x_{1: t}\right) p\left(x_{t+1: T} | z_{t}\right) && \text{// By Bayes Ball} \\
    &=(\text{Forward Recursion})(\text{Backward Recursion})
\end{align}

\subsubsection{Forward Filtering Recursion}
%There are two steps that we will do in this recursion, called predict-update cycle\info{From KPM}
%\begin{itemize}
%    \item \textit{\textbf{One step ahead predictive density}} can be computed as
%    \begin{align}
%        p\left(z_{t}=j | \mathbf{x}_{1: t-1}\right)
%        &= \sum_i p(z_t = j, z_{t - 1} = i | \bx_{1:t - 1}) \\
%        &= \sum_i p(z_t = j| z_{t - 1}= i, \bx_{1:t - 1})p(z_{t - 1} = i| \bx_{1:t - 1}) \\
%        &= \sum_{i} p\left(z_{t}=j | z_{t-1}=i\right) p\left(z_{t-1}=i | \mathbf{x}_{1: t-1}\right)
%    \end{align}
%    \item \textit{\textbf{Update Step}} is where we absorb the observed data form time $t$ using the Bayes Rule
%    \begin{align}
%        \alpha_{t}(j) 
%        &\triangleq p\left(z_{t}=j | \mathbf{x}_{1: t}\right)=p\left(z_{t}=j | \mathbf{x}_{t}, \mathbf{x}_{1: t-1}\right) \\
%        &= \frac{1}{Z_{t}} p\left(\mathbf{x}_{t} | z_{t}=j, \cancel{\mathbf{x}_{1: t-1}}\right) p\left(z_{t}=j | \mathbf{x}_{1: t-1}\right)
%    \end{align}
%    where
%    \begin{equation}
%        Z_{t} \triangleq p\left(\mathbf{x}_{t} | \mathbf{x}_{1: t-1}\right)=\sum_{j} p\left(z_{t}=j | \mathbf{x}_{1: t-1}\right) p\left(\mathbf{x}_{t} | z_{t}=j\right)
%    \end{equation}
%\end{itemize}
%Below is the version from course notes.\unsure{What is the connection?}
\begin{align}
    \alpha_t(z_t) = p\left(z_{t}, x_{1: t}\right)
    &=\sum_{z_{t-1}} p\left(z_{t-1}, z_{t}, x_{1: t}\right) \\
    &= \sum_{z_{t-1}} p\left(x_{t} | z_{t-1}, z_{t}, x_{1: t-1}\right) p\left(z_{t} | z_{t-1}, x_{1: t-1}\right) \underbrace{p\left(z_{t-1}, x_{1: t-1}\right)}_{ = \alpha_{t - 1}(z_{t - 1})}\\
    \implies &\alpha_{t}\left(z_{t}\right)=p\left(x_{t} | z_{t}\right) \sum_{z_{t-1}} p\left(z_{t} | z_{t-1}\right) \alpha_{t-1}\left(z_{t-1}\right)
\end{align}
Notice that our forward recursion contains our emission, $p(x_t | z_t)$ and transition $p(z_t | z_{t - 1})$. The base case of the recursion could be unwinded into
\begin{equation}
    \alpha_{1}\left(z_{1}\right)=p\left(z_{1}, x_{1}\right)=p\left(z_{1}\right) p\left(x_{1} | z_{1}\right)
\end{equation}


\subsubsection{Backward Filtering Recursion}
\begin{align}
    p\left(x_{t+1: T} | z_{t}\right)
    &=\sum_{z_{t+1}} p\left(z_{t+1}, x_{t+1: T} | z_{t}\right) \\
    &= \sum_{z_{t+1}} p\left(x_{t+2: T} | z_{t+1}, z_{t}, x_{t+1}\right) p\left(x_{t+1} | z_{t+1}, z_{t}\right) p\left(z_{t+1} | z_{t}\right) \\
    \implies &\beta_{t}\left(z_{t}\right)=\sum_{z_{t+1}} \underbrace{ p\left(x_{t+2 : T} | z_{t+1}\right)}_{ = \beta_{t +1}(z_{t + 1})} p\left(x_{t+1} | z_{t+1}\right) p\left(z_{t+1} | z_{t}\right)
\end{align}
and hence if we recurse the above relationship, we will unwind to the base case
\begin{equation}
    \beta_{1}\left(z_{1}\right)=p\left(x_{3: T} | z_{2}\right) p\left(x_{2} | z_{2}\right) p\left(z_{2} | z_{1}\right)
\end{equation}


\subsection{Sampling}
\note{The word sample here refers to a single realization from a distribution rather than a set. }
The goal is to, 
\begin{itemize}
    \item Generate sample $\left\{x^{(r)}\right\}_{r=1}^{R}$ from a probability distribution $p(x)$, and/or
    \item Estimate expectations of functions $f(x)$ under some distribution $p(x)$, usually we want to estimate moments for a distribution
    \begin{equation}
        E=\underset{x \sim p(x)}{\mathbb{E}}[f(x)]=\int f(x) p(x) d x
    \end{equation}
\end{itemize}

\subsubsection{Ancestral Sampling}
\paragraph{Generating marginal samples} If you are only interested in sampling a particular set of nodes, you can simply sample from all the nodes jointly, then ignore the nodes you don't need.

\paragraph{Generating conditional samples} If you want to sample conditional on a node with no parents, that's also easy - you can simple do ancestral sampling starting from the nodes you have. \newline \newline
\textit{\textbf{(\#! Important:)}} However, to sample from a DAG conditional on leaf nodes is hard in the same way that inference is hard in general. E.g. sampling the unknown key in a crypto-system given the cypher-text but not knowing the plaintext. Finding ways to do this approximately is what a lot of the rest of the course will be about.

\subsubsection{Simple Monte Carlo}
\paragraph{(Definition) - Simple MC} Given $\left\{x^{(r)}\right\}_{r=1}^{R} \sim p(x)$, we want to estimate the expectation $\underset{x \sim p(x)}{\mathbb{E}}[f(x)]$.
\begin{equation}
    E=\underset{x \sim p(x)}{\mathbb{E}}[f(x)] \approx \frac{1}{R} \sum_{r=1}^{R} f\left(x^{(r)}\right)=\hat{E}
\end{equation}
\paragraph{Unbiasedness of MC}
\begin{align}
    \mathbb{E}_{x \sim p\left(\left\{x^{(i)}\right\}_{r=1}^{R}\right)}[\hat{E}]
    &=\mathbb{E}\left[\frac{1}{R} \sum_{r=1}^{R} f\left(x^{(r)}\right)\right] \\
    &= \frac{1}{R} \sum_{r=1}^{R} \mathbb{E}\left[f\left(x^{(r)}\right)\right]\\
    &= \frac{1}{R} \sum_{r=1}^{R} \underset{x \sim p(x)}{\mathbb{E}}[f(x)] \\
    &= \frac{R}{R} \underset{x \sim p(x)}{\mathbb{E}}[f(x)] \\
    &= E
\end{align}
\qed

\paragraph{Variance of MC}
\begin{align}
    \operatorname{var}[\hat{E}]
    &=\operatorname{var}\left[\frac{1}{R} \sum_{r=1}^{R} f\left(x^{(r)}\right)\right] \\
    &= \frac{1}{R^{2}} \operatorname{var}\left[\sum_{r=1}^{R} f\left(x^{(r)}\right)\right] \\
    &= \frac{1}{R^{2}} \sum_{r=1}^{R} \operatorname{var}\left[f\left(x^{(r)}\right)\right]\\
    &= \frac{1}{R}\operatorname{var}\left[f(x) \right]
\end{align}
We notice that the accuracy of MC estimates only depends on the variance of $f$, not on the dimension of $x$. Also, we the number of samples, $R$, increases, the variance $\hat{E}$ will decrease at a rate proportional to $\frac{1}{R}$.
\qed


\section{Lecture 7 - Stochastic Variational Inference (SVI/ADVI)}
\subsection{Motivation - Approximating Posterior Inference}
Notice that if we want to calculate the \textit{exact} posterior distribution, for example, 
\begin{equation}
    p(z | x)=\frac{p(x | z)}{p(x)}=\frac{p(x, z) p(z)}{\int p(x, z) dz}
\end{equation} 
the bottom integral over all possible states soon becomes intractable, making the computation of the entire posterior distribution intractable. Hence, instead, we will approximate the posterior inference with \textit{\textbf{variational methods}}. Generally speaking, it works as follows
\begin{enumerate}
    \item We introduce a family of distributions, with variational parameters $\phi$, denoted as $q_\phi$. 
    \item We want to encode a ``distance metric'' between $p(z|x)$ and $q_\phi(z)$.
    \item Try to minimize the proposed ``distance metric''.
\end{enumerate}
There are a few things to note
\begin{itemize}
    \item First, as we will soon see, we use a measure of closeness that is \textit{\textbf{not}} a distance metric. Also, using na\"ive distance metric could fail spectacularly (arbitrarily bad examples can be found if we use MSE).
    \item This formulation turns the Bayesian Inference into an optimization problem. If enough parts of the model is differentiable and could be well-approximated with Monte  Carlo, then we can use gradient based optimization methods to solve this problem scalably. 
\end{itemize}


\subsection{The Kullback-Leibler Divergence}
Suppose $q_\phi$ denotes a family of distributions with its own \textbf{variational parameters} $\phi$, and $p$ is some distribution. Then, the Kullback-Leibler divergence is defined as
\begin{align}
    D_{\mathrm{K L}}\left(q_{\phi}(z | x) \| p(z | x)\right)
    &=\int q_{\phi}(z | x) \log \frac{q_{\phi}(z | x)}{p(z | x)} d z \\
    &=\underset{z \sim q_{\phi}}{\mathbb{E}} \left[ \log \frac{q_{\phi}(z | x)}{p(z | x)} \right]
\end{align}

\paragraph{Properties} of KL Divergence
\begin{enumerate}
    \item $D_\kl (q_\phi || p) \geq 0$, non-negativeness
    \item $D_\kl (q_\phi || p) = 0 \iff q_\phi = p$
    \item $D_\kl (q_\phi || p) \neq D_\kl (p || q_\phi)$, i.e. KL Divergence is \textit{\textbf{not}} a metric distance. 
\end{enumerate}

\subsubsection{Expectation Propagation}
If we try to reverse the arguments, it leads to a different kind of variational inference that is called ``expectation propagation''. Typically, this leads to an algorithm that is more computationally expensive. 

\subsection{The Evidence Lower BOund (ELBO)}
\subsubsection{Definition and Equivalence Property}
We cannot minimize the KL divergence exactly\info{$D_\kl (q_\phi || p)$ is intractable because it contains $p(z|x)$, which is intractable.}, thus we introduce a function, ELBO, and try to minimize that. Notice that ELBO is equal to the actual KL divergence up to a constant. 
\begin{align}
    D_{\kl}\left(q_{\phi}(z | x) \| p(z | x)\right)
    &=\underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{q_{\phi}(z | x)}{p(z | x)} \\
    &= \underset{z \sim q_{\phi}}{\mathbb{E}}\left[\log \left(q_{\phi}(z | x) \cdot \frac{p(x)}{p(z, x)}\right)\right] \\
    &= \underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{q_{\phi}(z | x)}{p(z, x)}+\underset{z \sim q_{\phi}}{\mathbb{E}} \log p(x) \\
    &= -\mathcal{L}(\phi; x) + \log p(x)
\end{align}
where $\mathcal{L}(\phi; x) = -\underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{q_{\phi}(z | x)}{p(z, x)}$ is defined as the ELBO. If we re-arrange what we have above,
\begin{equation}
    \log p(x) = \mathcal{L}(\phi ; x)+D_{\kl}\left(q_{\phi}(z | x) \| p(z | x)\right)
\end{equation}
and we notice that $D_{\kl}\left(q_{\phi}(z | x) \| p(z | x)\right) \geq 0$, it then follows that 
\begin{equation}
    \mathcal{L}(\phi ; x) \leq \log p(x)
\end{equation}
Hence, to minimize the KL divergence we need to maximize the ELBO, and they are equivalent.

\subsubsection{Optimization goal of ELBO}
We have, by definition of ELBO, that
\begin{align}
    \mathcal{L}(\phi; x)
    &= -\underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{q_{\phi}(z | x)}{p(x, z)} \\
    &= \underset{z \sim q_{\phi}}{\mathbb{E}}\left[\log p(x, z)-\log q_{\phi}(z | x)\right]
\end{align}
Recall, that our goal is to use gradient based methods to optimize this objective, meaning that we have to somehow compute, estimate to be precise in this case, the gradient of ELBO $\mathcal{L}(\phi; x)$
\begin{equation}
    \nabla_{\phi} \mathcal{L}(\phi)=\nabla_{\phi} \mathbb{E}_{z \sim q_{\phi}(z | x)}\left[\log p(x, z)-\log q_{\phi}(z | x)\right]
\end{equation}
and we know from a previous lecture that one can get an unbiased estimator of any expectation as long as we can sample from the distribution and evaluate the function, using simple Monte Carlo.

\subsubsection{Pathwise Gradient}
Above, we saw that the objective is the gradient over an expectation and we now would like to make it an expectation over gradient. In general, such swapping is not allowed and we can only do so if the distribution we're taking the expectation over \textit{\textbf{does not}} depend on the parameter, i.e. $\phi$. Our goal is now to factor our the randomness of from $q$, and put it into a parameterless, fixed source of noice $p(\varepsilon)$. Formally, we need to find function $T(\phi,\varepsilon )$ such that 
\begin{equation}
    \begin{rcases}
        \varepsilon \sim p(\varepsilon ) \\
        z = T(\phi,\varepsilon ) 
    \end{rcases}
    \implies z\sim q_\phi(z)
\end{equation}
We usually start with $p(\varepsilon )$ being uniform or normal. Indeed it is not always easy to find these functions\footnote{\url{http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/}} and as a easy concrete example, we know
\begin{equation}
    \begin{rcases}
        \varepsilon \sim \mathcal{N}(\varepsilon | 0, 1) \\
        z = \sigma \varepsilon + \mu
    \end{rcases}
    \implies z\sim \mathcal{N}(z | \mu, \sigma )
\end{equation}
Then, by applying the trick we know,
\begin{align}
    \nabla_{\phi} \mathcal{L}(\phi)
    &=\nabla_{\phi} \mathbb{E}_{z \sim q_{\phi}(z | x)}\left[\log p(x, z)-\log q_{\phi}(z | x)\right] \\
    &=\nabla_{\phi} \mathbb{E}_{\varepsilon \sim p(\varepsilon)}\left[\log p(x, T(\phi, \varepsilon))-\log q_{\phi}(T(\phi, \varepsilon) | x)\right] \\
    &=\mathbb{E}_{\varepsilon \sim p(\varepsilon)} \nabla_{\phi}\left[\log p(x, T(\phi, \varepsilon))-\log q_{\phi}(T(\phi, \varepsilon) | x)\right]
\end{align}

\subsection{Tutorial}
\subsubsection{Derivation Using Jensen's Inequality}
Recall that by when Jensen's inequality is applied to probability distributions, we know that when $f$ is concave,
\begin{equation}
    f(\mathbb{E}[X]) \geq \mathbb{E}[f(X)]
\end{equation}
Then, for the log probability of the observations,
\begin{align}
    \log p(x) 
    &= \log \int p(x,z) dz \\
    &= \log \int p(x,z) \frac{q_\phi(z|x)}{q_\phi(z|x)} dz \\
    &= \log \mathbb{E}_{z\sim q_\phi} \frac{p(x,z)}{q_\phi(z|x)}
\end{align}
where we apply Jensen's Inequality
\begin{align}
    \implies \underset{z \sim q_{\phi}}{\mathbb{E}} \frac{p(x, z)}{q_{\phi}(z | x)} 
    &\geq \underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{p(x, z)}{q_{\phi}(z | x)} \\
    &= -\underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{q_{\phi}(z | x)}{p(x, z)} \\
    &=\mathcal{L}(\phi ; x)
\end{align}

\subsubsection{Interpretations of ELBO}
For the ELBO defined as
\begin{equation}
    \mathrm{ELBO} \triangleq \mathcal{L}(\phi ; x) =-\underset{z \sim q_{\theta}}{\mathbb{E}} \log \frac{q_{\phi}(z | x)}{p(x, z)}
\end{equation}
we have
\begin{enumerate}
    \item The most general interpretation is just breaking up the logarithm. We have
    \begin{align}
        \mathcal{L}(\phi ; x)
        &=-\underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{q_{\phi}(z | x)}{p(x, z)} \\
        &= \underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{p(x, z)}{q_{\phi}(z | x)} \\
        &= \underset{z \sim q_{\phi}}{\mathbb{E}} \log \frac{p(z) p(x | z)}{q_{\phi}(z | x)} \\
        &= \underset{z \sim q_{\phi}}{\mathbb{E}}\left[\log p(x | z)+\log p(z)-\log q_{\phi}(z | x)\right]
    \end{align}
    \item We can try to write this using entropy as 
    \begin{equation}
        \mathcal{L}(\phi ; x) = \underset{z \sim q_{\phi}}{\mathbb{E}}[\log p(x | z)+\log p(z)] \mathcal{H}\left(q_{\phi}(z | x)\right)
    \end{equation}
    \item We can frame ELBO as a trade off
    \begin{equation}
        \mathcal{L}(\phi ; x) = \underbrace{\underset{z \sim q_{\phi}}{\mathbb{E}}[\log p(x | z)]}_{(\dag)} - \underbrace{D_{\kl }\left(q_{\phi}(z | x) \| p(z)\right)}_{(\ddag)}
    \end{equation}
    \begin{itemize}
        \item $(\dag)$ is ``reconstruction likelihood'', i.e. how probable is $x$ given $z$, which encourages the model to choose the distribution which best reconstructs the data. 
        \item $(\ddag)$ acts as regularization, enforcing the idea that our parametrization shouldn't move us too far from the true distribution. 
    \end{itemize}
\end{enumerate}


\section{Lecture 8 - Sampling and Monte Carlo Methods}
\subsection{Motivation: Sampling}
Note: In this section, we refer to a ``sample'' as a single realization $x$ that is from a probability distribution $p(x)$.\footnote{This is different from the normal sense of sample, which means one batch of sample from a distribution.}

\paragraph{Problems to be solved} Monte Carlo methods are computational techniques that make use of random numbers, it aims to solve one or both of the following problems
\begin{enumerate}
    \item To generate samples
        \begin{equation}
            \{x^{(r)}\}_{r=1}^{R} \sim p(x)
        \end{equation}
    \item and to estimate expectations of functions $(\phi(x) )$ under the distribution of $p(x)$, i.e.,
        \begin{equation}
            \Phi=\underset{x \sim p(x)}{\mathbb{E}}[\phi(x)]=\int \phi(x) p(x) d x
        \end{equation}
\end{enumerate}

\paragraph{Sampling $p(x)$ is like hornets' nest}
To sample from
\begin{equation}
    p(x)=\frac{\tilde{p}(x)}{Z}
\end{equation}
there are two main complications even if we know how to evaluate $\tilde p (x)$
\begin{enumerate}
    \item We don't typically know the normalizing constant, $Z$.
    \item Even if we did know $Z$, the problem of drawing samples from $p(x)$ is still a chanlenging one, especially in high-dimensional spaces, because there is no obvious way to sample from $p$\todo{Should this be $\tilde p$ ??} without enumerating most or all of the  possible states. \note{Note: There are only a handful of distribution, for example gaussian distribution, that we can easily sample from in a high dimensional space. }
\end{enumerate}

\subsection{Lattice Discretization}
Imagine that we wish to draw samples from the density
$p(x)=\frac{\tilde{p}(x)}{Z}$, then what we can do is discretize the variable $x$ and sample from the discrete probability distribution over a finite set of uniformly spaced points $S = \{x_i\}_i$. Then for $x_i \in S$, we evaluate $\tilde p (x_i)$ after which we can compute the normalizer
\begin{equation}
    Z = \sum_i \tilde p_i (x_i)
\end{equation}
and
\begin{equation}
    p_{i}=\frac{\tilde{p}_{i}}{Z}
\end{equation}

The main problem with this method, however, comes from the accuracy and cost of evaluating at lattice. If we want to sample $D$ uniformly spaced points in one dimension and the system has $N$ dimensions in total. Then, the the complexity is asymptotically $\mathcal{O}(D^N)$ which grows exponentially as dimension grows and soon become intractable. 

\subsection{Sampling Procedure}
Note: Importance Sampling doesn't help us solve the problem of generating samples from $p(x)$, rather it is a method that helps us estimate expectation of a function $\phi (x)$, i.e. problem 2. 
\paragraph{Assumptions}
\begin{enumerate}
    \item To do importance sampling, we still need to assume that we can evaluate $p(x)$ (density that we wish to draw samples from) within a multiplicative constant. To be precise, we can evaluate a function $\tilde p(x)$ such that 
        \begin{equation}
            p(x) = \tilde p(x) / Z
        \end{equation}
    \item We further assume that we have a simpler density, $q(x)$ from which it is easy to \textit{\textbf{sample}} from, i.e., $x \sim q(x)$ and easy to \textit{\textbf{evaluate}}, i.e., $tilde q(x)$
        \begin{equation}
            q(x)=\frac{\tilde{q}(x)}{Z_{q}}
        \end{equation}
    where the density $q(x)$ is referred to as the sampler density. 
\end{enumerate}
\paragraph{Importance Sampling}
In importance sampling, we generate in total $R$ samples from $q(x)$, i.e. $\left\{x^{(r)}\right\}_{r=1}^{R} \sim q(x)$. If these points were samples from $p(x)$ then we could estimate $\Phi$ by
\begin{equation}
    \Phi=\underset{x \sim p(x)}{\mathbb{E}}[\phi(x)] \approx \frac{1}{R} \sum_{r=1}^{R} \phi\left(x^{(r)}\right)=\hat{\Phi}
\end{equation}
i.e., we could use a \textit{{Simple Monte Carlo Estimator}}. The problem is that these sample are from $q$ rather than $p$ which can be completely unrelated densities. To solve this over-represented/under-represented problem, we can take into account that we are sampling from a different distribution by introducing weights, (essentially corrections)
\begin{align}
    \tilde{w}_{r}=\frac{\tilde{p}\left(x^{(r)}\right)}{\tilde{q}\left(x^{(r)}\right)}
\end{align}
Finally, we re-write our estimator under $q$, i.e.
\begin{align}
    \Phi 
    &= \int \phi(x) p(x) dx \\
    &= \int \phi(x) \frac{p(x)}{q(x)} q(x) dx \\
    &\approx \frac{1}{R} \sum_{r = 1}^R \phi (x^{(r)}) \frac{p\left(x^{(r)}\right)}{q\left(x^{(r)}\right)} \\
    &= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \frac{\tilde p(x^{(r)})}{\tilde q(x^{(r)})}\\ 
    &= \frac{Z_q}{Z_p} \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r  && \text{since } w_{r}=\frac{\tilde{w}_{r}}{\sum_{r=1}^{R} \tilde{w}_{r}}\\
    &= \frac{\frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot \tilde w_r}{\frac{1}{R}\sum_{r=1}^R \tilde w_r} && \text{since } \frac{Z_{p}}{Z_{q}}=\frac{1}{R} \sum_{r=1}^{R} \tilde{w}_{r}\\ 
    &= \frac{1}{R}\sum_{r=1}^R \phi(x^{(r)}) \cdot w_r \\ &= \hat \Phi_{iw}
\end{align}
The estimator $\hat \Phi_{iw}$ is biased, but is consistent.

\paragraph{Pitfalls} As it is easy to see, if we choose $q$ to be some short tailed distribution, we might not get a good coverage for samples on $p$, and thus making the estimation not accurate. Notice that we want to support the entire real line with this method\footnote{Or otherwise this will just be uniform sampling. Indeed, importance sampling is a generalization of uniform sampling. } and hence we cannot use a na\"ive uniform over a certain range, and using uniform over the entire real line will cause uniform distribution pdf to be infinitely small everywhere. 

\paragraph{Remedy} As a way to (semi-) solve the above pitfall, we can choose $q$ to be a heavy-tailed distribution, for example Cauchy. Of course, the best of all solution is to find a $q$ that looks like $p$, but this is in a lot of cases an un achievable goal. 

\subsection{Rejection Sampling}
In rejection sampling we assume again a one-dimensional density $p(x) = \tilde p(x) /Z$ that is too complicated a function for us to be able to sample from it directly. We assume that we have a simpler \textit{\textbf{proposal}} density $q(x)$ which we can evaluate (within a multiplicative factor $Z_q$, as before), and from which we can generate samples. We further assume that we know the value of a constant $c$ such that
\begin{align}
    c \tilde q(x) > \tilde p(x) \quad \text{for some $c$ constant}\, , \forall x
\end{align}
It is somewhat important to note that in rejection sampling we care only about the proportion, and we need a $q$, or $\tilde q$ actually, that ``covers'' $p$, or $\tilde p$ actually. This means that we don't care, and thus don't need to know the normalizing constant to perform the sampling procedure. Na\"ively, we can just scale $q$ to a huge $\tilde q$, and as long as it covers $\tilde p$, we are in a good shape. 

\paragraph{Sampling Procedure} The procedure works as follows
\begin{enumerate}
    \item Generate two random numbers, the first $x$ is such that $x \sim q(x)$, and the second $u$ is generated such that $u \sim Unif[0, c\tilde q(x)]$
    \item Evaluate $\tilde p(x)$ and accept or reject the sample $x$ by comparing the value of $u$ with the value of $\tilde p(x)$, to be more specific
    \begin{itemize}
        \item If $u > \tilde p(x)$, then we reject $x$, 
        \item otherwise we accept $x$, effectively adding $x$ to our set of samples. (value of $u$ is discarded, and in the next iteration $u$ will be sampled again from, possibly, a different uniform distribution. )
    \end{itemize}
\end{enumerate}

\paragraph{Pitfalls}
\begin{itemize}
    \item Rejection sampling works the best if we are able to find $q$ that is a good approximation to $p$. If $q$ is very different from $p$ then, for $cq$ to exceed $p$ everywhere, $c$ will almost always necessarily be large. This will cause us a very high rejection ratio, and making the algorithm super inefficient. 
    \item In high-dimensional space, the problem is even worse and it is very likely that the requirement that $c \tilde q > \tilde p$ will force $c$ to be so huge that acceptances will be very rare indeed. 
    \item Besides, finding such a value of $c$ may be difficult too, since in many problems we know neither where the modes of $\tilde p$ are located nor how high they are. 
    \item In general, $c$ grows exponentially with the dimensionality $N$, so the acceptance rate is expected to be exponentially small in $N$, since
    \begin{equation}
        \text{ acceptance rate } =\frac{\text { area under } \tilde{p}}{\text { area under } c \tilde{q}}=\frac{1}{Z}
    \end{equation}\todo{vas ist das?}
\end{itemize}


\subsection{Metropolis-Hasting Method: Example of MCMC}
The proposed importance sampling and rejection sampling work well only if the proposal density $q(x)$ is similar to $p(x)$. In general, in a high dimensional case, such $q$ is very hard to find. 
In Metropolis-Hasting, in contrast to importance and rejection sampling methods, it is not necessary for $q(x'| x^{(t)})$ to lock similar to $p(x)$ in order for the algorithm to be useful. The density $q(x'| x^{(t)})$ might be a simple distribution such as a gaussian centred at $x^{(t)}$, but can in general be any \textit{fixed} density from which we can draw samples. 
\paragraph{Sampling Procedure}
\begin{itemize}
    \item A tentative new state $x'$ is generated from the proposal density $q(x'|x^{(t)})$, then we compute
        \begin{equation}
            a=\frac{\tilde{p}\left(x^{\prime}\right) q\left(x^{(t)} | x^{\prime}\right)}{\tilde{p}\left(x^{(t)}\right) q\left(x^{\prime} | x^{(t)}\right)}
        \end{equation}
    \item If $a \geq 1$, then the new state is accepted. 
    \item Otherwise, the new state is accepted with probability $a$. Then, we perform update as follows,
    \begin{equation}
        x^{(t+1)} \gets \text{accepted}\,\,?\,\, x^{\prime} \,\,:\,\,x^{(t)} 
    \end{equation}
\end{itemize}

\paragraph{Note on Difference} Notice that in rejection sampling, rejected points are discarded and have no influence on the list of samples $x^{(r)}_r$ that we collected. Here, a rejection causes the current state to be written again onto the list. 

\paragraph{Convergence and MCMC} Metropolis-Hastings converges to $p(x)$ for any $q\left(x^{\prime} | x^{(t)}\right) \geq 0 \quad \forall x^{\prime}, x^{(t)}$ as $t \rightarrow \infty$. That is, out list of samples is such that
\begin{equation}
    \{x^{(r)}\}_{r=1}^{R} \rightarrow p(x)
\end{equation}
The Metropolis-Hastings method is an example of a Markov Chain Monte Carlo method. In contrast to rejection sampling, where the accepted points are independent samples from the desired distribution, MCMC methods involve a Markov process in which a sequence of states is generated, each sample $x^{(t)}$ having a probability distribution that depends on the previous value, $x^{(t - 1)}$. Since successive samples are dependent, the Markov chain may have to run for a considerable time in order to generate samples that are effectively independent samples from $p$. 


\section{Lecture 9 - Amortized Inference and Variational Auto Encoders}
\subsection{Motivations}
%\subsubsection{SVI on a Per-Example LVM don't Scale to Large Data}
%Suppose that we are trying to use a per-example LVM for a model containing $N$ entities, each of which we are trying to get a, hopefully good, $q_\phi$ of. Then, we can na\"ively model this using the simple variational methods from Lecture 8. However, there is a complication if we try to solve this this way. Namely, at each step of gradient descent, we will have to update all of the $N$ $\phi_i$'s, which causes some of the update estimates very out of date.\footnote{Let's say $N = 1000$, then when we perform updates through sub-sampling, the $1000$'th update estimate is 1000 steps out of date! Notice that these are just estimates, and are quite fragile, so 1000 steps changes the \textit{\textbf{true posterior}} a lot. }

\subsubsection{Efficient Use of (Accumulative) Partial Evidence}
With enough experience, doctors, plumbers, detectives, etc. can very quickly tell what is going on and what they still need more information about, if they've seen enough similar situations. Hence, perhaps we could somehow train a neural network to look at the data for a person $x_i$, and then output an approximate posterior $q_{\phi_{i}}\left(z_{i} | x_{i}\right)$

\subsection{Amortized Inference}
Here ``amortized'' essentially means to spread out over time. Previously, we do SVI from scratch every time we see a new datapoint. Now, instead, we will gradually learn a function that can look at the data for a person $x_i$, and then output an approximate posterior $q_{\phi}\left(z_{i} | x_{i}\right)$. This is known as a \textit{\textbf{recognition model}}, instead of a separate $\phi_i$ for each data example, we will just have a single global $\phi$ that specifies the parameter of the recognition model. Because the relationship between data and posteriors is complex and hard to specify by hand, we will do this through a neural net. (we make the neural net learn the function!)

\subsubsection{Simple Example (Parametrizing Gaussian)}
We have seen one way to specify a probability distribution given an input with neural networks in assignment 2: we can simply have a network take in $x_i$, and output the mean and variance vector for a Gaussian, parametrized as\note{The graphical model for such model specification has $\phi$ \textit{\textbf{outside of}} the plate, which means we only have one global $\phi$ to update and optimize.}
\begin{align}
    q_{\phi}\left(z_{i} | x_{i}\right)=\mathcal{N}\left(z_{i} | \mu_{\phi}\left(x_{i}\right), \Sigma_{\phi}\left(x_{i}\right)\right)
\end{align}

\subsubsection{Amortized Inference - Algorithmic Procedure}
\begin{enumerate}
    \item Sample a datapoint
    \item compute parameters of approximate posterior (recognition)
    \item compute gradient of Monte Carlo estimate of ELBO with respect to $\phi$, i.e. $\nabla_\phi \overset{\sim MC}{ELBO}$
    \item update according to gradient descent
\end{enumerate}
Then, when we want to make predictions about a new datapoint, we can use out fast recognition model if we want. Of course, we might also want to stop and do something slower but more accurate, for example like per-sample SVI, and/or MCMC.

\subsubsection{Optimizing Model Parameters}
Now that $p_\theta (x)$ depends on parameters $\theta$, then the ELBO is a function of both $\phi$ and $\theta$. We can optimize them together, still, using the re-parametrize trick we saw earlier.
\begin{align}
    \nabla_{\theta, \phi} \mathcal{L}(\phi)
    &=\nabla_{\theta, \phi} \mathbb{E}_{z \sim q_{\phi}(z | x)}\left[\log p_{\theta}(x, z)-\log q_{\phi}(z | x)\right] \\
    &=\mathbb{E}_{\epsilon \sim p(\epsilon)} \nabla_{\theta, \phi}\left[\log p_{\theta}(x, T(\phi, \epsilon))-\log q_{\phi}(T(\phi, \epsilon) | x)\right]
\end{align}
This allows us to jointly fit the model parameters and the recognition network, by subsampling training examples and using simple Monte Carlo with gradient descent optimizers. Such formulation is referred to as a variational auto encoder, VAE (See Figure \ref{vae} for layout of architecture).

\begin{figure}[h]
    \begin{center}
        \begin{tikzpicture}[scale=0.2]
        \tikzstyle{every node}+=[inner sep=0pt]
        \draw (16.2,-32.6) node {$x$};
        \draw (3.2,-25.7) node {$\mu_{z|x}$};
        \draw (28.6,-25.7) node {$\Sigma_{z|x}$};
        \draw (16.2,-17.7) node {$z \rightharpoonup z|x\sim N(\mu_{z|x},\Sigma_{z|x})$};
        \draw (3.2,-9.5) node {$\mu_{x|z}$};
        \draw (28.6,-9.5) node {$\Sigma_{x|z}$};
        \draw (16.2,-1) node {$\hat{x} \rightharpoonup x|z\sim N(\mu_{x|z}, \Sigma_{x|z})$};
        \draw [black] (5.75,-24.13) -- (13.65,-19.27);
        \fill [black] (13.65,-19.27) -- (12.7,-19.27) -- (13.23,-20.12);
        \draw [black] (26.08,-24.07) -- (18.72,-19.33);
        \fill [black] (18.72,-19.33) -- (19.12,-20.18) -- (19.66,-19.34);
        \draw [black] (13.55,-31.19) -- (5.85,-27.11);
        \fill [black] (5.85,-27.11) -- (6.32,-27.92) -- (6.79,-27.04);
        \draw [black] (18.82,-31.14) -- (25.98,-27.16);
        \fill [black] (25.98,-27.16) -- (25.04,-27.11) -- (25.52,-27.98);
        \draw (32.28,-29.67) node [below] {$q_\phi(z|x)\mbox{  }[\mathrm{Encoder\,\, Net}]$};
        \draw [black] (13.66,-16.1) -- (5.74,-11.1);
        \fill [black] (5.74,-11.1) -- (6.15,-11.95) -- (6.68,-11.1);
        \draw [black] (18.7,-16.05) -- (26.1,-11.15);
        \fill [black] (26.1,-11.15) -- (25.15,-11.18) -- (25.71,-12.01);
        \draw [black] (5.71,-7.86) -- (13.69,-2.64);
        \fill [black] (13.69,-2.64) -- (12.75,-2.66) -- (13.29,-3.5);
        \draw [black] (26.13,-7.8) -- (18.67,-2.7);
        \fill [black] (18.67,-2.7) -- (19.05,-3.56) -- (19.62,-2.74);
        \draw (32.26,-4.75) node [above] {$p_\theta(x|z)\mbox{  }[\mathrm{Decoder\,\, Net}]]$};
        \end{tikzpicture}
    \end{center}
    \caption{\label{vae}Variational Autoencoder Architecture}
\end{figure}

\subsection{Variational Auto Encoders (VAEs)}
\subsubsection{Autoencoders: Definition and Problems}
An autoencoder takes an input $x$, and encodes it into a code vector $z$\footnote{Notice that for autoencoders to actually ``encode'' the inputs, we necessarily have to choose the dimension of $z$ so that it forms a bottleneck. If we allow $z$ to have the same (or even larger) dimensionally with/than $x$ and $\hat x$, then likely the model will learn two identity mappings!}, after which the code vector $z$ is decoded into $\hat{x}$ that is ``similar'' to $x$, our original data. Essentially, what we want to learn is two mappings, namely the encoder $g(x) \rightarrow z$ and the decoder $f(z) \rightarrow \hat{x}$. In general, $g$ and $f$ could be arbitrarily complex, and thus we model them neural nets. The optimization goal of such problem is to minimize the reconstruction error, i.e. 
\begin{align}
    \mathcal J = \texttt{reconstruction\_error} (x, \hat{x})\label{autoencoder_cost}
\end{align}
\paragraph{Unsupervised Nature of Autoencoders} Notice that here the cost defined in equation \ref{autoencoder_cost} doesn't rely on \textit{any} explicit label input. Rather, it is a form of unsupervised learning, where the learning goal is to learn the mappings to reconstruct the inputs, i.e. $\tilde x = f\circ g(x) \approxeq x$.

\subsubsection{Problems with Deterministic Autoencoders \label{deterministic_problems}} 
\paragraph{Proximity in data space doesn't mean proximity in feature space}
The codes that we learn by the model is deterministic, i.e.
\begin{align}
    \begin{array}{l}
    g\left(x_{1}\right)=z_{1} \Rightarrow f\left(z_{1}\right)=\tilde{x}_{1} \\
    g\left(x_{2}\right)=z_{2} \Rightarrow f\left(z_{2}\right)=\tilde{x}_{2}
    \end{array}
\end{align}
but proximity in feature space is not enforced for inputs in close proximity in data space, i.e.
\begin{align}
    z_{1} \approx z_{2} \not \implies x_{1} \approx x_{2}
\end{align}
If the space has regions where no data gets encoded to, and you sample/generate a variation from there, the decoder will generate an unrealistic output, because the decoder has no idea how to deal with that region of the latent space. During training, it never saw encoded vectors coming from that region of latent space
\paragraph{Adding Noise By Hand is Hard} We said that we wish to embed $x$'s into a lower dimension space, and code it as $z$. However, in practice it is very hard to determine what the low dimension $z$ should be. Similar to this, 
\begin{enumerate}
    \item We can add noise to data before encoding, after which we try to reconstruct the original data. But how much noise?
    \item We can add noise to the latent variable, code, $z$ after encoding, after which we try to reconstruct the original data. Again, how much noise should we add?
\end{enumerate}

\subsubsection{Variational Auto Encoders}
In an hope to address the problems above, we introduce stochasticity into out model: even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling. Figure \ref{vae} illustrates the architecture of a variational autoencoder. 

\paragraph{VAE Solves Problems in Section \ref{deterministic_problems}, WHY?} The VAE generation model learns to reconstruct its inputs not only from the encoded points but also from the area around them. This allows the generation model to generate new data by sampling from an ``area/volume'' instead of only being able to generate already seen data corresponding to the particular fixed encoded points. 

\subsubsection{Utilization of Latent Variable Models} 
The question is: once we have obtained the model, what do we do with it? 

If a latent variable model has a compact prior and vector valued code $z$'s, we can
\begin{enumerate}
    \item Sample new data to check the model,
    \item Encode (sample from the approximate posterior) of two data points, and interpolate between them in the latent space, then decode along the path
    \item Learn a function to predict some property of the examples from the low-dimensional $z$ space instead of directly rom $x$. (semi-supervised learning, essentially use unsupervised learning to cleanup the data, and to learn a ``better'' representation.)
\end{enumerate}






\end{document}