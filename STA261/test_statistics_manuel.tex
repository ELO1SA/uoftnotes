\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Inferential Statistics, Test Statistics Manuel}
\author{Tingfeng Xia}
\date{2019, Winter Term}
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Test for $\mu = \mu_0$, w/$\sigma^2$ known}
Assume that $X_i\sim N(\mu, \sigma^2)$ are i.i.d, then the test 
statistic is
\begin{equation*}
    T(X) = \frac{\Bar{X} - \mu_0}{\sigma / \sqrt{n}} \sim N(0, 1)
\end{equation*}
then under $\alpha$ significance level, we have the rejection region
\begin{equation*}
    R_\alpha(T) = ( -\infty, z_{\frac{\alpha}{2}} ) \cup 
(z_{1-\frac{\alpha}{2}}, \infty )
\end{equation*}

\section{Test for $\mu = \mu_0$, w/$\sigma^2$ unknown}
Assume that $X_i\sim N(\mu, \sigma^2)$ are i.i.d, then the test 
statistic is
\begin{equation*}
    T(X) = \frac{\Bar{X} - \mu_0}{S / \sqrt{n}} \sim t_{n-1}
\end{equation*}
then under $\alpha$ significance level, we have the rejection region
\begin{equation*}
    R_\alpha(T) = ( -\infty, t_{\frac{\alpha}{2}, df=n-1} ) \cup 
(t_{1-\frac{\alpha}{2}, df=n-1}, \infty )
\end{equation*}

\section{Test for $\sigma^2 = \sigma_0^2$}
Assume that $X_i\sim N(\mu, \sigma^2)$ are i.i.d, then the test 
statistic is
\begin{equation*}
    T(X) = \frac{(n-1)S^2}{\sigma_0^2} \sim \chi^2_{df = n-1}
\end{equation*}
and the $\alpha$ significance level rejection region is 
\begin{equation*}
    R_\alpha(T) = ( -\infty, \chi^2_{\frac{\alpha}{2}, df=n-1} ) \cup 
(\chi^2_{1-\frac{\alpha}{2}, df=n-1}, \infty )
\end{equation*}

\section{Equality of Variances $\sigma_x = \sigma_y$}
If we have $X_1,\ldots,X_n\sim N(\mu_x, \sigma^2_x)$ and $Y_1,\ldots,Y_n\sim 
N(\mu_y, \sigma^2_y)$, then under our null hypothesis
\begin{equation*}
    T(X,Y) = \frac{S_x^2/\sigma_x^2}{S_y^2/\sigma_y^2} = \frac{S_x^2}{S_y^2} \sim F_{(n-1)(m-1)}
\end{equation*}
With $\alpha$ significance level, we then have the rejection region
\begin{equation*}
     R_\alpha(T) = \left(-\infty,F_{\frac{\alpha}{2}(n-1)(m-1)}\right) 
\cup \left(F_{1-\frac{\alpha}{2}(n-1)(m-1)}, \infty\right) 
\end{equation*}

\section{Equality of $\mu_x = \mu_y$, w/$\sigma_x, \sigma_y$ known}
If we have $\Bar{X} \sim N\left( \mu_x, \frac{\sigma_x^2}{n} \right)$ 
and $\Bar{Y} \sim N\left( \mu_y, \frac{\sigma_y^2}{n} \right)$, then 
\begin{equation*}
    T(X,Y) = \frac{\Bar{X} - \Bar{Y}}{\sqrt{\frac{\sigma_x^2}{n} + 
\frac{\sigma_y^2}{m}}}\sim N(0, 1)
\end{equation*}

\section{Equality of $\mu_x = \mu_y$, w/$\sigma=\sigma_x=\sigma_y$ 
known}
If this is the case, we can pull the $\sigma_x = \sigma_y = \sigma$ 
out from the above equation, we will have
\begin{equation*}
    T(X,Y) = \frac{\bar{X} - \bar{Y}}{\sigma \sqrt{(\frac{1}{n} + 
\frac{1}{m})}} \sim N(0, 1)
\end{equation*}

\section{Equality of $\mu_x = \mu_y$, w/$\sigma = \sigma_x = \sigma_y$ 
unknown}
If we have $\Bar{X} \sim N\left( \mu_x, \frac{\sigma_x^2}{n} \right)$ 
and $\Bar{Y} \sim N\left( \mu_y, \frac{\sigma_y^2}{m} \right)$, then 
\begin{equation*}
    T(X,Y) = \frac{\Bar{X} - \Bar{Y}}{S_p \sqrt{(\frac{1}{n} + 
\frac{1}{m})}} \sim t_{n+m-2}
\end{equation*}
where, $S_p$ is the polled sample standard deviation, the square root of 
of the polled sample variance, defined as
\begin{equation*}
    S_p^2 := \frac{(n-1)S_x^2 + (m-1)S_y^2}{n+m-2}
\end{equation*}

\section{Equality of $\mu_x = \mu_y$, w/$\sigma_x, \sigma_y$ unknown}
In this case, we have a messy formula for the degrees of freedom, the
test statistics that we use stays the same as above.

\section{Equality of $\mu_x = \mu_y$ for paired data}
We set the $H_0: \mu_x - \mu_y = 0$, define $D = X-Y$, then $\mu_d = \mu_x - \mu_y$. Notice that $\mu_d = 0 \iff \mu_x = \mu_y$, then our test statistic is
\begin{equation*}
    T(D) = \frac{\Bar{D}}{s_d/\sqrt{n}} \sim t_{n-1}
\end{equation*}

\section{Restricted Likelihood Ratio Test}
Define
\begin{equation*}
    \Lambda := \frac{max_{\theta \in 
\Omega_0}[L(\theta)]}{L(\Hat{\theta})}
\end{equation*}
Denoting $p = \dim\Omega =$ number of free var in the whole space, and 
$d = \dim\Omega_0 =$ number of free var under our null hypothesis, we 
have
\begin{equation*}
    T(X) = -2\ln{\Lambda} \xrightarrow{D} \chi^2_{df = p-d}
\end{equation*}

\section{Unrestricted Likelihood Ratio Test for Equality of $\mu_x = \mu_y$ for Normally Distributed Random Variables}
Consider i.i.d $X_1,\ldots,X_n \sim N(\mu_x, \sigma_x^2)$ and i.i.d $Y_1,\ldots,Y_m \sim N(\mu_y, \sigma_y^2)$. Notice that we have $p-d = 2 - 1 = 1$ in this case, and the likelihood is
\begin{equation*}
    L(\mu_x, \mu_y) = \left\{\left(2\pi \sigma_x^2\right)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma_x^2}\sum_i{\left(X_i - \mu_x\right)^2}}\right\} \left\{\left(2\pi \sigma_y^2\right)^{-\frac{m}{2}}e^{-\frac{1}{2\sigma_y^2}\sum_i{\left(Y_i - \mu_y\right)^2}}\right\} \\
\end{equation*}    
by re-writing with $H_0: \mu = \mu_x = \mu_y$, we have
\begin{equation*}
    L(\mu) = \left\{\left(2\pi \sigma_x^2\right)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma_x^2}\sum_i{\left(X_i - \mu\right)^2}}\right\} \left\{\left(2\pi \sigma_y^2\right)^{-\frac{m}{2}}e^{-\frac{1}{2\sigma_y^2}\sum_i{\left(Y_i - \mu\right)^2}}\right\}
\end{equation*}
Our test statistic is then
\begin{equation*}
    T(X,Y) = -2 \ln \Lambda = -2 \ln \frac{L(\hat{\mu})}{L(\hat{\mu}_x, \hat{\mu}_y)} \sim \chi^2_{df = 1}
\end{equation*}
where we remind ourselves that $\hat{\mu}_x = \Bar{x}$ and $\hat{\mu}_y = \Bar{y}$ here and $\hat{\mu}$ is some wright-ed average of $\Bar{x}$ and $\Bar{y}$ as below
\begin{equation*}
    \hat{\mu} = \left( \frac{\frac{1}{\sigma^2_x/n}}{ \frac{1}{\sigma^2_x/n} + \frac{1}{\sigma^2_y/m} }\right)\Bar{x} + \left( \frac{\frac{1}{\sigma^2_y/m}}{ \frac{1}{\sigma^2_x/n} + \frac{1}{\sigma^2_y/m} }\right)\Bar{y}
\end{equation*}

\section{Chi-Square Test of Goodness of Fit}
\subsection{Known categorical probabilities}
Suppose, $X_1, X_2, \ldots, X_k$ are the observed counts of category $1, 2, \ldots, k$ respectively. Then
\begin{equation*}
    (X_1, X_2, \ldots, X_k) \sim \text{Mult}(n, p_1, p_2, \ldots, p_k)~~~~\text{where}~E[X_i] = np_i (\geq 1), \forall i
\end{equation*}
and our test statistic will be, in this case
\begin{equation*}
    T(X) = X^2 = \sum_{i = 1}^k \frac{\left(X_i - np_i\right)^2}{np_i} \xrightarrow{D} \chi^2_{(df=k-1)}
\end{equation*}
\subsection{Unknown categorical probabilities}
In this case, we have $X_1,\ldots,X_k\sim \text{Mult}(n, p_1(\theta), \ldots,p_k(\theta))$ and the test statistic will then be
\begin{equation*}
    T(X) = X^2 = \sum_{i = 1}^k \frac{\left(X_i - np_i(\hat{\theta})\right)^2}{np_i(\hat{\theta})} \xrightarrow{D} \chi^2_{(df=k-1-\dim \Omega)}
\end{equation*}
where $\dim\Omega$ is the number of params that need to be estimated to calculate $p_1,\ldots,p_k$.

\section{Chi-square Test of Independence}
Suppose that we have two categorical random variables $X,Y$. Let $i = 1,\ldots,a$ and $j=1,\ldots,b$ 
represent the categories of $X$ and $Y$ respectively. Let $f_{ij}$ represent the number of samples
corresponding to the $i$-th of $X$ and $j$-th of $Y$, notice that $\sum_{ij}f_{ij} = n$. Let $F_{ij}$ represent
the random variable corresponding to the cell at position $(i,j)$. Let $\theta_{ij} = P(X=i,Y=j)$, then we have
To access the null hypothesis, $H_0: X\perp Y$, we have $P(X=i,Y=j) = P(X=i)P(Y=j)$, so $\theta_{ij} = \theta_{i.}\theta_{.j}\footnote{The ``.'' here is a wildcard.}$, and our statistics follows
\begin{equation*}
    F_{11},F_{12},\ldots,F_{ab}\sim \text{Mult}(n, \theta_{1.}\theta_{.1},\theta_{1.}\theta_{.2},\ldots,\theta_{a.}\theta_{.b})
\end{equation*}
By using the MLEs
\begin{align*}
    \hat{\theta}_{i.} &= \sum_{j=1}^{b}f_{ij}/n \\
    \hat{\theta}_{.j} &= \sum_{i=1}^{a}f_{ij}/n
\end{align*}
we have our test statistic
\begin{equation*}
    T(X,Y) = X^2 = \sum_{i=1}^a\sum_{j=1}^b \frac{\left(f_{ij} - n\hat{\theta}_{.j}\hat{\theta}_{i.}\right)^2}{n\hat{\theta}_{.j}\hat{\theta}_{i.}} \xrightarrow{D} \chi^2_{df=(a-1)\times(b-1)}
\end{equation*}
\paragraph{Note.} In performing such test, we need to calculate the expected count of each slot and there is a neat formula for this
\begin{equation*}
    E_{ij} = \frac{i\text{-th row total * $j$-th column total}}{\text{grand total of the table}}
\end{equation*}

\section{Chi-square Test of Homogeneity}
Let $n_i$ be the marginal total of $X=i$ category, then we have $\sum_i n_i = n$. Notice that this is different
from the test of independence above in the sense that we are fixing marginal totals of all categories of $X$ before hand.
We wish to test the hypothesis $H_0: \theta_{j|X=1} = \theta_{j|X=2} =\ldots= \theta_{j|X=a} = \theta_j$. Using the MLE
\begin{equation*}
    \hat{\theta}_j = \sum_{i=1}^a f_{ij}/n
\end{equation*}
we have our test statistic
\begin{equation*}
    T(X,Y) = X^2 = \sum_{i=1}^a\sum_{j=1}^b \frac{\left(f_{ij} - n_i\hat{\theta}_{j}\right)^2}{n_i\hat{\theta}_{j}} \xrightarrow{D} \chi^2_{df=(a-1)\times(b-1)}
\end{equation*}
Again, we calculate the $E_{ij}$ using the formula above. (and this is a coincidence.)


\section{Discrepancy Statistic for Normal R.V.s}
Consider $X_1,\ldots,X_n\sim N(\hat{\mu}, \sigma_0^2)$ where $\sigma_0^2$ is known. Define $R = X_i - \Bar{X}$, where $R\sim N(0, \sigma_0^2 (1 - \frac{1}{n}))$ then, the descrepancy statistic is defined as
\begin{equation*}
    D(R) = \frac{1}{\sigma_0^2}\sum_{i = 1}^n R_i^2 =  \frac{1}{\sigma_0^2}\sum_{i=1}^n(X_i - \Bar{X})^2 \sim \chi^2_{n-1}
\end{equation*}

\section{Simple Linear Regression}
Consider the set $\{ (x_1,y_1),\ldots,(x_n,y_n) \}\subseteq \real^2$ and we want to find the line that
fits the best for our set of data. Let the hypothetical line be $y = b_1 + b_2 x$, then we define
\begin{equation*}
    res_i = (y_i - b_1-b_2x_i)\,\,\text{is the deviation of $y_i$ from the line}
\end{equation*}
and to minimize $\sum_{i=1}^nres_i^2$ we need
\begin{equation*}
    b_1 = \overline{y} - b_2\overline{x}
\end{equation*}
\begin{equation*}
    b_2 = \frac{\sum_{i=1}^n(x_i - \overline{x}(y_i-\overbar{y}))}{\sum_{i=1}^n\left(x_i-\overbar{x}\right)^2}
\end{equation*}


\end{document}

