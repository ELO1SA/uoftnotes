\documentclass[10pt]{article}
\usepackage[margin=3.5cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage{fancyhdr} 
\usepackage{youngtab}
\usepackage{logicproof}
\usepackage[normalem]{ulem}
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\qed}{\hfill $\mathcal{Q}.\mathcal{E}.\mathcal{D}.\dagger$}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\text{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% Call settag{\ldots} first to initialize, and then \para{} for a new paragraph
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vzero}{\mathbf{0}}
% For convenience, I am setting both of these to refer to the same thing.
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\boldf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bm}{\mathbf{m}}
\renewcommand{\iff}{\leftrightarrow}
\renewcommand{\implies}{\rightarrow}
\renewcommand{\neg}{\sim}

\title{PHL245 Modern Symbolic Logic}
\author{\ccLogo \,\,Tingfeng Xia}
\date{Fall 2019, modified on \today}

\begin{document}
\maketitle
\doclicenseThis
\tableofcontents
\newpage

\section{Arguments}
\subsection{Validity}
We say a deductive argument is valid iff it is not invalid. This means that we can
find out if a argument is valid or not by assessing the possiblity of the case where 
the premise is true and conclusion is false.
\subsection{Soundness}
We say a deductive argument is sound iff it is 
\begin{itemize}
    \item It is valid
    \item All the premises are TRUE
\end{itemize}

\section{Semantics in Sentential Logic}
\subsection{Syntax}
\paragraph{Sentential Logic(SL)} Complex (compound) statements are all built up by joining statements together using LOGICAL CONNECTIONS.
we have $\wedge, \vee, \rightarrow, \leftarrow, \sim$. Where $\sim$ is the only unitary connector and others are binary.

\paragraph{Atomic vs Molecular} A \textbf{statement} is Atomic if it has no logical connector and is molecular otherwise. We use \textbf{P-Z letters} to represent \textbf{atomic statments}
Here is an example: You can have fries or salad. $\equiv P \vee Q.$ Then, $P$ is ``You can have fires'' and $Q$ is ``You can have salad''. Notice that we are folowing the definition that $P$ and $Q$ are \textbf{statments}.

\paragraph{Informal Notation Hierachy}\footnote{We use this Informal notation because the formal one is cumbersome. But in order to use the informal one, we have some conventions to follow.} 
We shall see this through some examples:
\begin{itemize}
    \item In official notation, we need $(P\vee Q)$ while it is safe to drop the parenthesis.
\end{itemize}
We can check if a statement is official or informal if it has the same number of brackets as binary connectors in the statement. If you see a bracket around a unitary connector, the sentence is not well-formed.
\textbf{Right Most Rule} We say the rightmost connector in a sentence with connectors of the same level the main connector.



\section{Truth Tables}
\subsection{Summary} First we remark that we are here talking about binary operators, 
so they will have and only have two operands. We have the following to consider:
\begin{itemize}
    \item \textbf{Logical OR ($\vee$)} is true when either one of the operands is true. 
        It evaluates to false otherwise.
    \item \textbf{Logical AND ($\land$)} is true when both of the operands are true and false otherwise.
    \item \textbf{Implication ($\implies$)} is true in two cases. The first case is when the
        first operand is false and second case is when both operands are true.
    \item \textbf{Double implication, Iff ($\iff$)} is true when either both operands are true 
        or when both operands are false.
    \item \textbf{Negation ($\neg$\footnote{Or in this course, we may see $\sim$})} is true when the operand
        is false and false otherwise.
\end{itemize}

\subsection{Semantic Properties in Truth Tables}
\begin{itemize}
    \item Tautology if always true
    \item Contradiction if always false 
    \item Contingent if mixed
    \item Consistent if there exists a row where the conclusion is all true
    \item Inconsistent if all rows are not all true, negation of the consistent
    \item Logically equivalent if two sentences have the same truth table
\end{itemize}



\subsection{Full truth tables}
Example: \footnote{This example was adapted from Scharer 4.4 EG3.}
evaluate $(P\land \neg Q)\vee R. ~~\neg R\vee Q. ~~\therefore \neg P\implies Q$. 
Notice that this is equivalent to evaluating
\begin{equation*}
    \left(\left(\left(P\land \neg Q\right)\vee R \right) \land 
    \left(\neg R\vee Q\right)\right) \implies \neg P\implies Q
\end{equation*}
% and by staring at it we see this statement is valid and only valid
%  when $(P,Q,R)\in \{(T,T,T),(T,F,F),(F,T,T)\}$, which has a non-null 
%  set of solution. Hence the statement is consistent.


\section{Addressing Ambiguity in Symbolization}
\subsection{Restrictive and Non-Restrictive Clauses}
We shall see this through an example
\begin{itemize}
    \item \textbf{Cats that scratch terrifies me} `Cats \textit{that} scratch' 
    is a restricted subject compared to just `cats'.
    \item \textbf{Cats, which scratch, terrifies me} `Cats, \textit{which} scratch'
    is a non-restricted claim about cats, so essentially we are saying two sentences:
    the first one is that `Cats terrify me' and `Cats scratch'. (The which part 
    is an inserted clause describing the subject before it.)
\end{itemize}
\subsubsection{Symbolization of Non-Restrictive Clauses} In symbolization of
non-restrictive sentences, we simply treat the clause and the main sentence 
as two separate sentences and use logical and to connect them. (Two concurrent
event)
\subsubsection{Symbolization of Restrictive Clauses} We have to introduce a 
premise (conditional statement) to address the descriptive part.

\subsubsection{Example}
Consider the sentence ``Having gasoline, which is smelly, is a necessary 
condition for my car to run, and in that case I'm going to Vegas.'' and we
define the following: W: Gasoline is smelly. X: I have gasoline. Y: My car runs.
Z: I'm going to Vegas. \textbf{Solution:} We first extract the which clause out
from the sentence since as we mentioned before this is just another concurrent 
sentence (use logical and to connect). The necessary condition part is easy ($Y\rightarrow X$).
The last `in that case' part is a little bit tricky and we have rule for it (stated below), 
the correct interpretation is $(Y\rightarrow Z)$. Summarizing what we did, the
entire sentence could be symbolized as $W\land ((Y\rightarrow X)\land (Y\rightarrow Z))$

\subsubsection{``In that case'' treatment}
In english language, when we say `bar foo, and in that case baz' the `case' always refer
to the thing mentioned immediately before. So in our example `foo' would be reason for `baz'.

\section{Derivations in Sentential Logic}
Full truth tables are impartial when we have a long sentence to check validity 
and it turns out that derivation is a nice and simple way to achieve such goal.

\subsection{Ten Basic Rules}
\paragraph{Types of Rules}
\begin{itemize}
    \item \textbf{Elimination Rule} remove the connective (Elimination
     Rules are Automatic Moves)
    \item \textbf{Introduction} introduce the connective
\end{itemize}

\subsubsection{Modus Ponens (MP)} $\phi \rightarrow \psi$, if we know $\phi$, then it is reasonable to say $\psi$
\subsubsection{Modus Tollens (MT)} $\phi \rightarrow \psi$ then $\sim\psi \rightarrow \sim\phi$, aka contrapositive
\subsubsection*{Fallacy} If we deny the antecedent or affirm the consequent, that \textit{doesn't} tell us anything!
\subsubsection{Double Negation (DN)} $\sim\sim\phi \equiv \phi$. Negating a statement two times is the same as the statement itself.
\subsubsection{Repetition (R)} $\phi \rightarrow \phi$ If you know $\phi$ then you know $\phi$

\subsubsection{Conjunction}
\paragraph{Simplification (S or SL/SR)} If I know $\phi\land \psi$ then
 I know $\phi$ and I know $\psi$
\paragraph{Adjunction (ADJ)} If know $\phi$ and I know $\psi$ then I
 know $\phi\land\psi$

\subsubsection{Disjunction}
\paragraph{Modus Tollendo Ponens (MTP)} If I know $\phi \vee \psi$ and 
I know $\sim\phi$ then I know $\psi$. WLOG the other way around is also coorect.
\paragraph{Addition (ADD)} If I know $\phi$ then  I know $\phi \vee \psi$.
 WLOG the other way around is also true.

\subsubsection{Biconditional}
\paragraph{Biconditional-Conditional (BC)} If I know $\phi \iff \psi$ then I 
know that $\phi \rightarrow \psi$ and $\psi \rightarrow \phi$
\paragraph{Conditional-Biconditional (CB)} If I knwo $\phi \rightarrow \psi$ 
and $\psi \rightarrow \phi$, then (by definition) I know  $\phi \iff \psi$

\subsection{Basic Derivations}
\subsubsection{Examples of rules above}
Let's see the Modus Tollens in action
\begin{equation*}
    \sim (S\vee P) \rightarrow \sim R.~ R. ~\text{so}~ S\vee P
\end{equation*}
THIS IS INCORRECT! We have to take the rules ``as is'', since we are constructing a
contrapositive, we will have $\sim\sim (S\vee P)$ as our conclusion. Here is an example of a 
correct usage of Modus Tollens
\begin{equation*}
    \sim P \rightarrow \sim (S\rightarrow Z). ~ \sim\sim (S\rightarrow Z) ~ so ~ \sim\sim P
\end{equation*}
\textbf{Take away:} We can only apply rules ``as they are'' so the double negation, for example,
only works for statements that are of the form $\sim\sim(\text{complicated stuff})$ \textit{we can't 
jump over any step, we have to write down all the steps.}

\subsubsection{Justification Types}
There are three things that we can do here: 1.Restate any of the given premises. 
2. Use any of the aforementioned rules (write down the lines that we used the rules on and 
the abbreviation of the rule that we used) 3. Direct Derivation: usually an 
indicator of arriving at the final conclusion.

\subsubsection{Example (S3.3 E2a)}
Consider the problem $P\rightarrow Q. ~ R\rightarrow\sim Q.~ \sim S \rightarrow R.~ P.~ \text{so}~ S.$
\begin{logicproof}{1}
    \text{\sout{show}}~ S \\
    \begin{subproof}
        P\rightarrow Q & Pr1 \\
        P & Pr4 \\
        Q & 2,3,MP \\
        R \rightarrow \sim Q & Pr2 \\
        \sim\sim Q & 4,DN \\
        \sim R & 5,6,MT \\
        \sim S\rightarrow R & Pr3 \\
        \sim\sim S & 7,8,MT \\
        S & 9,DN \\
        & 10,DD 
    \end{subproof}
    <neglect~this~line>
\end{logicproof}

\subsubsection{Example (S3.3 E2b)}
Consider the problem $Y. ~ X\rightarrow(Y\rightarrow Z).~ \sim X\rightarrow \sim W. ~ W. ~\text{so}~ \sim\sim Z$
\begin{logicproof}{1}
    \text{\sout{show}}~ \sim\sim Z  & \text{show conclusion}\\
    \begin{subproof}
        \sim\sim W & Pr4, DN \\
        \sim X \rightarrow \sim W & Pr3\\
        \sim\sim X & 2,3,MT \\
        X & 4, DN \\
        Y \rightarrow Z & 5, Pr2, MP \\
        Y & Pr1 \\
        Z & 6,7,MP \\
        \sim\sim Z & 8,DN \\
        & 9,DD
    \end{subproof}
    <neglect~this~line>
\end{logicproof}

\subsubsection{Available Lines}
In doing a proof, we want to keep track of what is available for us to use:
\begin{itemize}
    \item Premises, we can always use the premises
    \item Unboxed lines that is not a show line
    \item N.B. A crossed unboxed show line is available (something that we have already shown)
\end{itemize}

\subsubsection{Completeness of a Derivation}
\begin{itemize}
    \item Every show line is crossed off (goals complete)
    \item All lines that are not show lines are boxed off (subproof complete)
    \item Every line except show lines is properly justified (state the reasons for derivations of each step)
\end{itemize}

\subsubsection{Abbreviations}
\begin{itemize}
    \item Donnot restate the premises
    \item Do multiple moves in one line
\end{itemize}
Here is an example of using the abbrevaitions in action. Consider the exmaple
$(P\rightarrow \sim Q) \rightarrow(\sim R \rightarrow S). \sim S. \sim(P\rightarrow \sim Q)\rightarrow T. T\rightarrow S ~\text{so}~R$
\begin{logicproof}{1}
    \text{\sout{show}}~ \sim\sim Z  & \text{show conclusion} \\
    \begin{subproof}
        \sim T & P2, P4, MT \\
        P \rightarrow \sim Q & 2, P3, MT, DN \\
        \sim R \rightarrow S & 3, P1, MP \\
        R & 4, P2, MT, DN, DD
    \end{subproof}
    <neglect~this~line>
\end{logicproof}

\subsubsection{Conditional Derivation (Hypothetical Reasoning)}
We have seen above are all direct derivations, there are two more types of derivations
common. They are Conditional Derivation and Indirect Derivation. Lets take a look at conditional
derivation here. The general format of this type of proofs is $\phi\rightarrow\psi$. To prove 
a conditional, we assume $\phi$ and then prve that $\psi$ follows.

\paragraph{Example} Consider the problem $T\rightarrow S. \sim T \rightarrow \sim R. \text{so}~ R\rightarrow S$
\begin{logicproof}{2}
    \text{\sout{show}}~ R\rightarrow S\\
    \begin{subproof}
        R & ACD (\text{Assume antecedent}) \\
        \text{\sout{show}}~ S  & \text{show consequence}\\
        \begin{subproof}
            \sim\sim R & 2, DN \\
            T & 4, P2, MT, DN \\
            S & P1, 5, MP, DD
        \end{subproof}
        & 3, CD(Conditional Derivation)
    \end{subproof}
    <neglect~this~line>
\end{logicproof}
\paragraph{Caveat:} It is not required to write the derivation
 as a subproof, we can also
carry the moves in just one layer and state on which line we 
achived a conditional derivation

\subsubsection{Indirect Derivation (Reductio ad Absurdum)}
The question takes the following form ``Show $\phi$''. If we assume 
$\sim \phi$ and derive a contradiction, then our assumption must be false.
\paragraph{Example} Consider the problem $P\rightarrow \sim Q. 
R\rightarrow Q. \sim R \rightarrow \sim P. ~\text{so}~ P$
\begin{logicproof}{1}
    \text{\sout{show}} \sim P \\
    \begin{subproof}
        P & AID(Assume ID) \\
        \sim Q & 2, P1, MP \\
        \sim R & P2, 3, MT \\
        \sim P & 4, P3, MP \\
        & 2, 5, ID (Indirect Derivation / Reductio ad Absurdum)
    \end{subproof}
    <neglect~this~line>
\end{logicproof}

\subsubsection{Breaking down show lines}
\begin{itemize}
    \item Look at your most recent show line
    \item If it's of the form $\phi\rightarrow \psi$, start a CD 
    \item If it's any other form, start an ID
\end{itemize}
This works because our system allows mixed derivations!


\subsection*{Theorems}
\textit{Definition:} A Theorem is a tautology. And being a tautology 
means it could be derived from any set of premises, even including
 $\emptyset$ as the premise set.

\subsection{Derived Rules}
There are two types dericed rules: negation of blah rules (NC, NB, and DM) and situational rules.

\subsubsection{Negation of Condition (NC)} If have a negation of a conditional 
statement, then $$\sim (\phi \implies \psi) \equiv \phi \land \sim \psi$$ This is
essentially a `proof by conterexample'
\subsubsection{Negation of Biconditional (NB)}
\begin{equation*}
    \sim (\phi\iff \psi) \equiv (\phi \iff \sim \psi) \equiv (\sim \phi \iff \psi)
\end{equation*}
\subsubsection{De Morgan's Laws (DM)}
\begin{align*}
    &\sim (\phi \vee \psi) \equiv \sim \phi \land \sim \psi \\
    &\sim (\phi \land \psi) \equiv \sim \phi \vee \sim \psi
\end{align*}

\subsubsection{Seperation of Cases (SC)}
If I know $\phi \vee \psi$, $\phi \implies \chi$, and $\psi \implies \chi$, then
it must be the case that $\chi$. \textbf{Special Case of SC:} Since in the generic
SC, we didn't specify what our $\phi$ and $\psi$ need to be, so it we can take $\phi = \xi$
and $\psi = \sim \xi$. In this case, $\psi \vee \phi \equiv \xi \vee \sim \xi$ is
trivially true, and hence it must be the case that $\chi$.

\subsubsection{Conditional as Disjunction (CDJ)}
We have already seen this when symbolizing `unless', but for completeness, I shall
present them here.
\begin{align*}
    &\phi \implies \psi \equiv \sim \phi \vee \psi \\
    &\phi \vee \psi \equiv \sim \phi \implies \psi
\end{align*}

\section{Predicate Logic}
Although our system of sentential logic is great and powerful, it is not quite strong enough since it doesn't allow assesment of logic within the sentence itself. What we need is the predicate logic, as an extension to the sentential system. 

\subsection{Sub-Sentential Logic} The discussion of subjects and predicates: subjects specify what we want to talk about (as its name indicates) and predicates allows to describe the subject in some certain way. 
\subsubsection{Subjects}
\begin{itemize}
    \item \textbf{Singular Terms} Proper names (Joe, Steve), definite descriptors (the person in the front of the class), pronouns (he/she)
    \item \textbf{General Terms} Universally quantifies terms and Existentially quantified terms
\end{itemize}
\subsubsection{Predicates}
DIfferent types of predicates are related to number of subjects that the predicate is related to. 
\begin{itemize}
    \item \textbf{Single Place} Only one subject, description of the one and only subject. For example `Raina is evil. Raina is a student of Plato'
    \item \textbf{Two-Place} Usually describes the relationship between two subjects. For example `Joe likes Marry'
    \item \textbf{Three-Place} Common in mathematics
    \item so on\dots
\end{itemize}

\subsection{Four Types of Statements}
\begin{itemize}
    \item \textbf{Universal Affirmative (A)} All X are Y
    \item \textbf{Existential Affirmative (I)} There exists X that is Y
    \item \textbf{Universal Negative (E)} No X is ever Y
    \item \textbf{Existential Negative (O)} There exists X that is not Y
\end{itemize}

\subsection{Deciding Universal vs Existential}
\subsubsection{Case where both makes sense}
There is generally no right or wrong answer to this question. For example, the sentence `Not all teachers dislike marking' could be the negation of a universally quantified sentence while we can also phrase it as there exists some teacher who likes marking. These two formulation of the sentence are both correct and in fact equivalent. 
\subsubsection{Case where only one way to interpret} 
Consider the sentence `Some animals are dogs'. The only way that we can formulatethis is `there exists some animal, who is a dog'. This case is rather rare.

\subsection{Single-Place Predicate Logic Symbolization}
\subsubsection{Quantifier Symbols}
Two symbols for the universal quantifier is $\forall$ while existential quantifier uses the symbol $\exists$. 
\subsubsection{Subject Symbols}
\begin{itemize}
    \item Inividual constants or names are marks using lower case lettes $a-h$
    \item Variable are written as lower case lettes $i-z$. Notice that this is different from the previous case. For example, when I don't want to specify any name for the subject (i want to pick a generic one) I will need a variable.
    \item Operation Letters is also $a-h$, but possibly with number places as superscript.
\end{itemize}

\subsubsection{Predicate Sumbols}
\begin{itemize}
    \item Predicates are marked with upper case letters $A-O$ (may have number places as superscript in case of non-single place predicates)
    \item The identity sign ($=$) is rather special. More on this latter.
\end{itemize}

\subsubsection{Examples} We use lower case letters or brackets (like \texttt{format} statements) to indicate the position of insertion of the subject. Consider the sentence `Tina Fey is amazing' and define $b:=$ Tina Fey and $A:\{1\}$ is amazing. Then the original sentence can be symbolized as $Ab$.

\subsubsection{Canonical Form of the Universally Quantified Sentence} 
The sentence all $\Phi$'s are $\Psi$'s can be represented in the following canonical form:
\begin{equation*}
    \forall \alpha (\Phi \alpha \implies \Psi \alpha)
\end{equation*}
\begin{equation*}
    \forall \alpha (\text{Group}~\alpha \implies \text{Property}~\alpha)
\end{equation*}

\subsubsection{Formulation of Restricted Clause}
Consider the sentence `All Dogs with cube heads are cute', and define the following $D^1$: \{1\} is a dog. $C^1$: \{1\} is cute. $F^1$:\{1\} has a cube head. There are two way of symnbolizing it, the first one is rather straight forward
\begin{equation*}
    \forall y(Dy \wedge Fy \implies Cy)
\end{equation*}
However, we can also address this using two layers restrictions, like the follow
\begin{equation*}
    \forall y(Dy \implies (Fy \implies Cy))
\end{equation*}

\subsubsection{Canonical Form of the Existentially Quantified Sentence} 
Ways to say `Some $\Phi$'s are $\Psi$'s' OR `At least one of $\Phi$ is $\Psi$' OR `There is a $\Phi$ that is a $\Psi$'. It could be formulated as follows
\begin{equation*}
    \exists \alpha (\Phi \alpha \wedge \Psi \alpha)
\end{equation*}
i.e., 
\begin{equation*}
    \exists \alpha (\text{Group}~ \alpha \wedge \text{Property}~\alpha)
\end{equation*}

\subsection{Single Place Derivations}
\subsubsection{Substitution}
Replace a bound variable to a free one. This transforms a synmbolic sentence to a sumbolic formula. Notice that \textbf{All instances of $\alpha$ are replaced wuth a singular term $\beta$} in the following puppy example
\begin{align*}
    \forall \alpha \phi \alpha &\implies \phi \beta \\
    \exists \alpha \phi \alpha &\implies \phi \beta
\end{align*}

\subsubsection{Universal Instantiation (UI)} This rule describes the subtitution of $\forall \alpha \phi \alpha$ to $ \phi \beta$. This could also be described as the $\forall$ elimination rule. There is a restriction that comes with this rule which is ``\textbf{Restriction:} $\beta$ could not be a bound variable within $\phi \alpha$''. In naive words, this means we can't substitute in a variable that is already quantified in the inner scope. 

\paragraph{Example of Proof using UI}
Suppose I want to show that $\forall x (Fx\implies Gx). Fa. \text{so} Ga.$. The general format is still the same as proofs in sentential logic. We will do
\begin{enumerate}
    \item show $Ga$
    \item \quad $Fa\implies Ga$, Pr1, UI
    \item \quad $Ga$ Pr2, 2, MP
    \item \quad 3, DD
\end{enumerate}

\subsubsection{Existential Instantiation (EI)} If we have a $\exists \alpha \phi \alpha$, we can replace it with $\phi \beta$. The rule comes with a \textbf{Restriction:} $\beta$ must be a \textit{arbitrary} term\footnote{Arbitrary Term: Weak condition is `A term that is not free in an unboxed line', while the strong condition is `A term that does not appear in any previous line or premise'} ($i$ through $z$) that does not occyr in ANY previous line or premise. This will also be refered to ass the $\exists elimination rule$.  

\subsubsection{Golden Rule} EI first, UI to MATCH. Always match UI to something useful. There is one exception to this rule, which is the case of Buried Existential.  

\subsubsection{Example}
Consider the quesiton
\begin{equation*}
    \exists x(Fx \vee \sim Gx). \forall y (Fy \implies Ay). \forall z Gz. \text{~~~So~} \sim \forall x \sim Ax
\end{equation*}
\begin{logicproof}{1}
    \text{\sout{show}} \sim \forall x \sim Ax \\
    \begin{subproof}
        \forall x \sim x & AID \\
        Fi \vee \sim Gi & P1, EI \\
        Gi & P2, UI \\
        Fi & 4, DN, 3, MTP \\
        Fi \implies Ai & P2 UI/i \\
        Ai & 5, 6, MP \\
        \sim Ai & 2, UI \\
        & 7, 8, ID
    \end{subproof}
    <neglect~this~line>
\end{logicproof}

\subsubsection{Existential Generalization (EG)}
If I know $\phi \beta$ then I can conclude $\exists \alpha \phi \alpha$. This rule does come with two restrictions
\begin{itemize}
    \item \textbf{Restriction 1:} $\alpha$ cannot be a bound variable within $\phi \beta$
    \item \textbf{Restriction 2:} If $\alpha $ is different from $\beta$, then $\alpha$ cannot be free within $\phi \beta$
\end{itemize}
This rule is also refered to as the $\exists$ Introduction Rule. I puppy example would be to say `Steven enjoys the notability app', then for sure `there exists something, something is a person and something enjoys the notability app'. There are certain difference between generalization rules and Instantiation rules, we shall take a look at some examples on genralizing `$Fa\wedge Ga$'
\begin{itemize}
    \item $\exists x(Fx \wedge Gx)$ generalizing both $a$'s to $x$
    \item $\exists y(Fy \wedge Gy)$ generalziing both $a$'s to $y$
    \item $\exists x(Fx \wedge Ga)$ generalizing the first $a$ to $x$. Notice that this definately incorrect in the Instantiation settings, but in fact this is valid here in generalization. 
    \item $\exists z(Fa \wedge Gz)$ genralizing the second $a$ to $z$
    \item $\exists x (Fa \wedge Ga)$ generalziing to $x$. This is a wierd one.
\end{itemize}

\subsubsection{Universal Derivation (UD)} This is a technique rather than a rule which allows the introduction of the universal quantifier. To add a universal quantifier, we can simply do a `show' of the statement, with no premise whatsoever. The general format will be
\begin{itemize}
    \item Show $\forall \alpha \phi \alpha$
    \item \quad Show $\phi \alpha$
    \item \quad CD/ID/DD 
    \item $\forall \alpha\phi \alpha$, UD
\end{itemize}
The restriction of this technique is `$\alpha$ cannot appear unbound in any previous available line, or in a premise used in an available line'. This adds to our list of derivation skills: DD, ID, CD. Now we have the fourth way UD. 

\paragraph{Example on UD (Double)}
This is considered to be a double UD example because we have to do UD twice. The exmaple is
\begin{equation*}
    Ha.~\exists w \exists z (Hz \land \neg Gw) \implies \forall x Ax.~\exists i \neg Ai \text{~~So~} \forall x \forall y (Fx \implies Gy)
\end{equation*}


\subsection{Quantifier Negation (QN)}
A quantifier or a negation of quantifier must be the main operator.
\begin{itemize}
    \item $\neg \forall \alpha \phi \equiv \exists \alpha \neg \phi$
    \item $\neg \exists \alpha \phi \equiv \forall \alpha \neg \phi$
\end{itemize}

 


\end{document}
