\documentclass[10pt]{article}
\usepackage[margin=0.4cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{fancyhdr} 
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{youngtab}
\usepackage[dvipsnames]{xcolor}
\setlength{\parskip}{-0.9em}
% \setlength{\itemsep}{-1em}

\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\qed}{\hfill $\mathcal{Q}.\mathcal{E}.\mathcal{D}.\dagger$}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\text{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% Call settag{\ldots} first to initialize, and then \para{} for a new paragraph
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vzero}{\mathbf{0}}
% For convenience, I am setting both of these to refer to the same thing.
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\boldf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bW}{\mathbf{W}}
% \title{CSC311 Introduction to Machine Learning}
% \author{\ccLogo \,\,Tingfeng Xia}
% \date{Fall 2019, modified on \today}

\begin{document}
% \maketitle
% \doclicenseThis
% \tableofcontents
% \newpage
\abovedisplayskip=-0.1cm
\abovedisplayshortskip=-0.2cm
\belowdisplayskip=-0.1cm
\belowdisplayshortskip=0.4cm

\paragraph{KNN} Find $k$ examples $\left\{ \bx^{(i)}, t^{(i)} \right\}$ closest to the test instance $\bx$ and then output majority $\arg \max_{t^{z}} \sum_{r = 1}^k \delta (t^{(z)}, t^{(r)})$. Define $\delta (a, b) = 1$ if $a = b$, $0$ otw. \textbf{Choice of $k$:} Rule is $k < \sqrt{n}$, small $k$ may overfit, while large may under-fit. \textbf{Curse of Dim:} In high dimensions, ``most'' points are approximately the same distance. \textbf{Computation Cost:} 0 (minimal) at training/ no learning involved. Query time find $N$ distances in $D$ dimension $\mathcal{O}(ND)$ and $\mathcal{O}(N \log N)$ sorting time.
% \paragraph{Accuracy Gain} $L(R)-\frac{\left|R_{1}\right| L\left(R_{1}\right)+\left|R_{2}\right| L\left(R_{2}\right)}{\left|R_{1}\right|+\left|R_{2}\right|}$ 
\paragraph{Entropy} $H(X)=-\mathbb{E}_{X \sim p}\left[\log _{2} p(X)\right]=-\sum_{x \in X} p(x) \log _{2} p(x)$ \textbf{ Multi-class: } $H(X, Y)=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} p(x, y)$ \textbf{ Properties: } $H$ is non-negative, $H(Y|X) \leq H(Y)$, $X\perp Y \implies H(Y|X) = H(Y)$, $H(Y|Y) = 0$, and $H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X)$
\paragraph{Expected Conditional Entropy} $ H(Y | X) =\mathbb{E}_{X \sim p(x)}[H(Y | X)] =\sum_{x \in X} p(x) H(Y | X=x) =-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} p(y | x) =-\mathbb{E}_{(X, Y) \sim p(x, y)}\left[\log _{2} p(Y | X)\right]$ \textbf{Information Gain} $IG(Y | X)=H(Y)-H(Y | X)$ 
\paragraph{Bias Variance Decomposition} Using the square error loss $L(y, t)=\frac{1}{2}(y-t)^{2}$, \textbf{Bias ($\uparrow \implies$ under-fitting):} How close is our classifier to true target. \textbf{Variance ($\uparrow \implies$ overfitting):} How widely dispersed are out predictions as we generate new datasets 
$$\begin{aligned} \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-t\right)^{2}\right]=& \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]+\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}\right] \\=& \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}+\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}+ 2\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)\right] \\=& \underbrace{\mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}\right]}_{\text {variance }}+\underbrace{\mathbb{E}_{\mathbf{x}}\left[\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}\right]}_{\text {bias }} \end{aligned} $$ 

\paragraph{Bagging with Generating Distribution} Suppose we could sample $m$
 independent training sets $\left\{ \mathcal{D}_i\right\}_{i=1}^m$ from $p_{dataset}$. Learn $h_i := h_{\mathcal{D}_i}$ and out final predictor is $h = 1/m \sum_{i=1}^m h_i$. \textbf{Bias Unchanged:} 
 $\mathbb{E}_{\mathcal{D}_{1}, \ldots, \mathcal{D}_{m} \stackrel{i i d}{\sim} p_{\text {dataset }}}[h(\mathbf{x})]=\frac{1}{m} \sum_{i=1}^{m} \mathbb{E}_{\mathcal{D}_{i} \sim p_{\text {dataset }}}\left[h_{i}(\mathbf{x})\right]=\mathbb{E}_{\mathcal{D} \sim p_{\text {dataset }}}\left[h_{\mathcal{D}}(\mathbf{x})\right]$ \textbf{Variance Reduced:} $\operatorname{Var}_{\mathcal{D}_{1}, \ldots, \mathcal{D}_{m}}[h(\mathbf{x})]=\frac{1}{m^{2}} \sum_{i=1}^{m} \operatorname{Var}\left[h_{i}(\mathbf{x})\right]=\frac{1}{m} \operatorname{Var}\left[h_{\mathcal{D}}(\mathbf{x})\right]$

\paragraph{Bootstrap Aggregation} Take a single dataset $\mathcal{D}$ with $n$ sample and generate $m$ new datasets, each by sampling $n$ training examples from $\mathcal{D}$, with replacement. We then the average the predictions. We have the reduction in variance to be $\operatorname{Var}\left(\frac{1}{m} \sum_{i=1}^{m} h_{i}(\mathbf{x})\right)=\frac{1}{m}(1-\rho) \sigma^{2}+\rho \sigma^{2}$ 

\paragraph{Random Forest} Upon bootstrap aggregation, for each bag we choose a random set of features to make the trees grow on (decorrelates predictions, lower $\rho$). 

\paragraph{Bayes Optimality} $\mathbb{E}_{\mathbf{x}, \mathcal{D}, t | \mathbf{x}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-t\right)^{2}\right]=  \underbrace{\mathbb{E}_{\mathbf{x}}\left[\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-y_{*}(\mathbf{x})\right)^{2}\right]}_{\text {bias }}+\underbrace{\mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}\right]}_{\text {variance }}+\underbrace{\mathbb{E}_{\mathbf{x}}[\operatorname{Var}[t | \mathbf{x}]]}_{\text {Bayes }} $

% \paragraph{Vectorized Cost} $\mathbf{y}=\mathbf{X} \mathbf{w}+b \mathbf{1}$ and $\mathcal{J}=\frac{1}{2}\|\mathbf{y}-\mathbf{t}\|^{2}$ 

% \paragraph{Regression OLS Direct Solution} 

\paragraph{Feature Mapping} Some time we want fit a polynomial curve, we can do this using a feature map $y=\mathbf{w}^{\top} \boldsymbol{\psi}(x)$ where $\boldsymbol{\psi}(x)=\left[1, x, x^{2}, \ldots\right]^{\top}$. In general the feature map could be anything. 

\paragraph{Ridge Regression} $\mathbf{w}_{\lambda}^{R i d g e}=\underset{\mathbf{w}}{\operatorname{argmin}} \mathcal{J}_{\mathrm{reg}}(\mathbf{w})=\underset{\mathbf{w}}{\operatorname{argmin}} \frac{1}{2}\|\mathbf{X} \mathbf{w}-\mathbf{t}\|_{2}^{2}+\frac{\lambda}{2}\|\mathbf{w}\|_{2}^{2} =\left(\mathbf{X}^{T} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T} \mathbf{t} $ When $\lambda = 0$ this is just OLS.

\paragraph{Gradient Descent} Consider the some cost function $\mathcal{J}$ and we want to optimize it. 
\begin{itemize}
    \setlength\itemsep{-0.45em}

    \item \textbf{GD:} $\mathbf{w} \leftarrow \mathbf{w}-\alpha \frac{\partial \mathcal{J}}{\partial \mathbf{w}}$; \textbf{GD w/ Reg} $\mathbf{w} \leftarrow \mathbf{w}-\alpha\left(\frac{\partial \mathcal{J}}{\partial \mathbf{w}}+\lambda \frac{\partial \mathcal{R}}{\partial \mathbf{w}}\right) =(1-\alpha \lambda) \mathbf{w}-\alpha \frac{\partial \mathcal{J}}{\partial \mathbf{w}}$ 

    \item \textbf{mSGD:} Choose mini batch $\mathcal{M}\subset \{1,...,N\}$ and update $\bw \gets \bw - \frac{\alpha}{|\mathcal{M}|}\sum_{i = 1}^{\mathcal{|M|}} \frac{\partial \mathcal{L}^{(i)}}{\partial \bw}$ \textbf{Reasonable size} would be $\mathcal{|M|} \approx 100$
    
    \item \textbf{SGD:} Choose $i$ at uniform; $\bw \gets \bw - \alpha \frac{\partial \mathcal{L}^{(i)}}{\partial \bw}$; \textbf{Pro//Cons:} Progress w/o seeing all data//High Variance \& Not efficiently vectorized
\end{itemize}


\paragraph{Cross Entropy Loss} $\mathcal{L}_{CE} = -t \log y - (1-t) \log (1-y)$ \textbf{Logistic CE} $\mathcal{L}_{LCE}(z,t) = \mathcal{L}_{CE}(\sigma (z), t) = t\log (1+e^{-z}) + (1-t) \log (1+e^z)$

\paragraph{Multi-class Classification}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Softmax Function} Natural generalization of logistic func: $y_{k}=\operatorname{softmax}\left(z_{1}, \ldots, z_{K}\right)_{k}=\frac{e^{z_{k}}}{\sum_{k^{\prime}} e^{z_{k^{\prime}}}}$; iuputs $z_k$ are called logits. 
    \item \textbf{CE Loss, Vectorized} $\mathcal{L}_{\mathrm{CE}}(\mathbf{y}, \mathbf{t})=-\sum_{k=1}^{K} t_{k} \log y_{k} =-\mathbf{t}^{\top}(\log \mathbf{y})$ where the $\log $ is applied elementwise.
    \item \textbf{Softmax Regression} $\bz = \mathbf{W}\bx + \bb$, $\by = \operatorname{softmax}(\bz)$, and $\mathcal{L}_{CE} = -\mathbf{t}^{\top}(\log \mathbf{y})$; GD Updates is $\mathbf{w}_{k} \leftarrow \mathbf{w}_{k}-\alpha \frac{1}{N} \sum_{i=1}^{N}\left(y_{k}^{(i)}-t_{k}^{(i)}\right) \mathbf{x}^{(i)}$ where $\bw_k$ means the $k$-th row of $\mathbf{W}$
\end{itemize}

\paragraph{Activation Functions} \textbf{Identity} $y = z$ \textbf{ReLU} $y = \max (0, z)$ \textbf{Soft ReLU} $y = \log(1+e^z)$ \textbf{Thresholding} $y = 1$ if $z> 0$ else $0$. \textbf{Logistic} $y=\frac{1}{1+e^{-z}}$ \textbf{tanh} $y=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$

\paragraph{Multilayer Perceptron} 
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Modularity of Layers} $\mathbf{h}^{(1)}=f^{(1)}(\mathbf{x})=\phi\left(\mathbf{W}^{(1)} \mathbf{x}+\mathbf{b}^{(1)}\right)$, $\mathbf{h}^{(2)}=f^{(2)}\left(\mathbf{h}^{(1)}\right)=\phi\left(\mathbf{W}^{(2)} \mathbf{h}^{(1)}+\mathbf{b}^{(2)}\right)$, \dots, $\mathbf{y}=f^{(L)}\left(\mathbf{h}^{(L-1)}\right) = f^{(L)} \circ \cdots \circ f^{(1)}(\mathbf{x})$
    \item \textbf{Choice of Last Layer Activation Func} Regression: $\mathbf{y}=f^{(L)}\left(\mathbf{h}^{(L-1)}\right)=\left(\mathbf{w}^{(L)}\right)^{T} \mathbf{h}^{(L-1)}+b^{(L)}$; Binary Classification: $\mathbf{y}=f^{(L)}\left(\mathbf{h}^{(L-1)}\right)=\sigma\left(\left(\mathbf{w}^{(L)}\right)^{T} \mathbf{h}^{(L-1)}+b^{(L)}\right)$
    \item \textbf{Back Propagation} Suppose $\mathcal{L}$ what I want to optimize, then for some variable $\bw$ that we want to optimize w.r.t., $\frac{\partial \mathcal{L}}{\partial \bw} =: \overbar{\bw}$
    \item \textbf{Back Prop Cost} Forward: one add-multiplicity operation per weight; Backward: two add-multiplicity operations per weight $\implies$ the Backward pass is about as expensive as two Forward passes. (cost is linear in \# of layers, quadratic in \# of units per layer)
\end{itemize}

% TODO: Support vector machine to be added here.


\paragraph{Statistic on Samples}
\begin{itemize}
	\setlength\itemsep{-0.45em}
	\item \textbf{Sample Mean} $\hat{\boldsymbol{\mu}}=\frac{1}{N} \sum_{i=1}^{N} \mathbf{x}^{(i)}$. $\hat{\boldsymbol{\mu}}$ roughly quantifies where your data is located in space
	\item \textbf{Sample Cov} $\hat{\mathbf{\Sigma}}=\frac{1}{N} \sum_{i=1}^{N}\left(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}}\right)\left(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}}\right)^{\top}$ quantifies the shape of spread of the data
\end{itemize}
\paragraph{Euclidean projection} Let $\mathcal{S}$ denote the subspace with dim = $k$ that is spanned by the basis $\left\{ \mathbf{u}_1,...,\mathbf{u}_K \right\} \subseteq \real^D $. Then,
\begin{itemize}
	\setlength\itemsep{-0.45em}
	\item Any vector $\mathbf{y} \in \mathcal{S}$ could be represented as $\mathbf{y} = \sum_{i=1}^K z_i\mathbf{u}_i$, for some $z_1,...,z_k\in \real$
	\item The projection of $\mathbf{x}$ onto $\mathcal{S}$ is given as $\operatorname{Proj}_{\mathcal{S}}(\mathbf{x})= \sum_{i=1}^{K} (\mathbf{x}^{\top} \mathbf{u}_{i}) \mathbf{u}_{i} =\sum_{i=1}^{K} z_{i} \mathbf{u}_{i} \quad \text { where } \quad z_{i}=\mathbf{x}^{\top} \mathbf{u}_{i}$
\end{itemize}

\paragraph{Principle Component Analysis - Projection onto Subspace} 
\begin{itemize}
	\setlength\itemsep{-0.45em}
	\item Let $\left\{\mathbf{u}_{k}\right\}_{k=1}^{K}$ be an \color{BurntOrange}orthonormal \color{Black} basis of the subspace $\mathcal{S}$.
	\item Define $\mathbf{U}$ to be a matrix with columns $\left\{\mathbf{u}_{k}\right\}_{k=1}^{K}$ then ${\mathbf{z}=\mathbf{U}^{T}(\mathbf{x}-\hat{\boldsymbol{\mu}})}$. Here the $\bz$ is called the code vector 
	\item Also, $\tilde{\mathbf{x}}=\hat{\boldsymbol{\mu}}+\mathbf{U} \mathbf{z}=\hat{\boldsymbol{\mu}}+\mathbf{U} \mathbf{U}^{T}(\mathbf{x}-\hat{\boldsymbol{\mu}})$ is called the reconstruction of $\mathbf{x}$
	\item Note: $\mathbf{U} \mathbf{U}^{T}$ is the projector matrix and $\mathbf{U}^{T} \mathbf{U}=I$ is the identity matrix. 
	\item $\bx$ and $\tilde{\bx}$ are both in $\real^D$ while $\tilde{\bx}$ lives in a low dimensional subspace in $\real^D$. The code vector $\bx$ is in $\real^K$, and is the low dim representation of the vector $\bx$
\end{itemize}
\paragraph{PCA - Learning Subspace}
\begin{itemize}
	\setlength\itemsep{-0.45em}
	\item \textbf{Criteria I:} Minimize the reconstruction error: Find vectors in a subspace that are closest to data points, $\min _{\mathbf{U}} \frac{1}{N} \sum_{i=1}^{N}\left\|\mathbf{x}^{(i)}-\tilde{\mathbf{x}}^{(i)}\right\|^{2}$
	\item \textbf{Criteria II:} Maximize the variance of reconstructions: Find subspaces where data has the most variability, $\max _{\mathbf{U}} \frac{1}{N} \sum_{i}\left\|\tilde{\mathbf{x}}^{(i)}-\hat{\boldsymbol{\mu}}\right\|^{2}$
	\item \textbf{Proof: Criteria I $\equiv$ Criteria II}; It suffices to show that $\frac{1}{N} \sum_{i=1}^{N}\left\|\mathbf{x}^{(i)}-\tilde{\mathbf{x}}^{(i)}\right\|^{2}=\text { const }-\frac{1}{N} \sum_{i}\left\|\tilde{\mathbf{x}}^{(i)}-\hat{\boldsymbol{\mu}}\right\|^{2}$
%	\begin{itemize}
%		\setlength\itemsep{-0.45em}
%		\item Using $\tilde{\mathbf{x}}^{(i)}=\hat{\boldsymbol{\mu}}+\mathbf{U} \mathbf{z}^{(i)} \text { and } \mathbf{z}^{(i)}=\mathbf{U}^{\top}\left(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}}\right)$ and $\|\mathbf{v}\|^{2}=\mathbf{v}^{\top} \mathbf{v} \text { and } \mathbf{U}^{\top} \mathbf{U}=I \implies \|\mathbf{U} \mathbf{v}\|^{2}=\mathbf{v}^{\top} \mathbf{U}^{\top} \mathbf{U} \mathbf{v}=\|v\|^{2}$ we have
%			$$
%				\left\|\tilde{\mathbf{x}}^{(i)}-\hat{\boldsymbol{\mu}}\right\|^{2}=\left(\mathbf{U} \mathbf{z}^{(i)}\right)^{\top}\left(\mathbf{U} \mathbf{z}^{(i)}\right)=\left[\mathbf{z}^{(i)}\right]^{\top} \mathbf{U}^{\top} \mathbf{U} \mathbf{z}^{(i)}=\left[\mathbf{z}^{(i)}\right]^{\top} \mathbf{z}^{(i)}=\left\|\mathbf{z}^{(i)}\right\|^{2}
%			$$
%	\end{itemize}
\end{itemize}

\paragraph{Support Vector Machines}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Hinge Loss} is defined as $\mathcal{L}_{z, t}:= \max \{0, 1- zt\}$, where $z:= \bw^T \bx + b$ and $t$ is the target
    \item \textbf{Formulation} $\operatorname{minimize}_{\bw, b} \sum_{i =1}^N\max\{0, 1 - t^{(i)}z^{(i)}(\bw, b)\}$ 
    \item \textbf{L2 - Regularized} $\operatorname{minimize}_{\bw, b} \sum_{i =1}^N\max\{0, 1 - t^{(i)}z^{(i)}(\bw, b)\} + \frac{\lambda}{2} \norm{\bw}^2_2$
    \item \textbf{Optimal Separating \textit{Hyperplane}} A hyperplane that separate two classes and maximizes the distance to the closest point from either class, i.e., maximizes the \textit{margin} ($C = \frac{1}{\norm{\bw}_2}$) of the classifier. 
    \item \textbf{Note:} A separating hyperplane is optimal \textit{iff} it touches three data points near the margin. 
\end{itemize}

\paragraph{AdaBoost Key Concepts}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Boosting:} Train classifier sequentially, each time focusing on training data points that were previously misclassified. 
    \item \textbf{Weighted Training Set:} The weighted misclassification rate is $\sum_{i = 1}^N w^{(n)} \mathbb{I}(h(x^{(n)} \neq t^{(n)}))$ where $w^{(n)} > 0 \text{ and } \sum_n w^{(n)} = 1$
    \item \textbf{Weak Learner (Informal):} Weak learners are algorithms that output slightly better than chance
    \item \textbf{Efficient Weak Learners:} We are interested in weak learners that are computationally efficient, for example decision trees, or even decision stumps (decision trees with only one split) 
    \item \textbf{Weak Classifier:} The error of classifier $h$ according to the given weights $\{w^{(1)},...,w^{(N)}\}$ is $err:= \sum_{n = 1}^N w^{(n)} \mathbb{I}(h(x^{(n)} \neq t^{(n)})) < \frac{1}{2} - \gamma$ for some $\gamma >0$ ~(``better than chance'')
    \item \textbf{Reduced Bias} AdaBoost reduces bias by making each classifier focus on previous mistakes. 
\end{itemize}

\paragraph{AdaBoost Workflow}
\begin{enumerate}
    \setlength\itemsep{-0.45em}
    \item At each iteration we re-weight the training samples by assigning larger weights to samples (data points) that were classified incorrectly. 
    \item We train a new weak classifier based on the re-weighted samples
    \item We add this weak classifier to the ensemble of weak classifiers. This ensemble is our new classifier.
    \item Repeat
\end{enumerate}

\paragraph{AdaBoost Algorithm}
\begin{itemize}
    \setlength\itemsep{-0.4em}
    \item Input data $\mathcal{D}_N = \{\bx^{(n)}, t^{(n)}\}_{n = 1}^N$ where $t^{(n)}\in \{-1, 1\}$
    \item A classifier (hypothesis $h:\bx \to \{-1, 1\}$), and 0-1 loss $\mathbb{I}[h(x^{(n)})\neq t^{(n)}] := \frac{1}{2}(1 - h(x^{n})\cdot t^{(n)})$ 
    \item A classification procedure that returns a classifier $h$, e.g. best decision stump from a set of classifier $\mathcal{H}$, e.g. all possible decision stumps)
    \item Output a master classifier $H(x)$
    \item For $t = 1,...,T$ ($T$ is the number of iteration)
    \begin{enumerate}
    \setlength\itemsep{-1.8em}
    \item Fit a classifier to data using weighted samples $h_t \gets WeakLearn(\mathcal{D}_N,\bw)$. For example we can use $$h_t \gets \arg\min_{h \in \mathcal{H}}\sum_{n = 1}^N w^{(n)}\mathbb{I} \{h(\bx^{(n)} \neq t^{(n)}\} $$
    \item Compute the weighted error $$\mathrm{err}_t = \frac{\sum_{n=1}^{N} w^{(n)} \mathbb{I}\left\{h_{t}\left(\mathbf{x}^{(n)}\right) \neq t^{(n)}\right\}}{\sum_{n=1}^{N} w^{(n)}}$$
    \item Compute classifier coefficient $$\alpha_{t}=\frac{1}{2} \log \frac{1-\operatorname{err}_{t}}{\operatorname{err}_{t}} \quad(\in(0, \infty))$$
    \item Update data weights $$w^{(n)} \leftarrow w^{(n)} \exp \left(-\alpha_{t} t^{(n)} h_{t}\left(\mathbf{x}^{(n)}\right)\right)\left[\equiv w^{(n)} \exp \left(2 \alpha_{t} \mathbb{I}\left\{h_{t}\left(\mathbf{x}^{(n)}\right) \neq t^{(n)}\right\}\right)\right]$$
\end{enumerate}
    \item Return $H(x) = \operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} h_{t}(\mathbf{x})\right)$
\end{itemize}
\paragraph{AdaBoost Weighting Intuition}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item $H(\mathbf{x})=\operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} h_{t}(\mathbf{x})\right) \text { where } \alpha_{t}=\frac{1}{2} \log \frac{1-\mathrm{err}_{t}}{\mathrm{err}_{t}}$ is ``{how much you trust weak learner $t$}''. 
    \item Weak classifiers which get lower weighted error get more weight in the final classifier. When some $err_t$ is small, $\alpha_t = \frac{1}{2}\log \frac{1 - err_t}{err_t}$ is large in the final classifier.  
    \item Also in weight updates, $w^{(n)} \leftarrow w^{(n)} \exp \left(2 \alpha_{t} \mathbb{I}\left\{h_{t}\left(\mathbf{x}^{(n)}\right) \neq t^{(n)}\right\}\right)$. \textbf{If $err_t \approx 0$ then}, $\alpha_t$ hight so misclassified examples get more attention. \textbf{Else If $err_t \approx 0.5$ then} $\alpha_t$ low hence misclassified examples not emphasized. 
\end{itemize}

\paragraph{AdaBoost Geometric Convergence and Generalization Errors}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Theorem:} Assume that at each iteration of AdaBoost the WeakLearn returns a hypothesis with error $err_t < \frac{1}{2} - \gamma$ for all $t = 1,...,T$ with $\gamma > 0$. The trainning error of the output hypothesis $H(\bx) = \operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} h_{t}(\mathbf{x})\right)$ is at most $$L_{N}(H)=\frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left\{H\left(\mathbf{x}^{(i)}\right) \neq t^{(i)}\right\} \leq \exp \left(-2 \gamma^{2} T\right)$$ under the simplifying assumption that each weak learner is $\gamma>0$ better than a random predictor. The convergence is called geometric convergence, very fast!
    \item \textbf{Generalization Error:} \textit{AdaBoost's Training error converges to zero.} In testing set, AdaBoost can overfit when we add too much weak learners. However this doesn't always happen. \textbf{There are cases where the testing error keeps decreasing even when training error is zero.} \color{Red}WHY?\color{Black}~This is because even when training error is zero, the margin (= sample distance to decision boundary) is still improved by further boosting iterations. 
\end{itemize}

\paragraph{Additive Models - AdaBoost Alternative View Point}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item Consider hypothesis class $\mathcal{H}$ with $\mathcal{H} \ni h_i: \bx \to \{-1, 1\}$ weak learners. Aka bases in this context.
    \item An additive model with $m$ terms is given by $H_{m}(x)=\sum_{i=1}^{m} \alpha_{i} h_{i}(\mathbf{x})$ where $(\alpha_1,...,\alpha_m) \in \real^m$ (generally $\alpha_i \geq 0$ and $\sum_i \alpha_i = 1$)
    \item \textbf{Stage-wise Training of Additive Models}
    \begingroup
    \begin{algorithm}
    \abovedisplayskip=0.1cm
    \abovedisplayshortskip=-0.1cm
    \belowdisplayskip=-0.1cm
    \belowdisplayshortskip=0.1cm
    Initialize $H_0(x) = 0$ \\
    For $m = 1$ up to $T$ do \\
    \indent ~~~~~~ Compute $m$-th hypothesis $h_m = H_{m - 1} + \alpha_m h_m$, i.e. $h_m$ and $\alpha_m$ assuming previous additive model $H_{m-1}$ is fixed
    \begin{equation}\left(h_{m}, \alpha_{m}\right) \leftarrow \underset{h \in \mathcal{H}, \alpha}{\operatorname{argmin}} \sum_{i=1}^{N} \mathcal{L}\left(H_{m-1}\left(\mathbf{x}^{(i)}\right)+\alpha h\left(\mathbf{x}^{(i)}\right), t^{(i)}\right)\end{equation}
    \indent ~~~~~~ Add it to the additive model
    $$H_m = H_{m - 1} + \alpha_m h_m$$
    \end{algorithm}
    \endgroup
    \item \textbf{Additive Model with Exp Loss:} Consider the Exponential Loss $\mathcal{L}_E (z, t) := \exp (-tz)$. Then, (1) becomes
    \small{
    \begin{align*}
        \left(h_{m}, \alpha_{m}\right) &\leftarrow \underset{h \in \mathcal{H}, \alpha}{\operatorname{argmin}} \sum_{i=1}^{N} \exp \left(-\left[H_{m-1}\left(\mathbf{x}^{(i)}\right)+\alpha h\left(\mathbf{x}^{(i)}\right)\right] t^{(i)}\right) = \sum_{i=1}^{N} \exp \left(-H_{m-1}\left(\mathbf{x}^{(i)}\right) t^{(i)}-\alpha h\left(\mathbf{x}^{(i)}\right) t^{(i)}\right) \\
        &=\sum_{i=1}^{N}\underbrace{\exp \left(-H_{m-1}\left(\mathbf{x}^{(i)}\right) t^{(i)}\right) }_{ \triangleq w_{i}^{(m)} }\exp \left(-\alpha h\left(\mathbf{x}^{(i)}\right) t^{(i)}\right) =\sum_{i=1}^{N} w_{i}^{(m)} \exp \left(-\alpha h\left(\mathbf{x}^{(i)}\right) t^{(i)}\right)
    \end{align*}}
    \item \textbf{$h$ Optimized at:} $h_{m} \leftarrow \underset{h \in \mathcal{H}}{\operatorname{argmin}} \sum_{i=1}^{N} w_{i}^{(m)} \exp \left(-\alpha h\left(\mathbf{x}^{(i)}\right) t^{(i)}\right) \equiv \underset{h \in \mathcal{H}}{\operatorname{argmin}} \sum_{i=1}^{N} w_{i}^{(m)} \mathbb{I}\left\{h\left(\mathbf{x}^{(i)}\right) \neq t^{(i)}\right\}$
    \item \textbf{Weight Update Optimized at:} $w_{i}^{(m + 1)} \gets w_{i}^{(m)} \exp \left(-\alpha_{m} h_{m}\left(\mathbf{x}^{(i)}\right) t^{(i)}\right)$
    \item \textbf{Coefficient Optimized at:} $\alpha=\frac{1}{2} \log \left(\frac{1-\operatorname{err}_{m}}{\operatorname{err}_{m}}\right)$ \quad where $err_m \triangleq \frac{\sum_{i=1}^{N} w_{i}^{(m)} \mathbb{I}\left\{h_{m}\left(\mathbf{x}^{(i)}\right) \neq t^{(i)}\right\}}{\sum_{i=1}^{N} w_{i}^{(m)}}$
\end{itemize}

\paragraph{Na\"ive Bayes}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Na\"ive Assumption:} Na\"ive Bayes assumes that the features are \textit{conditionally independent given the class $c$}, i.e. $p(c, x_1, ..., x_D) = p(c) p(x_1 |c) ... p(x_D|c)$. \textbf{Benefit:} This gives us a compact representation of the joint distribution. $\mathcal{O}({2^{D+1} - 1}) \to  \mathcal{O}(2D + 1)$
    \item \textbf{Bayes Rule} $p(c|\bx) = \frac{p(\bx, c)}{p(\bx)} = \frac{p(\bx |c) p(c)}{p(\bx)}$ ~~ \textbf{Formally}~ $\text{posterior} = \frac{\text{Class likelihood } \times \text{ prior}}{\text{Evidence}}$
    \item \textbf{Na\"ive Bayes Inference} $p(c | \mathbf{x})=\frac{p(c) p(\mathbf{x} | c)}{\sum_{c^{\prime}} p\left(c^{\prime}\right) p\left(\mathbf{x} | c^{\prime}\right)}=\frac{p(c) \prod_{j=1}^{D} p\left(x_{j} | c\right)}{\sum_{c^{\prime}} p\left(c^{\prime}\right) \prod_{j=1}^{D} p\left(x_{j} | c^{\prime}\right)}$ ~~\textbf{Shorthand}~$p(c | \mathbf{x}) \propto p(c) \prod_{j=1}^{D} p\left(x_{j} | c\right)$
    \item \textbf{Computational Cost of Na\"ive Bayes:} (1) \textbf{Training Time:} estimate parameters using MLE, requires one pass in the data. (2) \textbf{Test Time:} apply Baye's Rule. Cheap because of the model structure. 
\end{itemize}

\paragraph{Bayesian Parameter Estimation}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Posterior Distribution:} $p(\boldsymbol{\theta} | \mathcal{D})=\frac{p(\boldsymbol{\theta}) p(\mathcal{D} | \boldsymbol{\theta})}{\int p\left(\boldsymbol{\theta}^{\prime}\right) p\left(\mathcal{D} | \boldsymbol{\theta}^{\prime}\right) \mathrm{d} \boldsymbol{\theta}^{\prime}}$
    \item \textbf{Gamma As Prior:} $p(\theta ; a, b)= \color{Gray}\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \color{Black}\theta^{a-1}(1-\theta)^{b-1}$.~ \textbf{Proportionality:} $p(\theta ; a, b) \propto \theta^{a-1}(1-\theta)^{b-1}$
    \item \textbf{Maximum A-posteriori Estimation:} $\hat{\boldsymbol{\theta}}_{\mathrm{MAP}}=\arg \max _{\boldsymbol{\theta}} p(\boldsymbol{\theta} | \mathcal{D}) =\arg \max _{\boldsymbol{\theta}} p(\boldsymbol{\theta}) p(\mathcal{D} | \boldsymbol{\theta}) = \arg \max _{\boldsymbol{\theta}} \log p(\boldsymbol{\theta})+\log p(\mathcal{D} | \boldsymbol{\theta})$
\end{itemize}

\paragraph{Properties of Gaussian Distribution}
\begin{itemize}
    \setlength\itemsep{-0.65em}
    \item $\mathbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})$ is defined as $p(\mathbf{x})=\frac{1}{(2 \pi)^{d / 2}|\mathbf{\Sigma}|^{1 / 2}} \exp \left[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right]$
    \item \textbf{Empirical Mean} $\hat{\boldsymbol{\mu}}=\frac{1}{N} \sum_{i=1}^{N} \mathbf{x}^{(i)}$~~ \textbf{Empirical Cov}~ $\hat{\mathbf{\Sigma}}=\frac{1}{N} \sum_{i=1}^{N}\left(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}}\right)\left(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}}\right)^{\top}$
\end{itemize}

\paragraph{Gaussian Discriminant Analysis (Gaussian Bayes Classifier)}
\begin{itemize}
    \setlength\itemsep{-0.4em}
    \item \textbf{Idea:} Make decisions by comparing class posteriors. $\log p\left(t_{k} | \mathbf{x}\right)=\log p\left(\mathbf{x} | t_{k}\right)+\log p\left(t_{k}\right)-\log p(\mathbf{x})$
    \item \textbf{Expanded} $ \log p\left(t_{k} | \mathbf{x}\right) = \color{Red} -\frac{d}{2} \log (2 \pi) -\frac{1}{2} \log \left|\boldsymbol{\Sigma}_{k}^{-1}\right| \color{Black}-\frac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}_{k}\right)^{T} \boldsymbol{\Sigma}_{k}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_{k}\right) \color{Red} +\log p\left(t_{k}\right)-\log p(\mathbf{x})\color{Black}$
    \item \textbf{Decision Boundary:} $\log p\left(t_{k} | \mathbf{x}\right)=\log p\left(t_{l} | \mathbf{x}\right) \implies \left(\mathbf{x}-\boldsymbol{\mu}_{k}\right)^{T} \boldsymbol{\Sigma}_{k}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_{k}\right)=\left(\mathbf{x}-\boldsymbol{\mu}_{\ell}\right)^{T} \boldsymbol{\Sigma}_{\ell}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_{\ell}\right)+\color{Red}C_{k, l}\color{Black}$ \\Then, $\mathbf{x}^{T} \boldsymbol{\Sigma}_{k}^{-1} \mathbf{x}-2 \boldsymbol{\mu}_{k}^{T} \mathbf{\Sigma}_{k}^{-1} \mathbf{x}=\mathbf{x}^{T} \mathbf{\Sigma}_{\ell}^{-1} \mathbf{x}-2 \boldsymbol{\mu}_{\ell}^{T} \mathbf{\Sigma}_{\ell}^{-1} \mathbf{x}+\color{Red}C_{k, l}\color{Black}$
    \item \textbf{Decision Boundary:} is quadratic since gaussian is quadratic. When we have to humps that share the same covariance, the decision boundary is linear. 
    \item \textbf{VS Logistic Regression}~ (1) GDA is generative while LR is discriminative model. (2) GDA makes stringer modelling assumptions: assumes gaussian distributon. When assumption true, GDA asymptotically efficient. (3) LR more robust, less sensitive to incorrect modelling assumptions (LR uses CE, no assumption.) (4) Class-conditional distributions usually lead to logistic classifier. 
\end{itemize}




\end{document}
