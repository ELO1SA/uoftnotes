\documentclass[10pt]{article}
\usepackage[margin=0.3cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{fancyhdr} 
\usepackage{youngtab}
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\qed}{\hfill $\mathcal{Q}.\mathcal{E}.\mathcal{D}.\dagger$}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\text{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% Call settag{\ldots} first to initialize, and then \para{} for a new paragraph
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vzero}{\mathbf{0}}
% For convenience, I am setting both of these to refer to the same thing.
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\boldf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bm}{\mathbf{m}}

% \title{CSC311 Introduction to Machine Learning}
% \author{\ccLogo \,\,Tingfeng Xia}
% \date{Fall 2019, modified on \today}

\begin{document}
% \maketitle
% \doclicenseThis
% \tableofcontents
% \newpage

\paragraph{KNN} Find $k$ examples $\left\{ \bx^{(i)}, t^{(i)} \right\}$ closest to the test instance $\bx$ and then output majority $\arg \max_{t^{z}} \sum_{r = 1}^k \delta (t^{(z)}, t^{(r)})$. Define $\delta (a, b) = 1$ if $a = b$, $0$ otw. \textbf{Choice of $k$:} Rule is $k < \sqrt{n}$, small $k$ may overfit, while large may underfit. \textbf{Curse of Dim:} In high dimensions, ``most'' points are approximately the same distance. \textbf{Computation Cost:} 0 (minimal) at trianing/ no learning involved. Query time find $N$ distances in $D$ dimension $\mathcal{O}(ND)$ and $\mathcal{O}(N \log N)$ sorting time.
% \paragraph{Accuracy Gain} $L(R)-\frac{\left|R_{1}\right| L\left(R_{1}\right)+\left|R_{2}\right| L\left(R_{2}\right)}{\left|R_{1}\right|+\left|R_{2}\right|}$ 
\paragraph{Entropy} $H(X)=-\mathbb{E}_{X \sim p}\left[\log _{2} p(X)\right]=-\sum_{x \in X} p(x) \log _{2} p(x)$ \textbf{ Multi-class: } $H(X, Y)=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} p(x, y)$ \textbf{ Properties: } $H$ is non-negative, $H(Y|X) \leq H(Y)$, $X\perp Y \implies H(Y|X) = H(Y)$, $H(Y|Y) = 0$, and $H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X)$
\paragraph{Expected Conditional Entropy} $ H(Y | X) =\mathbb{E}_{X \sim p(x)}[H(Y | X)] =\sum_{x \in X} p(x) H(Y | X=x) =-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} p(y | x) =-\mathbb{E}_{(X, Y) \sim p(x, y)}\left[\log _{2} p(Y | X)\right]$
\paragraph{Information Gain} $IG(Y | X)=H(Y)-H(Y | X)$ 
\paragraph{Bias Variance Decomposition} Using the square error loss $L(y, t)=\frac{1}{2}(y-t)^{2}$, \textbf{Bias ($\uparrow \implies$ underfitting):} How close is our classifier to true target. \textbf{Variance ($\uparrow \implies$ overfitting):} How widely dispersed are out predictions as we generate new datasets 
$$\begin{aligned} \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-t\right)^{2}\right]=& \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]+\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}\right] \\=& \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}+\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}+ 2\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)\right] \\=& \underbrace{\mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}\right]}_{\text {variance }}+\underbrace{\mathbb{E}_{\mathbf{x}}\left[\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}\right]}_{\text {bias }} \end{aligned} $$ 

\paragraph{Bagging with Generating Distribution} Suppose we could sample $m$
 independent trianing sets $\left\{ \mathcal{D}_i\right\}_{i=1}^m$ from $p_{dataset}$. Learn $h_i := h_{\mathcal{D}_i}$ and out final predictor is $h = 1/m \sum_{i=1}^m h_i$. \textbf{Bias Unchanged:} 
 $\mathbb{E}_{\mathcal{D}_{1}, \ldots, \mathcal{D}_{m} \stackrel{i i d}{\sim} p_{\text {dataset }}}[h(\mathbf{x})]=\frac{1}{m} \sum_{i=1}^{m} \mathbb{E}_{\mathcal{D}_{i} \sim p_{\text {dataset }}}\left[h_{i}(\mathbf{x})\right]=\mathbb{E}_{\mathcal{D} \sim p_{\text {dataset }}}\left[h_{\mathcal{D}}(\mathbf{x})\right]$ \textbf{Variance Reduced:} $\operatorname{Var}_{\mathcal{D}_{1}, \ldots, \mathcal{D}_{m}}[h(\mathbf{x})]=\frac{1}{m^{2}} \sum_{i=1}^{m} \operatorname{Var}\left[h_{i}(\mathbf{x})\right]=\frac{1}{m} \operatorname{Var}\left[h_{\mathcal{D}}(\mathbf{x})\right]$

\paragraph{Bootstrap Aggregation} Take a single dataset $\mathcal{D}$ with $n$ sample and generate $m$ new datasets, each by sampling $n$ training examples from $\mathcal{D}$, with replacement. We then the average the predictions. We have the reduction in variance to be $\operatorname{Var}\left(\frac{1}{m} \sum_{i=1}^{m} h_{i}(\mathbf{x})\right)=\frac{1}{m}(1-\rho) \sigma^{2}+\rho \sigma^{2}$ 

\paragraph{Random Forest} Upon bootstrap aggregation, for each bag we choose a random set of features to make the trees grow on (decorrelates predictions, lower $\rho$). 

\paragraph{Bayes Optimality} $\mathbb{E}_{\mathbf{x}, \mathcal{D}, t | \mathbf{x}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-t\right)^{2}\right]=  \underbrace{\mathbb{E}_{\mathbf{x}}\left[\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-y_{*}(\mathbf{x})\right)^{2}\right]}_{\text {bias }}+\underbrace{\mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}\right]}_{\text {variance }}+\underbrace{\mathbb{E}_{\mathbf{x}}[\operatorname{Var}[t | \mathbf{x}]]}_{\text {Bayes }} $

\paragraph{Vectorized Cost} $\mathbf{y}=\mathbf{X} \mathbf{w}+b \mathbf{1}$ and $\mathcal{J}=\frac{1}{2}\|\mathbf{y}-\mathbf{t}\|^{2}$ 

% \paragraph{Regression OLS Direct Solution} 

\paragraph{Feature Mapping} Some time we want fit a polynomial curve, we can do this using a feature map $y=\mathbf{w}^{\top} \boldsymbol{\psi}(x)$ where $\boldsymbol{\psi}(x)=\left[1, x, x^{2}, \ldots\right]^{\top}$. In general the feature map could be anything. 

\paragraph{Ridge Regression} $\mathbf{w}_{\lambda}^{R i d g e}=\underset{\mathbf{w}}{\operatorname{argmin}} \mathcal{J}_{\mathrm{reg}}(\mathbf{w})=\underset{\mathbf{w}}{\operatorname{argmin}} \frac{1}{2}\|\mathbf{X} \mathbf{w}-\mathbf{t}\|_{2}^{2}+\frac{\lambda}{2}\|\mathbf{w}\|_{2}^{2} =\left(\mathbf{X}^{T} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T} \mathbf{t} $ Notice that when $\lambda = 0$ this is just OLS solution.

\paragraph{Gradient Descent} Consider the some cost function $\mathcal{J}$ and we want to optimize it. \textbf{GD:} $\mathbf{w} \leftarrow \mathbf{w}-\alpha \frac{\partial \mathcal{J}}{\partial \mathbf{w}}$ \textbf{GD w/ Reg} $\mathbf{w} \leftarrow \mathbf{w}-\alpha\left(\frac{\partial \mathcal{J}}{\partial \mathbf{w}}+\lambda \frac{\partial \mathcal{R}}{\partial \mathbf{w}}\right) =(1-\alpha \lambda) \mathbf{w}-\alpha \frac{\partial \mathcal{J}}{\partial \mathbf{w}}$ \textbf{mSGD:} \textbf{SGD:}

\paragraph{a}

\end{document}
