\documentclass[10pt]{article}
\usepackage[margin=0.5cm]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage{blindtext}
\usepackage{fancyhdr} 
\usepackage{youngtab}
\usepackage[dvipsnames]{xcolor}
\setlength{\parskip}{-0.7em}
% \setlength{\itemsep}{-1em}

\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\qed}{\hfill $\mathcal{Q}.\mathcal{E}.\mathcal{D}.\dagger$}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\text{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% Call settag{\ldots} first to initialize, and then \para{} for a new paragraph
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vzero}{\mathbf{0}}
% For convenience, I am setting both of these to refer to the same thing.
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\boldf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bW}{\mathbf{W}}
% \title{CSC311 Introduction to Machine Learning}
% \author{\ccLogo \,\,Tingfeng Xia}
% \date{Fall 2019, modified on \today}

\begin{document}
% \maketitle
% \doclicenseThis
% \tableofcontents
% \newpage

\paragraph{KNN} Find $k$ examples $\left\{ \bx^{(i)}, t^{(i)} \right\}$ closest to the test instance $\bx$ and then output majority $\arg \max_{t^{z}} \sum_{r = 1}^k \delta (t^{(z)}, t^{(r)})$. Define $\delta (a, b) = 1$ if $a = b$, $0$ otw. \textbf{Choice of $k$:} Rule is $k < \sqrt{n}$, small $k$ may overfit, while large may under-fit. \textbf{Curse of Dim:} In high dimensions, ``most'' points are approximately the same distance. \textbf{Computation Cost:} 0 (minimal) at training/ no learning involved. Query time find $N$ distances in $D$ dimension $\mathcal{O}(ND)$ and $\mathcal{O}(N \log N)$ sorting time.
% \paragraph{Accuracy Gain} $L(R)-\frac{\left|R_{1}\right| L\left(R_{1}\right)+\left|R_{2}\right| L\left(R_{2}\right)}{\left|R_{1}\right|+\left|R_{2}\right|}$ 
\paragraph{Entropy} $H(X)=-\mathbb{E}_{X \sim p}\left[\log _{2} p(X)\right]=-\sum_{x \in X} p(x) \log _{2} p(x)$ \textbf{ Multi-class: } $H(X, Y)=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} p(x, y)$ \textbf{ Properties: } $H$ is non-negative, $H(Y|X) \leq H(Y)$, $X\perp Y \implies H(Y|X) = H(Y)$, $H(Y|Y) = 0$, and $H(X,Y) = H(X|Y) + H(Y) = H(Y|X) + H(X)$
\paragraph{Expected Conditional Entropy} $ H(Y | X) =\mathbb{E}_{X \sim p(x)}[H(Y | X)] =\sum_{x \in X} p(x) H(Y | X=x) =-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log _{2} p(y | x) =-\mathbb{E}_{(X, Y) \sim p(x, y)}\left[\log _{2} p(Y | X)\right]$ \textbf{Information Gain} $IG(Y | X)=H(Y)-H(Y | X)$ 
\paragraph{Bias Variance Decomposition} Using the square error loss $L(y, t)=\frac{1}{2}(y-t)^{2}$, \textbf{Bias ($\uparrow \implies$ under-fitting):} How close is our classifier to true target. \textbf{Variance ($\uparrow \implies$ overfitting):} How widely dispersed are out predictions as we generate new datasets 
$$\begin{aligned} \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-t\right)^{2}\right]=& \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]+\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}\right] \\=& \mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}+\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}+ 2\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)\right] \\=& \underbrace{\mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}\right]}_{\text {variance }}+\underbrace{\mathbb{E}_{\mathbf{x}}\left[\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-t\right)^{2}\right]}_{\text {bias }} \end{aligned} $$ 

\paragraph{Bagging with Generating Distribution} Suppose we could sample $m$
 independent training sets $\left\{ \mathcal{D}_i\right\}_{i=1}^m$ from $p_{dataset}$. Learn $h_i := h_{\mathcal{D}_i}$ and out final predictor is $h = 1/m \sum_{i=1}^m h_i$. \textbf{Bias Unchanged:} 
 $\mathbb{E}_{\mathcal{D}_{1}, \ldots, \mathcal{D}_{m} \stackrel{i i d}{\sim} p_{\text {dataset }}}[h(\mathbf{x})]=\frac{1}{m} \sum_{i=1}^{m} \mathbb{E}_{\mathcal{D}_{i} \sim p_{\text {dataset }}}\left[h_{i}(\mathbf{x})\right]=\mathbb{E}_{\mathcal{D} \sim p_{\text {dataset }}}\left[h_{\mathcal{D}}(\mathbf{x})\right]$ \textbf{Variance Reduced:} $\operatorname{Var}_{\mathcal{D}_{1}, \ldots, \mathcal{D}_{m}}[h(\mathbf{x})]=\frac{1}{m^{2}} \sum_{i=1}^{m} \operatorname{Var}\left[h_{i}(\mathbf{x})\right]=\frac{1}{m} \operatorname{Var}\left[h_{\mathcal{D}}(\mathbf{x})\right]$

\paragraph{Bootstrap Aggregation} Take a single dataset $\mathcal{D}$ with $n$ sample and generate $m$ new datasets, each by sampling $n$ training examples from $\mathcal{D}$, with replacement. We then the average the predictions. We have the reduction in variance to be $\operatorname{Var}\left(\frac{1}{m} \sum_{i=1}^{m} h_{i}(\mathbf{x})\right)=\frac{1}{m}(1-\rho) \sigma^{2}+\rho \sigma^{2}$ 

\paragraph{Random Forest} Upon bootstrap aggregation, for each bag we choose a random set of features to make the trees grow on (decorrelates predictions, lower $\rho$). 

\paragraph{Bayes Optimality} $\mathbb{E}_{\mathbf{x}, \mathcal{D}, t | \mathbf{x}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-t\right)^{2}\right]=  \underbrace{\mathbb{E}_{\mathbf{x}}\left[\left(\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]-y_{*}(\mathbf{x})\right)^{2}\right]}_{\text {bias }}+\underbrace{\mathbb{E}_{\mathbf{x}, \mathcal{D}}\left[\left(h_{\mathcal{D}}(\mathbf{x})-\mathbb{E}_{\mathcal{D}}\left[h_{\mathcal{D}}(\mathbf{x})\right]\right)^{2}\right]}_{\text {variance }}+\underbrace{\mathbb{E}_{\mathbf{x}}[\operatorname{Var}[t | \mathbf{x}]]}_{\text {Bayes }} $

% \paragraph{Vectorized Cost} $\mathbf{y}=\mathbf{X} \mathbf{w}+b \mathbf{1}$ and $\mathcal{J}=\frac{1}{2}\|\mathbf{y}-\mathbf{t}\|^{2}$ 

% \paragraph{Regression OLS Direct Solution} 

\paragraph{Feature Mapping} Some time we want fit a polynomial curve, we can do this using a feature map $y=\mathbf{w}^{\top} \boldsymbol{\psi}(x)$ where $\boldsymbol{\psi}(x)=\left[1, x, x^{2}, \ldots\right]^{\top}$. In general the feature map could be anything. 

\paragraph{Ridge Regression} $\mathbf{w}_{\lambda}^{R i d g e}=\underset{\mathbf{w}}{\operatorname{argmin}} \mathcal{J}_{\mathrm{reg}}(\mathbf{w})=\underset{\mathbf{w}}{\operatorname{argmin}} \frac{1}{2}\|\mathbf{X} \mathbf{w}-\mathbf{t}\|_{2}^{2}+\frac{\lambda}{2}\|\mathbf{w}\|_{2}^{2} =\left(\mathbf{X}^{T} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T} \mathbf{t} $ When $\lambda = 0$ this is just OLS.

\paragraph{Gradient Descent} Consider the some cost function $\mathcal{J}$ and we want to optimize it. 
\begin{itemize}
    \setlength\itemsep{-0.45em}

    \item \textbf{GD:} $\mathbf{w} \leftarrow \mathbf{w}-\alpha \frac{\partial \mathcal{J}}{\partial \mathbf{w}}$; \textbf{GD w/ Reg} $\mathbf{w} \leftarrow \mathbf{w}-\alpha\left(\frac{\partial \mathcal{J}}{\partial \mathbf{w}}+\lambda \frac{\partial \mathcal{R}}{\partial \mathbf{w}}\right) =(1-\alpha \lambda) \mathbf{w}-\alpha \frac{\partial \mathcal{J}}{\partial \mathbf{w}}$ 

    \item \textbf{mSGD:} Choose mini batch $\mathcal{M}\subset \{1,...,N\}$ and update $\bw \gets \bw - \frac{\alpha}{|\mathcal{M}|}\sum_{i = 1}^{\mathcal{|M|}} \frac{\partial \mathcal{L}^{(i)}}{\partial \bw}$ \textbf{Reasonable size} would be $\mathcal{|M|} \approx 100$
    
    \item \textbf{SGD:} Choose $i$ at uniform; $\bw \gets \bw - \alpha \frac{\partial \mathcal{L}^{(i)}}{\partial \bw}$; \textbf{Pro//Cons:} Progress w/o seeing all data//High Variance \& Not efficiently vectorized
\end{itemize}


\paragraph{Cross Entropy Loss} $\mathcal{L}_{CE} = -t \log y - (1-t) \log (1-y)$ \textbf{Logistic CE} $\mathcal{L}_{LCE}(z,t) = \mathcal{L}_{CE}(\sigma (z), t) = t\log (1+e^{-z}) + (1-t) \log (1+e^z)$

\paragraph{Multi-class Classification}
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Softmax Function} Natural generalization of logistic func: $y_{k}=\operatorname{softmax}\left(z_{1}, \ldots, z_{K}\right)_{k}=\frac{e^{z_{k}}}{\sum_{k^{\prime}} e^{z_{k^{\prime}}}}$; iuputs $z_k$ are called logits. 
    \item \textbf{CE Loss, Vectorized} $\mathcal{L}_{\mathrm{CE}}(\mathbf{y}, \mathbf{t})=-\sum_{k=1}^{K} t_{k} \log y_{k} =-\mathbf{t}^{\top}(\log \mathbf{y})$ where the $\log $ is applied elementwise.
    \item \textbf{Softmax Regression} $\bz = \mathbf{W}\bx + \bb$, $\by = \operatorname{softmax}(\bz)$, and $\mathcal{L}_{CE} = -\mathbf{t}^{\top}(\log \mathbf{y})$; GD Updates is $\mathbf{w}_{k} \leftarrow \mathbf{w}_{k}-\alpha \frac{1}{N} \sum_{i=1}^{N}\left(y_{k}^{(i)}-t_{k}^{(i)}\right) \mathbf{x}^{(i)}$ where $\bw_k$ means the $k$-th row of $\mathbf{W}$
\end{itemize}

\paragraph{Activation Functions} \textbf{Identity} $y = z$ \textbf{ReLU} $y = \max (0, z)$ \textbf{Soft ReLU} $y = \log(1+e^z)$ \textbf{Thresholding} $y = 1$ if $z> 0$ else $0$. \textbf{Logistic} $y=\frac{1}{1+e^{-z}}$ \textbf{tanh} $y=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$

\paragraph{Multilayer Perceptron} 
\begin{itemize}
    \setlength\itemsep{-0.45em}
    \item \textbf{Modularity of Layers} $\mathbf{h}^{(1)}=f^{(1)}(\mathbf{x})=\phi\left(\mathbf{W}^{(1)} \mathbf{x}+\mathbf{b}^{(1)}\right)$, $\mathbf{h}^{(2)}=f^{(2)}\left(\mathbf{h}^{(1)}\right)=\phi\left(\mathbf{W}^{(2)} \mathbf{h}^{(1)}+\mathbf{b}^{(2)}\right)$, \dots, $\mathbf{y}=f^{(L)}\left(\mathbf{h}^{(L-1)}\right) = f^{(L)} \circ \cdots \circ f^{(1)}(\mathbf{x})$
    \item \textbf{Choice of Last Layer Activation Func} Regression: $\mathbf{y}=f^{(L)}\left(\mathbf{h}^{(L-1)}\right)=\left(\mathbf{w}^{(L)}\right)^{T} \mathbf{h}^{(L-1)}+b^{(L)}$; Binary Classification: $\mathbf{y}=f^{(L)}\left(\mathbf{h}^{(L-1)}\right)=\sigma\left(\left(\mathbf{w}^{(L)}\right)^{T} \mathbf{h}^{(L-1)}+b^{(L)}\right)$
    \item \textbf{Back Propagation} Suppose $\mathcal{L}$ what I want to optimize, then for some variable $\bw$ that we want to optimize w.r.t., $\frac{\partial \mathcal{L}}{\partial \bw} =: \overbar{\bw}$
    \item \textbf{Back Prop Cost} Forward: one add-multiplicity operation per weight; Backward: two add-multiplicity operations per weight $\implies$ the Backward pass is about as expensive as two Forward passes. (cost is linear in \# of layers, quadratic in \# of units per layer)
\end{itemize}

% TODO: Support vector machine to be added here.


\paragraph{Statistic on Samples}
\begin{itemize}
	\setlength\itemsep{-0.45em}
	\item \textbf{Sample Mean} $\hat{\boldsymbol{\mu}}=\frac{1}{N} \sum_{i=1}^{N} \mathbf{x}^{(i)}$. $\hat{\boldsymbol{\mu}}$ roughly quantifies where your data is located in space
	\item \textbf{Sample Cov} $\hat{\mathbf{\Sigma}}=\frac{1}{N} \sum_{i=1}^{N}\left(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}}\right)\left(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}}\right)^{\top}$ quantifies the shape of spread of the data
\end{itemize}
\paragraph{Euclidean projection} Let $\mathcal{S}$ denote the subspace with dim = $k$ that is spanned by the basis $\left\{ \mathbf{u}_1,...,\mathbf{u}_K \right\} \subseteq \real^D $. Then,
\begin{itemize}
	\setlength\itemsep{-0.45em}
	\item Any vector $\mathbf{y} \in \mathcal{S}$ could be represented as $\mathbf{y} = \sum_{i=1}^K z_i\mathbf{u}_i$, for some $z_1,...,z_k\in \real$
	\item The projection of $\mathbf{x}$ onto $\mathcal{S}$ is given as $\operatorname{Proj}_{\mathcal{S}}(\mathbf{x})= \sum_{i=1}^{K} (\mathbf{x}^{\top} \mathbf{u}_{i}) \mathbf{u}_{i} =\sum_{i=1}^{K} z_{i} \mathbf{u}_{i} \quad \text { where } \quad z_{i}=\mathbf{x}^{\top} \mathbf{u}_{i}$
\end{itemize}

\paragraph{Principle Component Analysis - Projection onto Subspace} 
\begin{itemize}
	\setlength\itemsep{-0.45em}
	\item Let $\left\{\mathbf{u}_{k}\right\}_{k=1}^{K}$ be an \color{BurntOrange}orthonormal \color{Black} basis of the subspace $\mathcal{S}$.
	\item Define $\mathbf{U}$ to be a matrix with columns $\left\{\mathbf{u}_{k}\right\}_{k=1}^{K}$ then ${\mathbf{z}=\mathbf{U}^{T}(\mathbf{x}-\hat{\boldsymbol{\mu}})}$. Here the $\bz$ is called the code vector 
	\item Also, $\tilde{\mathbf{x}}=\hat{\boldsymbol{\mu}}+\mathbf{U} \mathbf{z}=\hat{\boldsymbol{\mu}}+\mathbf{U} \mathbf{U}^{T}(\mathbf{x}-\hat{\boldsymbol{\mu}})$ is called the reconstruction of $\mathbf{x}$
	\item Note: $\mathbf{U} \mathbf{U}^{T}$ is the projector matrix and $\mathbf{U}^{T} \mathbf{U}=I$ is the identity matrix. 
	\item $\bx$ and $\tilde{\bx}$ are both in $\real^D$ while $\tilde{\bx}$ lives in a low dimensional subspace in $\real^D$. The code vector $\bx$ is in $\real^K$, and is the low dim representation of the vector $\bx$
\end{itemize}

\paragraph{PCA - Learning Subspace}
\begin{itemize}
	\setlength\itemsep{-0.45em}
	\item \textbf{Criteria I:} Minimize the reconstruction error: Find vectors in a subspace that are closest to data points, $\min _{\mathbf{U}} \frac{1}{N} \sum_{i=1}^{N}\left\|\mathbf{x}^{(i)}-\tilde{\mathbf{x}}^{(i)}\right\|^{2}$
	\item \textbf{Criteria II:} Maximize the variance of reconstructions: Find subspaces where data has the most variability, $\max _{\mathbf{U}} \frac{1}{N} \sum_{i}\left\|\tilde{\mathbf{x}}^{(i)}-\hat{\boldsymbol{\mu}}\right\|^{2}$
	\item \textbf{Proof: Criteria I $\equiv$ Criteria II}; It suffices to show that $\frac{1}{N} \sum_{i=1}^{N}\left\|\mathbf{x}^{(i)}-\tilde{\mathbf{x}}^{(i)}\right\|^{2}=\text { const }-\frac{1}{N} \sum_{i}\left\|\tilde{\mathbf{x}}^{(i)}-\hat{\boldsymbol{\mu}}\right\|^{2}$
%	\begin{itemize}
%		\setlength\itemsep{-0.45em}
%		\item Using $\tilde{\mathbf{x}}^{(i)}=\hat{\boldsymbol{\mu}}+\mathbf{U} \mathbf{z}^{(i)} \text { and } \mathbf{z}^{(i)}=\mathbf{U}^{\top}\left(\mathbf{x}^{(i)}-\hat{\boldsymbol{\mu}}\right)$ and $\|\mathbf{v}\|^{2}=\mathbf{v}^{\top} \mathbf{v} \text { and } \mathbf{U}^{\top} \mathbf{U}=I \implies \|\mathbf{U} \mathbf{v}\|^{2}=\mathbf{v}^{\top} \mathbf{U}^{\top} \mathbf{U} \mathbf{v}=\|v\|^{2}$ we have
%			$$
%				\left\|\tilde{\mathbf{x}}^{(i)}-\hat{\boldsymbol{\mu}}\right\|^{2}=\left(\mathbf{U} \mathbf{z}^{(i)}\right)^{\top}\left(\mathbf{U} \mathbf{z}^{(i)}\right)=\left[\mathbf{z}^{(i)}\right]^{\top} \mathbf{U}^{\top} \mathbf{U} \mathbf{z}^{(i)}=\left[\mathbf{z}^{(i)}\right]^{\top} \mathbf{z}^{(i)}=\left\|\mathbf{z}^{(i)}\right\|^{2}
%			$$
%	\end{itemize}
\end{itemize}
\newpage

\paragraph{Work Flow of EM and GGM}
\begin{itemize}
    \item Step 0; Assume (1). Pick a cluster (with probability $\pi_k$) and (2). sample  from it using the parameters $\mu_k, \Sigma_k$. (Some gaussian distribution)
    \item Step 1; The log-likelihood is $\log P(X | \pi, \mu_k, \Sigma_k) = \sum_{i=1}^N \log \sum_{k=1}^K \pi_k N(X_n| \mu_k, \Sigma_k)$
    \item Step 2; Optimize via EM
    \item Step 2.1 Compute responsibilities (E)
    \item Step 2.2 Maximize (M)
    \begin{equation*}
        \mu_k = \frac{1}{N_k}\sum_k
    \end{equation*}
    \LARGE
    \item Notice that Generative models are called ``generative'' because they try to learn the underlying distribution, while the discriminative analysis procedures (eg NN) are trying to simply map data. 
\end{itemize}









\end{document}
