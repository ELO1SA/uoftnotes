\documentclass[10pt]{article}
\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{datetime}
\usepackage{ccicons}

% ==== License =====
\usepackage[
    type={CC}, 
    modifier={by-nc-sa}, 
    version={4.0},
]{doclicense}

% General
\newcommand{\mc}[1]{\mathcal{#1}}

% Math Bold Font, Vector Notations
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bzero}{\mathbf{0}}

% Proofs, Structures
\newcommand{\proof}{\tit{\underline{Proof:}}} % This equivalent to the \begin{proof}\end{proof} block
\newcommand{\proofforward}{\tit{\underline{Proof($\implies$):}}}
\newcommand{\proofback}{\tit{\underline{Proof($\impliedby$):}}}
\newcommand{\proofsuperset}{\tit{\underline{Proof($\supseteq$):}}}
\newcommand{\proofsubset}{\tit{\underline{Proof($\subseteq$):}}}
\newcommand{\contradiction}{$\longrightarrow\!\longleftarrow$}
\newcommand{\qed}{\hfill $\mathcal{Q}.\mathcal{E}.\mathcal{D}.\dagger$}

% Number Spaces, Vector Space
\newcommand{\R}{\mathbb{R}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\field}{\mathbb{F}}

% customized commands
\newcommand{\settag}[1]{\renewcommand{\theenumi}{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\tit}[1]{\textit{#1}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\double}[1]{\mathbb{#1}} % Set to behave like that on word
\newcommand{\trans}[3]{$#1:#2\rightarrow{}#3$}
\newcommand{\map}[3]{\text{$\left[#1\right]_{#2}^{#3}$}}
\newcommand{\dime}[1]{\text{dim}(#1)}
\newcommand{\mat}[2]{M_{#1 \times #2}(\R)}
\newcommand{\aug}{\fboxsep=-\fboxrule\!\!\!\fbox{\strut}\!\!\!}
\newcommand{\basecase}{\textsc{\underline{Basis Case:}} }
\newcommand{\inductive}{\textsc{\underline{Inductive Step:}} }
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\independent}{\perp \!\!\! \perp}


\title{\LARGE{Miscellaneous Notes on Regression}\\ \normalsize{Based on SJS and KNN}}
\author{\ccLogo \,\,by Xia, Tingfeng}
\date{Fall 2019, modified on \today}

% ===== Setting the page =====
\setlength{\headheight}{15pt}
\pagestyle{fancy}
\fancyhf{}
\rhead{by Xia, Tingfeng}
\lhead{Miscellaneous Notes on Regression; Fall 2019}
\lfoot{\url{https://tingfengx.github.io/uoftnotes/}}
\rfoot{Page \thepage}

\begin{document}
\maketitle
\section*{Preface}
Notes for STA302H1F fall offering, 2019 with Prof. Shivon Sue-Chee. These notes are based on the KNN and SJS text, in an aim for better understanding of the course material.
\doclicenseThis
\tableofcontents
\newpage

\section{Preliminaries}
\subsection{Distribution Theories}
\begin{itemize}
    \item Suppose $Y_1, Y_2 ,\dots, Y_n \overset{iid}{\sim} N(\mu, \sigma^2)$, and consider $s^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}$. Then,
        \begin{equation*}
            \frac{(n-1) s^{2}}{\sigma^{2}} \sim \chi^{2}_{(d f=n-1)}
        \end{equation*}
    \item Under the Normal Error SLR model, where $e_{i} \overset{iid}{\sim} N\left(0, \sigma^{2}\right)$, and $S^{2}=\frac{1}{n-2} \sum_{i=1}^{n} \hat{\mathrm{e}}_{i}^{2}=\frac{1}{n-2} \sum_{i=1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}$ (Different from above!). Then,
        \begin{equation*}
            \frac{(n-2) S^{2}}{\sigma^{2}}=\frac{(n-2) S^{2} / S X X}{\sigma^{2} / S X X}=\sum_{i=1}^{n}\left(\frac{Y_{i}-\hat{Y}_{i}}{\sigma}\right)^{2} \sim \chi^{2}_{(d f=n-2)}
        \end{equation*}
    \item Let $Z\sim N(0,1)$, and $V\sim \chi^2_{(df=v)}$. Assume further that $Z\independent V$, then
        \begin{equation*}
            \frac{Z}{\sqrt{V / v}} \sim t_{(df=v)}
        \end{equation*}
    \item Under the Normal Error SLR model,
        \begin{equation*}
            \frac{\hat{\beta}_{1}-\beta_{1}}{\sqrt{\frac{S^{2}}{S X X}}} \sim t_{(df = n-2)}
        \end{equation*}
    \item Suppose $V \sim \chi_{(df=v)}^{2}$, $W \sim \chi_{(df=w)}^2$ and $V\independent W$. Then,
        \begin{equation*}
            \frac{V / v}{W / w} \sim F_{(v, w)}
        \end{equation*}
    \item Suppose $Q\sim t_{(df=v)}$, then
        \begin{equation*}
            Q^2 \sim F_{(1,v)}
        \end{equation*}
\end{itemize}
\subsection{Matrix Calculus}
Manuel to matrix calculus provided by professor: \url{http://biostat.mc.vanderbilt.edu/wiki/pub/Main/CourseBios312/chap2.pdf}
\subsubsection{Lemma I (Real Valued Fcn Matrix Differentiation)} If $\boldsymbol{\theta}^{\prime}=\left(\theta_{1}, \theta_{2}, \ldots, \theta_{k}\right) \text { and } c^{\prime}=\left(c_{1}, c_{2}, \ldots, c_{k}\right)$ is a vector of constant, such that 
\begin{equation*}
    f(\boldsymbol{\theta})=\mathbf{c}^{\prime} \boldsymbol{\theta}=\boldsymbol{\theta}^{\prime} \mathbf{c}=\sum_{i} c_{i} \theta_{i}
\end{equation*}
is a scalar, then
\begin{equation*}
    \frac{\partial f(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\mathbf{c}
\end{equation*}
\subsubsection{Lemma II}
$\text { Let } \mathbf{A} \text { be a } k \times k \text { symmetric matrix. Suppose } f(\boldsymbol{\theta})=\boldsymbol{\theta}^{\prime} \mathbf{A} \boldsymbol{\theta}$. Then,
\begin{equation*}
    \frac{\partial f(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=2 \mathbf{A} \boldsymbol{\theta}
\end{equation*}
\subsubsection{Matrix Idempotency}
We say that a matrix $A$ is idempotent if and only if $A^2 = AA = A$. It is worth mentioning that projection matrices are idempotent, i.e. you can only project once and projecting the second time makes no change to the already projected result.
% FIXME: Add to here


\section{Simple Linear Regression}
\subsection{Ordinary Least Square}
\subsubsection{Simple Linear Regression Models}
The cost function to use in this case is the RSS, defined as
\begin{equation*}
    \mathrm{RSS}=\sum_{i=1}^{n} \hat{e}_{i}^{2}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-b_{0}-b_{1} x_{i}\right)^{2}
\end{equation*}
We will now derive the OLS estimators as follows. 
\paragraph{Derivatives}
\begin{equation*}
    \frac{\partial \operatorname{RSS}}{\partial b_{0}}=-2 \sum_{i=1}^{n}\left(y_{i}-b_{0}-b_{1} x_{i}\right)=0
\end{equation*}
\begin{equation*}
    \frac{\partial \operatorname{RSS}}{\partial b_{1}}=-2 \sum_{i=1}^{n} x_{i}\left(y_{i}-b_{0}-b_{1} x_{i}\right)=0
\end{equation*}
\paragraph{Normal Equations} are obtained by rearranging
\begin{equation}
    \sum_{i=1}^{n} y_{i}=b_{0} n+b_{1} \sum_{i=1}^{n} x_{i}
\end{equation}
\begin{equation}
    \sum_{i=1}^{n} x_{i} y_{i}=b_{0} \sum_{i=1}^{n} x_{i}+b_{1} \sum_{i=1}^{n} x_{i}^{2}
\end{equation}
\paragraph{OLS Regressor}
\begin{equation*}
    \hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1} \bar{x}
\end{equation*}
\begin{equation*}
    \hat{\beta}_{1}=\frac{\sum_{i=1}^{n} x_{i} y_{i}-n \overline{x y}}{\sum_{i=1}^{n} x_{i}^{2}-n \bar{x}^{2}}=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}=\frac{S X Y}{S X X}
\end{equation*}
\paragraph{Estimating Variance of Error Term (Using Residuals)}
\begin{equation*}
    \text{Unbiased Estimator} \quad \hat{\sigma}^2 = S^{2}=\frac{\mathrm{RSS}}{n-2}=\frac{1}{n-2} \sum_{i=1}^{n} \hat{e}_{i}^{2}
\end{equation*}
\paragraph{Notes}
\begin{itemize}
    \item $\bar{\hat{e}}=0$, since $\sum \hat{e}_i = 0$ as the least square estimates minimizes RSS. (This is like a minimization goal where derivatives are taken w.r.t $\hat{e}_i$.)
    \item $S^2$ has $n-2$ degrees of freedom since we have estimated two parameters, namely $\beta_0$ and $\beta_1$.
\end{itemize}

\subsection{Inferences on Slope and Intercept}
\subsubsection{Inference Assumptions}
The following assumptions need to be made in order to preform inference
\begin{itemize}
    \item $Y$ is explained by $x$ through a simple linear regression model
        \begin{equation*}
            Y_{i}=\beta_{0}+\beta_{1} x_{i}+e_{i}(i=1, \ldots, n), \text { i.e., } \mathrm{E}\left(Y | X=x_{i}\right)=\beta_{0}+\beta_{1} x_{i}
        \end{equation*}
    \item Independent Errors, $e_i \independent e_j, \forall i \neq j$
    \item Homoscedasticity, $\text{Var}(e_i) = \sigma^2, \forall i$
    \item Normal Error: $e | X \sim N\left(0, \sigma^{2}\right)$
\end{itemize}

\subsubsection{Inference of Slope}
\paragraph{Distribution}
\begin{equation*}
    \hat{\beta}_{1} | X \sim N\left(\beta_{1}, \frac{\sigma^{2}}{S X X}\right)
\end{equation*}
\paragraph{Standardized Test Statistic (Var Known)}
\begin{equation*}
    Z=\frac{\hat{\beta}_{1}-\beta_{1}}{\sigma / \sqrt{S X X}} \sim N(0,1)
\end{equation*}
\paragraph{Test Statistic (Var Unknown)}
Recall that degrees of freedom = sample size - number of mean parameters estimated. Then,
\begin{equation*}
    T=\frac{\hat{\beta}_{1}-\beta_{1}}{S / \sqrt{S X X}}=\frac{\hat{\beta}_{1}-\beta_{1}}{\operatorname{SE}\left(\hat{\beta}_{1}\right)} \sim t_{(df=n-2)} \quad \text{where}~~ S^2 = \frac{\sum_i \hat{e}_i^2}{n-2} 
\end{equation*}
\paragraph{Confidence Interval (Var Unknown)}
The $100(1- \alpha)\%$ CI is
\begin{equation*}
    CI \gets \hat{\beta}_1 \pm t^\ast_{(\alpha/2, df=n-2)} \times SE(\hat{\beta}_1) \equiv \hat{\beta}_1 \pm t^\ast_{(\alpha/2, df=n-2)} \frac{S}{\sqrt{S X X}}
\end{equation*}
\paragraph{Distribution Proof}
Recall OLS regressor for $\beta_1$ is
\begin{equation*}
    \hat{beta}_1 = \sum_{i = 1}^n c_iy_i \quad \text{where}~~ c_i = \frac{x_i \overbar{x}}{SXX}
\end{equation*}
The expectation could be derived as\footnote{using the fact that $\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)=0$ and $\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right) x_{i}=\sum_{i=1}^{n} x_{i}^{2}-n \bar{x}^{2}=S X X$}
\begin{align*}
    \mathrm{E}\left(\hat{\beta}_{1} | X\right) &=\mathrm{E}\left[\sum_{i=1}^{n} c_{i} y_{i} | X=x_{i}\right] \\ &=\sum_{i=1}^{n} c_{i} E\left[y_{i} | X=x_{i}\right] \\ &=\sum_{i=1}^{n} c_{i}\left(\beta_{0}+\beta_{1} x_{i}\right) \\ &=\beta_{0} \sum_{i=1}^{n} c_{i}+\beta_{1} \sum_{i=1}^{n} c_{i} x_{i} \\ &=\beta_{0} \sum_{i=1}^{n}\left\{\frac{x_{i}-\bar{x}}{S X X}\right\}+\beta_{1} \sum_{i=1}^{n}\left\{\frac{x_{i}-\bar{x}}{S X X}\right\} x_{i} \\ &=\beta_{1} 
\end{align*}
and the variance
\begin{align*}
    \operatorname{Var}\left(\hat{\beta}_{1} | X\right) &=\operatorname{Var}\left[\sum_{i=1}^{n} c_{i} y_{i} | X=x_{i}\right] \\ &=\sum_{i=1}^{n} c_{i}^{2} \operatorname{Var}\left(y_{i} | X=x_{i}\right) \\ &=\sigma^{2} \sum_{i=1}^{n} c_{i}^{2} \\ &=\sigma^{2} \sum_{i=1}^{n}\left\{\frac{x_{i}-\bar{x}}{S X X}\right\}^{2} \\ &=\frac{\sigma^{2}}{S X X}
\end{align*}
Then, since $e_i |X$ are normally distributed, then $y_i = \beta_{0}+\beta_{1} x_{i}+e_{i}, Y_{i} | X$ is normally distributed. Since $\hat{\beta}_{1} | X$ is a linear combination of $y_i$'s , $\hat{\beta}_1 | X$ is normally distributed. \qed


\paragraph{\color{Thistle} \textit{t}-test using PMCC on Bi-variate Normal \color{Black}}

Recall that 
\begin{equation*}
    \hat{\rho}_{MLE} = r=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2} \sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}}} = \frac{SXY}{\sqrt{SXX~SYY}}
\end{equation*}
Use the null hypothesis that $H_0: \rho_{XY} = 0$ and alternative $H_1: \rho_{XY} \neq 0$, then
\begin{equation*}
    T=\frac{r \sqrt{n-2}}{\sqrt{1-r^{2}}}=\frac{b_{1}}{\sqrt{\frac{S^{2}}{S X X}}} \sim t_{(df = n-2)}
\end{equation*}


\subsubsection{Inference of Intercept}
\paragraph{Distribution}
\begin{equation*}
    \hat{\beta}_{0} | X \sim N\left(\beta_{0}, \sigma^{2}\left(\frac{1}{n}+\frac{\bar{x}^{2}}{S X X}\right)\right)
\end{equation*}
\paragraph{Standardized Test Statistic (Var Known)}
\begin{equation*}
    Z=\frac{\hat{\beta}_{0}-\beta_{0}}{\sigma \sqrt{\frac{1}{n}+\frac{\bar{x}^{2}}{S X X}}} \sim N(0,1)
\end{equation*}
\paragraph{Test Statistic (Var Unknown)}
\begin{equation*}
    Z=\frac{\hat{\beta}_{0}-\beta_{0}}{S \sqrt{\frac{1}{n}+\frac{\bar{x}^{2}}{S X X}}} = \frac{\hat{\beta}_{0}-\beta_{0}}{\operatorname{SE}\left(\hat{\beta}_{0}\right)} \sim t_{(df=n-2)} \quad \text{where}~~ S^2 = \frac{\sum_i \hat{e}_i^2}{n-2} 
\end{equation*}
\paragraph{Confidence Interval (Var Unknown)}
\begin{equation*}
    CI \gets \hat{\beta}_0 \pm t^\ast_{()\alpha/2, df=n-2)} \times SE(\hat{\beta}_0) \equiv \hat{\beta}_0 \pm t^\ast_{(\alpha/2, df=n-2)} S\sqrt{\frac{1}{n}+\frac{\bar{x}^{2}}{S X X}}
\end{equation*}
\paragraph{Distribution Proof}
Recall that the OLS regressor of $\beta_0$ is given by
\begin{equation*}
    \hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1} \bar{x}
\end{equation*}
The expectation,
\begin{align*}
    \mathrm{E}\left(\hat{\beta}_{0} | X\right)
    &= \mathrm{E}(\bar{y} | X)-\mathrm{E}\left(\hat{\beta}_{1} | X\right) \bar{x} \\
    &= \frac{1}{n} \sum_{i=1}^{n} \mathrm{E}\left(y_{i} | X=x_{i}\right) - \beta_{1} \bar{x} \\
    &= \frac{1}{n} \sum_{i=1}^{n} \mathrm{E}\left(\beta_{0}+\beta_{1} x_{i}+e_{i}\right) - \beta_{1} \bar{x} \\
    &= \beta_{0}+\beta_{1} \frac{1}{n} \sum_{i=1}^{n} x_{i} - \beta_{1} \bar{x} \\
    &= \beta_{0}+\beta_{1} \bar{x} - \beta_{1} \bar{x} = \beta_0
\end{align*}
and the variance,
\begin{align*}
    \operatorname{Var}\left(\hat{\beta}_{0} | X\right) &=\operatorname{Var}\left(\bar{y}-\hat{\beta}_{1} \bar{x} | X\right) \\ &=\operatorname{Var}(\bar{y} | X)+\bar{x}^{2} \operatorname{Var}\left(\hat{\beta}_{1} | X\right)-2 \bar{x} \operatorname{Cov}\left(\bar{y}, \hat{\beta}_{1} | X\right)
\end{align*}
where
\begin{equation*}
    \operatorname{Var}(\bar{y} | X)=\operatorname{Var}\left(\frac{1}{n} \sum_{i=1}^{n} y_{i} | X=x_{i}\right)=\frac{n \sigma^{2}}{n^{2}}=\frac{\sigma^{2}}{n}
\end{equation*}
\begin{equation*}
    \operatorname{Var}\left(\hat{\beta}_{1} | X\right)=\frac{\sigma^{2}}{S X X}
\end{equation*}
\begin{equation*}
    \operatorname{Cov}\left(\bar{y}, \hat{\beta}_{1} | X\right)=\operatorname{Cov}\left(\frac{1}{n} \sum_{i=1}^{n} y_{i}, \sum_{i=1}^{n} c_{i} y_{i}\right)=\frac{1}{n} \sum_{i=1}^{n} c_{i} \operatorname{Cov}\left(y_{i}, y_{i}\right)=\frac{\sigma^{2}}{n} \sum_{i=1}^{n} c_{i}=0
\end{equation*}
Thus,
\begin{equation*}
    \operatorname{Var}\left(\hat{\beta}_{0} | X\right)=\sigma^{2}\left(\frac{1}{n}+\frac{\bar{x}^{2}}{S X X}\right)
\end{equation*}
\qed

\subsection{CI for \textit{Unknown} Population Regression Line}
Goal: Find a confidence interval for a unkown population regression line at $X = x^\ast$. The population regression line is given by
\begin{equation*}
    \mathrm{E}\left(Y | X=x^{*}\right)=\beta_{0}+\beta_{1} x^{*}
\end{equation*}
\paragraph{Distribution}
To get an estimate of the y value at $X = x^*$, we can use the regression output (evaluate the estimated regression line at $X = x^*$)
\begin{equation*}
    \hat{y}^{*}=\hat{\beta}_{0}+\hat{\beta}_{1} x^{*}
\end{equation*}
where we claim it follows the distribution
\begin{equation*}
    \hat{y}^{*}=\hat{y} | X=x^{*} \sim N\left(\beta_{0}+\beta_{1} x^{*}, \sigma^{2}\left(\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)\right)
\end{equation*}
\paragraph{Proof of Distribution}
The expectation follows directly from definition, and we will now show that the variance has the claimed value. Notice that $\text{Var}(\hat{\beta}_0|X) = \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{SXX}\right)$, $\text{Var}(\hat{\beta}_1|X) = \frac{\sigma^2}{SXX}$ and $\text{Cov}(\hat{\beta}_0, \hat{\beta}_1 | X) = \frac{-\bar{x}\sigma^2}{SXX}$, then
\begin{align*}
    \text{Var}(\hat{y}|X = x^*)
    &= \text{Var}( \hat{\beta}_0 + \hat{\beta}_1 x^* |X = x^* ) \\
    &= \text{Var}(\hat{\beta}_0 | X = x^*) + \text{Var}(\hat{\beta}_1 x^* |X = x^*) + 2\text{Cov}(\hat{\beta}_1, \hat{\beta}_1 x^* | X = x^*) \\
    &= \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{SXX}\right) + (x^*)^2 \frac{\sigma^2}{SXX} + 2x^* \left(\frac{-\bar{x}\sigma^2}{SXX}\right) \\
    &= \sigma^{2}\left(\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)
\end{align*}
\hfill \qed
\paragraph{Standardized Test Statistic (Var Known)}
\begin{equation*}
    Z=\frac{\hat{y}^{*}-\left(\beta_{0}+\beta_{1} x^{*}\right)}{\sigma \sqrt{\left(\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)}} \sim N(0,1)
\end{equation*}

\paragraph{Test Statistic (Var Known)}
\begin{equation*}
    T=\frac{\hat{y}^*-\left(\beta_{0}+\beta_{1} x^{*}\right)}{S \sqrt{\left(\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)}} \sim t_{(df=n-2)} \quad \text{where}~~S^2 = \frac{\sum_i \hat{e}_i^2}{n-2} 
\end{equation*}

\paragraph{Confidence Interval} A $100 (1 - \alpha) \%$ CI for $\mathrm{E}\left(Y | X=x^{*}\right)=\beta_{0}+\beta_{1} x^{*}$ is given by
\begin{align*}
    CI &\gets \hat{y}^{*} \pm t^\ast_{(\alpha / 2, df = n-2)} S \sqrt{\left(\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)} \\
    &\equiv \hat{\beta}_{0}+\hat{\beta}_{1} x^{*} \pm t^\ast_{(\alpha / 2, df =n-2)} S \sqrt{\left(\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)}
\end{align*}
\color{Thistle} 
Do note that this is only valid for $x^\ast$ values in the range of the original data values of $X$. Avoid extrapolation.
\color{Black}

\subsection{Prediction Intervals for Actual Value of Y}
\paragraph{Goal} Find a prediction interval for the actual value of Y at $x^*$, a given value of $X$.

\paragraph{Important Notes}
\begin{itemize}
    \item $E(Y | X = x^*)$, the expected value or average value of $Y$ for a given value $x^*$ of $X$, is what one would expect $Y$ to be in the long run when $X = x^*$. $E(Y | X = x^*)$ is therefore a fixed but unknown quantity whereas $Y$ can take a number of values when $X = x^*$.
    \item $E(Y | X = x^*)$, the value of the regression line at $X = x^*$, is entirely different from $Y^*$, a single value of $Y$ when $X = x^*$. In particular, $Y^*$ need not lie on the population regression line.
    \item A confidence interval is always reported for a parameter (e.g., $E(Y | X = x^*) = b_0 + b_1x^*$) and a prediction interval is reported for the value of a random variable (e.g., $Y^*$).
\end{itemize}

\paragraph{Difference Between CI and PI (My Thoughts)} The intrinsic difference is that: The CI we found above for $E(Y | X = x^*)$ is a CI for a fixed value. We are trying to find, in the long run where can we expect the regression line to lie given infinite samples. However, PI is trying to report for a specific value, possibly a not-already-observed new value, what is the range that it may appear in. PI captures more variability. 

\paragraph{Distribution}
\begin{equation*}
    Y^{*}-\hat{y}^{*} \sim N\left(0, \sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right]\right)
\end{equation*}

\paragraph{Test Statistic (Var Unknown)}
\begin{equation*}
    T=\frac{Y^*-\hat{y}^*}{S \sqrt{\left(1+\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)}} \sim t_{(df = n-2)}
\end{equation*}

\paragraph{Derivation of Distribution}
We base our prediction of $Y$ at $X = x^*$ (which is $Y^*$) on
\begin{equation*}
    \hat{y}^{*}=\hat{\beta}_{0}+\hat{\beta}_{1} x^{*}
\end{equation*}
The error (deviation, to be precise) of our prediction is 
\begin{equation*}
    Y^{*}-\hat{y}^{*}=\beta_{0}+\beta_{1} x^{*}+e^{*}-\hat{y}^{*}= \left(\mathrm{E}\left(Y | X=x^{*}\right)- \hat{y}^{*} \right) + e^{*}
\end{equation*}
that is, the deviation between $E(Y | X = x^*)$ and $y^*$ plus the random fluctuation $e^*$ (which represents the deviation of $Y^*$ from $E(Y|X = x^*)$). Thus the variability in the error for predicting a single value of $Y$ will exceed the variability for estimating the expected value of Y (because of the random error $e^*$). We have
\begin{equation*}
    \mathrm{E}\left(Y^{*}-\hat{y}^{*}\right)=\mathrm{E}\left(Y-\hat{y} | X=x^{*}\right)=0
\end{equation*}
and
\begin{equation*}
    \operatorname{Var}\left(Y^{*}-\hat{y}^{*}\right)=\operatorname{Var}\left(Y-\hat{y} | X=x^{*}\right)=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right]
\end{equation*}

\paragraph{Prediction Interval}
A $100 (1 - \alpha)\%$ prediction interval for $Y^*$ (the value of $Y$  at $X = x^*$ is given by)
\begin{align*}
    PI &\gets \hat{y}^{*} \pm t_{(\alpha / 2, df = n-2)} S \sqrt{\left(1+\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)} \\
    &\equiv \hat{\beta}_{0}+\hat{\beta}_{1} x^{*} \pm t_{(\alpha / 2, df = n-2)} S \sqrt{\left(1+\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{S X X}\right)}
\end{align*}

\subsection{Analysis of Variance (ANOVA)}
\subsubsection{Sum of Squares Decomposition}
Define \color{Thistle} Total Sample Variability \color{Black}
\begin{equation*}
    \mathrm{SST}=S Y Y=\sum_{i}^{n}\left(y_{i}-\bar{y}\right)^{2}
\end{equation*}
and recall the familiar residual squared sum \color{Thistle} (Unexplained (or error) variability) \color{Black}
\begin{equation*}
    \mathrm{RSS}=\sum_{i}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}
\end{equation*}
and we define \color{Thistle}(Sum of Squares explained by the regression model) \color{Black}
\begin{equation*}
    \text { SSreg }=\sum_{i}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}
\end{equation*}
Then, the decomposition is
\begin{equation*}
    \text{SST} = \mathrm{RSS} + \text {SSreg}
\end{equation*}

\subsubsection{Test for Zero Slope}
\paragraph{\textit{t}-test} Note that for SLR, this is equivalent to the F-test outlined below. Consider the null $H_0: \beta_1 = 0$ against alternative $H_1: \beta_1 \neq 0$, then
\begin{equation*}
    T=\frac{\hat{\beta}_{1}-0}{\operatorname{se}\left(\hat{\beta}_{1}\right)} = \frac{\hat{\beta}_{1}- 0}{S / \sqrt{S X X}} \overset{H_0}{\sim} t_{(df = n-2)} 
\end{equation*}
This is just a specific application of the general $t$-test that we mentioned earlier, not very interesting. 

\paragraph{\textit{F}-test}
Assume that $e_i\independent e_j, \forall i\neq j \land e_i\sim N(0, \sigma^2), \forall i$. Consider the null $H_0: \beta_1 = 0$ against alternative $H_1: \beta_1 \neq 0$, then
\begin{equation*}
    F=\frac{\operatorname{SSreg} / 1}{\operatorname{RSS} /(n-2)} \overset{H_0}{\sim} F_{1, n-2}
\end{equation*}

\subsubsection{Coefficient of Determination}
The Coefficient of Determination ($R^2$) of a regression line is defined as the proportion of the total sample variability in the $Y$'s explained by the regression model, that is
\begin{equation*}
    R^{2}=\frac{\text { SSreg }}{\mathrm{SST}}=1-\frac{\mathrm{RSS}}{\mathrm{SST}}
\end{equation*}

\subsubsection{The ANOVA Table}
The above $F$-test, as well as the sum of squares decomposition, could be summarized using the following handy table. 
\begin{center}
    \begin{tabular}{|c||c|c|c|c|}
        \hline
        Source of Variation & $df$    & SS    & MS = SS/$df$& $F$                                      \\ \hline \hline
        Regression          & 1     & SSreg & SSreg/1     & $F = \frac{SSreg/1}{\text{RSS}/(n-2)}$ \\ \hline
        Residual            & $n-2$ & RSS   & RSS/$(n-2)$ &                                        \\ \hline
        Total               & $n-1$ & SST   &             &                                        \\ \hline
    \end{tabular}
\end{center}


\section{Diagnostics and Transformations for SLR}
\subsection{Valid and Invalid Data}
\subsubsection{Residuals}
One tool that we can use to validate a regression model is one or more plots of residuals (or standardized residuals). These plots will enable us to assess visually whether an appropriate model has been fit to the data no matter how many predictor variables are used. 

\paragraph{Expected Behaviour} We expect that the residual graph to have no discern-able pattern and centred at some value (0 in the case of standardized residual). Patterns such as curves, skewness et cetra indicates non-normal residuals. More on this in the below section.

\subsubsection{Reading Residual Plots}
\paragraph{Criterion} One way of checking whether a valid simple linear regression model has been fit is to plot residuals versus $x$ and look for patterns. If no pattern is found then this indicates that the model provides an adequate summary of the data, i.e., is a valid model. If a pattern is found then the shape of the pattern provides information on the function of $x$ that is missing from the model.  

\paragraph{Rationale} Suppose that the true model is a straight line (which we never know) defined as
\begin{equation}
    Y_{i}=\mathrm{E}\left(Y_{\mathrm{i}} | X_{i}=x_{i}\right)+e_{i}=\beta_{0}+\beta_{1} x_{i}+e_{i} \quad 
\end{equation}
where
\begin{equation*}
    e_i = \text{Random error on}~Y_i\quad \text{and} ~E(e_i) = 0
\end{equation*}
and we fit a regression line
\begin{equation*}
    \hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{i}
\end{equation*}
Under the assumption that our regression line is very close to the true model, i.e. $\beta_0 \approx b_0$ and $\beta_1 \approx b_1$), we see
\begin{align*}
    \hat{e}_{i} 
    &= y_{i}-\hat{y}_{i} \\
    &= \beta_{0}+\beta_{1} x_{i} + e_i - \hat{\beta}_{0} - \hat{\beta}_{1} x_{i} \\
    &= \left(\beta_{0}-\hat{\beta}_{0}\right)+\left(\beta_{1}-\hat{\beta}_{1}\right) x_{i}+e_{i} \\
    &\approx e_i
\end{align*}
which means that our residuals resembles the random error! 

\subsection{Regression Diagnostics}
\paragraph{Categorization}
\begin{itemize}
    \item \textbf{X-Direction Outlier, i.e. Leverage Point}: Away from the bulk of data in the $x$-direction. 
    \begin{itemize}
        \item \textbf{Good:} Not much change after removing the data point, i.e. the data point originally was quite close to the regression line although away from the bulk of data in the $x$ direction. \textit{``A good leverage point is a leverage point which is NOT also an outlier.''}
        \item \textbf{Bad, Influential Point:} If its $Y$-value does not follow the pattern set by the other data points, i.e. a bad leverage point is a leverage point which is also an outlier. 
    \end{itemize}
    \item \textbf{Y-Direction Outlier} Trait: large residuals
\end{itemize}

\subsubsection{Leverage Point}
\paragraph{Defining The Hat}
The hat came from yet another representation of the $\hat{y}_i$. Recall that $\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{i}$, where $\hat{\beta}_{0}=\bar{y}-\hat{\beta}_{1} \bar{x}$, $\hat{\beta}_{1}=\sum_{j=1}^{n} c_{j} y_{j}$ and $c_{j}=\frac{x_{j}-\bar{x}}{S X X}$.
Then we have
\begin{align*}
    \hat{y}_{i}	
    &=\bar{y}-\hat{\beta}_{1} \bar{x}+\hat{\beta}_{1} x_{i} 
    =\bar{y}+\hat{\beta}_{1}\left(x_{i}-\bar{x}\right) \\
    &=\frac{1}{n} \sum_{j=1}^{n} y_{j}+\sum_{j=1}^{n} \frac{\left(x_{j}-\bar{x}\right)}{S X X} y_{j}\left(x_{i}-\bar{x}\right) \\
    &=\sum_{j=1}^{n}\left[\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{S X X}\right] y_{j} 
    =\sum_{j=1}^{n} h_{i j} y_{j}
\end{align*}
where we define
\begin{equation*}
    h_{i j}=\left[\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{S X X}\right]
\end{equation*}
\paragraph{Property of The Hat}
Recall that $\sum_{j=1}^{n}\left[x_{j}-\bar{x}\right]=n\bar{x} - n\bar{x}=0$, then
\begin{equation*}
    \sum_{j=1}^{n} h_{i j}=\sum_{j=1}^{n}\left[\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{S X X}\right]=\frac{n}{n}+\frac{\left(x_{i}-\bar{x}\right)}{S X X} \sum_{j=1}^{n}\left[x_{j}-\bar{x}\right]=1
\end{equation*}
Thus,
\begin{equation*}
    \hat{y}_{i}=h_{i i} y_{i}+\sum_{j \neq i} h_{i j} y_{j} \quad \text{where}~h_{i i}=\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)^{2}}{\sum_{j=1}^{n}\left(x_{j}-\bar{x}\right)^{2}}
\end{equation*}
\paragraph{Defining Leverage} 
The term $h_{i i}=\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)^{2}}{\sum_{j=1}^{n}\left(x_{j}-\bar{x}\right)^{2}}$ above is commonly known as the leverage of the $i$th data point. Notice the following in the definition of the leverage $h_{ii}$
\begin{itemize}
    \item The second term measures the proportion, in terms of squared deviation in $x$-direction over sum of square of total deviation in $x$-direction, of the $i$-th data point's deviation. When the second term tends to 1, meaning that $i$-th data point is some extreme outlier in the $x$-direction, then $h_{ii}$ would close to one, signifying the `leverage'-ness. 
    \item Recall that $\sum_{j=1}^{n} h_{i j}=1$, then when $h_{i i} \cong 1$, $h_{ij}\rightarrow 0$ and
        \begin{equation*}
            \hat{y}_{i}=1 \times y_{i}+\text { other terms } \cong y_{i}
        \end{equation*}
    which means $\hat{y}_{i}$  will be very close to $y_i$, regardless of the rest dataset. 
    \item A point of high leverage (or a leverage point) can be found by looking at just the values of the $x$’s and not at the values of the $y$’s
\end{itemize}

\paragraph{Average of Leverage} For simple linear regression, 
\begin{equation*}
    \operatorname{average}\left(h_{i i}\right) = \frac{1}{n}\sum_{i = 1}^n h_{ii} =\frac{2}{n}
\end{equation*}

\paragraph{Identifying Leverage} Rule: $x_i$ is a high leverage (i.e., a leverage point) in a SLR model if 
\begin{equation*}
    h_{i i}>2 \times \operatorname{average}\left(h_{i i}\right)=2 \times 2 / n=4 / n
\end{equation*}

\paragraph{Dealing with `Bad' Leverage}
\begin{itemize}
    \item \textbf{Remove invalid data points;} Question the validity of the data points corresponding to bad leverage points, that is: Are these data points unusual or different in some way from the rest of the data? If so, consider removing these points and refitting the model without them. 
    \item \textbf{Fit a different regression model;} Question the validity of the regression model that has been fitted, that is: Has an incorrect model been fitted to the data? If so, consider trying a different model by including extra predictor variables (e.g., polynomial terms) or by transforming Y and/or x (which is considered later in this chapter).
\end{itemize}

\subsubsection{Standardized Residuals}
\paragraph{Problem of Non-constant Variance} Recall that 
\begin{equation*}
    h_{i j}=\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{\sum_{j=1}^{n}\left(x_{j}-\bar{x}\right)^{2}}=\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{S X X}
\end{equation*}
and (we will show this later)
\begin{equation*}
    \operatorname{Var}\left(\hat{e}_{i}\right)=\sigma^{2}\left[1-h_{i i}\right]
\end{equation*}
which is indeed non-constant for different data points. When  $h_{ii} \cong 1$ ($h_{ii}$ is very close to 1), the $i$-th data point is a leverage point and 
\begin{equation*}
    \operatorname{Var}(\hat{e}_i) = \sigma^{2}\left[1-h_{i i}\right] \approx 0 \quad \text{and} \quad \hat{y}_{i} \cong y_{i}
\end{equation*}
The above results intuitively makes sense: When $i$-th data point is a leverage, $\hat{e}_i$ will be small and it does not vary much (data point close to the estimated regression line). 

\paragraph{Derivation of Residual Variance (Not Important)}
Recall that
\begin{equation*}
    \hat{y}_{i}=h_{i i} y_{i}+\sum_{j \neq i} h_{i j} y_{j} \quad \text{where}~h_{i j}=\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{\sum_{j=1}^{n}\left(x_{j}-\bar{x}\right)^{2}}=\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{S X X}
\end{equation*}
Then,
\begin{equation*}
    \hat{e}_{i}=y_{i}-\hat{y}_{i}=y_{i}-h_{i i} y_{i}-\sum_{j \neq i} h_{i j} y_{j}=\left(1-h_{i i}\right) y_{i}-\sum_{j \neq i} h_{i j} y_{j}
\end{equation*}
Hence,
\begin{align*}
    \operatorname{Var}\left(\hat{e}_{i}\right) &= \operatorname{Var}\left(\left(1-h_{i i}\right) y_{i}-\sum_{j \neq i} h_{i j} y_{j}\right) \\
    &= \left(1-h_{i i}\right)^{2} \sigma^{2}+\sum_{j \neq i} h_{i j}^{2} \sigma^{2} \\
    &= \sigma^{2}\left[1-2 h_{i i}+h_{i i}^{2}+\sum_{j \neq i} h_{i j}^{2}\right] \\
    &=\sigma^{2}\left[1-2 h_{i i}+\sum_{j} h_{i j}^{2}\right]
\end{align*}
Notice that 
\begin{align*} 
    \sum_{j=1}^{n} h_{i j}^{2} &=\sum_{j=1}^{n}\left[\frac{1}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{S X X}\right]^{2} \\ &=\frac{1}{n}+2 \sum_{j=1}^{n} \frac{1}{n} \times \frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right)}{S X X}+\sum_{j=1}^{n} \frac{\left(x_{i}-\bar{x}\right)^{2}\left(x_{j}-\bar{x}\right)^{2}}{S X X^{2}} \\ &=\frac{1}{n}+0+\frac{\left(x_{i}-\bar{x}\right)^{2}}{S X X} \\ &=h_{i i} 
\end{align*}
So,
\begin{equation*}
    \operatorname{Var}\left(\hat{e}_{i}\right)=\sigma^{2}\left[1-2 h_{i i}+h_{i i}\right]=\sigma^{2}\left[1-h_{i i}\right]
\end{equation*}
and
\begin{equation*}
    \color{BurntOrange}{\operatorname{Var}\left(\hat{y}_{i}\right)} \color{Black}=\operatorname{Var}\left(\sum_{j=1}^{n} h_{i j} y_{j}\right)=\sum_{j \neq i} h_{i j}^{2} \operatorname{Var}\left(y_{j}\right)=\sigma^{2} \sum_{j} h_{i j}^{2}=\color{Thistle}{\sigma^{2} h_{i i}}
\end{equation*}

\paragraph{Overcome with Standardization} The above problem of each $\hat{e}_i$ having different variances could be overcame by standardizing the residuals. The $i$-th standardized residual is defined as (notice that the $s = \hat{\sigma}$ is the estimated variance in the SLR settings)
\begin{equation*}
    r_{i}=\frac{\hat{e}_{i}}{s \sqrt{1-h_{i i}}}\quad \text{where}~ s=\sqrt{\frac{1}{n-2} \sum_{j=1}^{n} \hat{e}_{j}^{2}}
\end{equation*}

\paragraph{Advantages of Standardization} 
\begin{itemize}
    \item When points of high leverage exist, instead of looking at residual plots, it is generally more informative to look at plots of standardized residuals since plots of the residuals will have non-constant variance even if the errors have constant variance.
    \item When points of high leverage do not exist, there is generally little difference in the patterns seen in plots of residuals when compared with those in plots of standardized residuals.
    \item The other advantage of standardized residuals is that they immediately tell us how many estimated standard deviations any point is away from the fitted regression model. 
\end{itemize}

\color{RawSienna}
\paragraph{$\star~$Recognizing Outliers Using Standardized Residuals} 
\begin{itemize}
    \item An \textbf{outlier} is a point whose standardized residual falls outside the interval from -2 to 2, i.e. $|r_i| > 2$
    \item A \textbf{Bad Leverage Point} is a leverage point whose standardized residual falls outside the interval from -2 to 2, i.e. $|r_i| > 2 \wedge h_{ii} > \frac{4}{n}$
    \item A \textbf{Good Leverage Point} is a leverage point whose standardized residual falls inside the interval from -2 to 2, i.e. $|r_i| \leq 2 \wedge h_{ii} > \frac{4}{n}$
    \item \textbf{Dealing with large datasets:} In this case, we should change the above criterion to $|r_i| > 4$ and $|r_i| \leq 4$ respectively. This is to give allowance for more occurrence of rare events in a large data set.
\end{itemize}
\color{Black}

\paragraph{Correlation Between Residuals} Even if the errors are independent (homogeneous), i.e. $e_i \independent e_j~(i \neq j)$, the residuals are still correlated. It can be shown that the covariance and the correlation is given by
\begin{equation*}
    \operatorname{Cov}\left(\hat{e}_{i}, \hat{e}_{j}\right)=-h_{i j} \sigma^{2}(i \neq j)
\end{equation*}
\begin{equation*}
    \operatorname{Corr}\left(\hat{e}_{i}, \hat{e}_{j}\right)=\frac{-h_{i j}}{\sqrt{\left(1-h_{i i}\right)\left(1-h_{i j}\right)}}(i \neq j)
\end{equation*}
Such correlation could be safely ignored in practice. They are usually given raise by inherent correlation such as data collected over time.

\paragraph{Variance of Residuals} Above we discussed `inter-correlation' of residuals. \color{BurntOrange} The variance of a single residual is
\begin{equation*}
    \operatorname{Var}\left(\hat{e}_{i}\right)=\left(1-h_{i i}\right) \sigma^{2}
\end{equation*}
\color{Black}

\subsubsection{Recommendations for Handling Outliers \& Leverage}
We have discussed multiple ways of assessing outliers and talked about the way to deal with them by removing them. However, it is not always a good idea to delete them for the following reasons:
\begin{itemize}
    \item Points should not be routinely deleted from an analysis just because they do not fit the model. Outliers and bad leverage points are signals, flagging potential problems with the model.
    \item Outliers often point out an important feature of the problem not considered before. They may point to an alternative model in which the points are not an outlier. In this case it is then worth considering fitting an alternative model. 
\end{itemize}

\subsubsection{Influence of Certain Cases}
It can sometimes be the case that certain data points in a data set are drastically controlling the entire regression model (the model has payed too much attention to them). We now develop methods where we measure the ``importance'' of a data point.

\paragraph{Cook's Distance} First, define (recall if already defined) the following notation
\begin{itemize}
    \item $\hat{{y}}_{j(i)}$ means the the fitted value of the $j$-th data point on the regression line obtained by removing the $i$-th case. 
    \item $S^2 = \frac{1}{n-2} \sum_{j=1}^n \hat{e}_j^2$ is the variance (Original MSE) of the \color{Thistle}{total regression model}. \color{black}
    \item $r_{i}=\frac{\hat{e}_{i}}{s \sqrt{1-h_{i i}}}\quad \text{where}~ s=\sqrt{\frac{1}{n-2} \sum_{j=1}^{n} \hat{e}_{j}^{2}}$
\end{itemize}
Then, the Cook's Distance of the $i$-th data point is given by
\begin{equation*}
    \color{OliveGreen}
    D_{i}=\frac{\sum_{j=1}^{n}\left(\hat{y}_{j(i)}-\hat{y}_{j}\right)^{2}}{2 S^{2}} = \frac{r_{i}^{2}}{2} \frac{h_{i i}}{1-h_{i i}}
\end{equation*}
where we should note that $D_i$ may be large due to large $r_i$, or large $h_{ii}$ or both.

\paragraph{Rule: Cook's Distance} 
\begin{itemize}
    \item A point is noteworthy if
        \begin{equation*}
            D_i > \frac{4}{n-2}
        \end{equation*}
    \item In practice, look for gaps in the values of Cook's Distance and not just whether one value exceeds tyhe suggested cut off.
\end{itemize}

\subsubsection{Normality of the Errors}
The assumption of normal errors is (especially) needed in small samples for the validity of $t$-dist based tests and inferences. This assumption is generally checked by looking at the distribution of the residuals or standardized residuals. Recall that the $i$-th least squares residuals is given by $\hat{e}_{i}=y_{i}-\hat{y}_{i}$. We will now show $\hat{e}_{i}=e_{i}-\sum_{j=1}^{n} h_{i j} e_{j}$. First, in the derivation we will need these two facts
\begin{equation*}
    \sum_{i=1}^{n} h_{i j}=1
\end{equation*}
and 
\begin{equation*}
    \sum_{j=1}^{n} x_{j} h_{i j}=\sum_{j=1}^{n}\left[\frac{x_{j}}{n}+\frac{\left(x_{i}-\bar{x}\right)\left(x_{j}-\bar{x}\right) x_{j}}{S X X}\right]=\bar{x}+\frac{\left(x_{i}-\bar{x}\right) S X X}{S X X}=x_{i}
\end{equation*}
We then proceed as follows
\begin{align*} 
    \hat{e}_{i} &=y_{i}-\hat{y}_{i} =y_{i}-h_{i i} y_{i}-\sum_{j \neq i} h_{i j} y_{j} \\ 
    &=y_{i}-\sum_{j=1}^{n} h_{i j} y_{j} \\ 
    &=\beta_{0}+\beta_{1} x_{i}+e_{i}-\sum_{j=1}^{n} h_{i j}\left(\beta_{0}+\beta_{1} x_{j}+e_{j}\right) \\ 
    &=\beta_{0}+\beta_{1} x_{i}+e_{i}-\beta_{0}-\beta_{1} x_{i}-\sum_{j=1}^{n} h_{i j} e_{j} \\ 
    &\color{BurntOrange}{=e_{i}-\sum_{j=1}^{n} h_{i j} e_{j}}
\end{align*}
\hfill \qed \newline
The above result showed that the $i$-th least squares residual is equal to $e_i$ minus a weighted sum of all the $e$'s. There are two cases to consider,
\begin{itemize}
    \item In small to moderate samples, the second term could dominate the first and first and the residuals can look like they come from a normal distribution even if the errors do not. 
    \item When $n$ is large, the second term in the derived result (thistle coloured) has a much smaller variance than that of the first term and as such the first term dominates the last equation. 
\end{itemize}
\textbf{Conclusion:} For large samples, the residuals can be used to assess normality of the errors.

\paragraph{Assessment Using Normal Q-Q}
A normal probability plot of the standardized residuals is obtained by plotting the ordered standardized residuals on the vertical axis against the expected order statistics from a standard normal distribution on the horizontal axes. If the resulting plot produces points ``close'' to a straight line then the data are said to be consistent with that from a normal distribution. On the other hand, departures from linearity provide evidence of non-normality. 
\begin{center}
    \includegraphics[scale=0.25]{img/qq_left.png}
    \includegraphics[scale=0.25]{img/qq_right.png}
    \includegraphics[scale=0.24]{img/qq_over.png}
    \includegraphics[scale=0.24]{img/qq_under.png}
\end{center}
The above four are figures that I borrowed from \url{http://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html} which illustrates how to interpret QQ plots with non-normal behaviour.

\subsubsection{Constant Variance (Homoscedasticity)}
A crucial assumption in any regression analysis is that errors have constant variance. \color{BurntOrange} Notice the difference between error and residual, we have demonstrated in \textbf{(3.2.2 Standardized Residuals)} that residuals are not of constant variance. \color{Black} There are two general methods that we can adopt to overcome this issue, namely (both of which will be discussed later)
\begin{itemize}
    \item Transformations
    \item Weighted Least Squares
\end{itemize}
\color{Red}
\textit{\textbf{Important:} Ignoring non-constant variance when it exists invalidates all inferential tools, including p-values, CI, PI, et cetra!}
\color{Black}

\paragraph{Behaviour of Non-Homoscedasticity} For example, on the plot explanatory var against standardized residuals, we might see that as $x$ increases, the residuals are more spread out, indicating an increasing trend in the variance. 

\paragraph{Checking for Constant Variance} To check this, check the plot of
\begin{equation*}
    |\text{ Residuals }|^{0.5} \text { against } x \quad \text{or} \quad |\text { Standardized Residuals }|^{0.5} \text { against } x
\end{equation*}
The power of $0.5$ here is used to reduce skewness in the absolute values. In the above mentioned example where the residuals become more spread out as $x$ increases, the plot $|\text { Standardized Residuals }|^{0.5} \text { against } x$ will have an overall increasing trend! \color{BurntOrange} This is essentially mirroring all the points to the positive side (and de-skew) to observe a general trend. \color{Black}

\subsection{Transformation}
\subsubsection{Variance Stabilizing Transformations}
\paragraph{Goal} When non-constant variance exists, it is often possible to transform one or both of the regression variables to produce a model in which the error variance is constant. 

\paragraph{Delta Method, Poisson} Suppose that $Y\sim \operatorname{Poi}(\mu = \lambda)$ and we want to find the appropriate transformation of $Y$ for stabilizing variance. \color{Thistle}{In this case, square root is the appropriate transformation to apply.} \color{Black} We will now justify this choice. Consider the McLauren Series expansion
\begin{equation*}
    f(Y)=f(\mathrm{E}(Y))+f^{\prime}(\mathrm{E}(Y))(Y-\mathrm{E}(Y))+\ldots
\end{equation*}
According to the delta rule, the first order variance term is obtained by taking variance on both sides of the above equation, which yields
\begin{equation*}
    \operatorname{Var}(f(Y)) \simeq\left[f^{\prime}(\mathrm{E}(Y))\right]^{2} \operatorname{Var}(Y)
\end{equation*}
Using the proposed transformation $f(Y)=Y^{0.5}$ and recall from properties of Poisson Random Variable that $\operatorname{Var}(Y)=\lambda=\mathrm{E}(Y)$, then
\begin{equation*}
    \operatorname{Var}\left(Y^{0.5}\right) \simeq\left[0.5(\mathrm{E}(Y))^{-0.5}\right]^{2} \operatorname{Var}(Y)=\left[0.5 \lambda^{-0.5}\right]^{2} \lambda=\mathrm{constant}
\end{equation*}
\color{BurntOrange} \textbf{Rule of Thumb:} When both $Y$ and $X$ are measured in the same units then it is often natural to consider the same transformation for both $X$ and $Y$ \color{Black} \newline \newline
Hence in this case our regression model would be
\begin{equation*}
    Y=\beta_{0}+\beta_{1} x+e
\end{equation*}
where
\begin{equation*}
    Y \gets \sqrt{Y} \quad \text{and} \quad x \gets \sqrt{x}
\end{equation*}

\subsubsection{Logarithms to Estimate Percentage Effects}
Consider the regression model
\begin{equation*}
    \log (Y)=\beta_{0}+\beta_{1} \log (x)+e
\end{equation*}
The slope,\footnote{Notice that the first step is possible since here we are considering the regression straight line}
\begin{align*}
    \beta_{1} &=\frac{\Delta \log (Y)}{\Delta \log (x)} =\frac{\log \left(Y_{2}\right)-\log \left(Y_{1}\right)}{\log \left(x_{2}\right)-\log \left(x_{1}\right)}
    =\frac{\log \left(Y_{2} / Y_{1}\right)}{\log \left(x_{2} / x_{1}\right)} \\
    &\cong \frac{Y_{2} / Y_{1}-1}{x_{2} / x_{1}-1}\left(\text { using } \log (1+z) \cong z \text { and assuming } \beta_{1} \text { is small }\right) \\
    &=\frac{100\left(Y_{2} / Y_{1}-1\right)}{100\left(x_{2} / x_{1}-1\right)} =\frac{\% \Delta Y}{\% \Delta x}
\end{align*}

\paragraph{Interpretation} We showed above that $\% \Delta Y \simeq \beta_{1} \times \% \Delta x$. Thus for every \(1 \%\) increase in \(x,\) the model predicts a \(\beta_{1} \%\) increase in \(Y\) (provided
\(\beta_{1}\) is small).


\section{Weighted Least Square Regression}
\subsection{Motivation and Set-Up}
Consider the straight line (simple) linear regression model
\begin{equation*}
    Y_{i}=\beta_{0}+\beta_{1} x_{i}+e_{i} \quad \text{where}~~ e_i \sim N \left(0, \frac{\sigma^2}{w_i}\right)
\end{equation*}
For the weight $w_i$, we should note the following
\begin{itemize}
    \item $w_i \rightarrow \infty \implies Var(e_i) \rightarrow 0$. In this case, the estimates of the regression parameters $\beta_0, \beta_1$ should be such that the fitted line at $x_i$ should be very close to $y_i$. (Small variance means more strict in terms of deviation from the regression line, corresponding to a larger emphasis on the $i$-th data point.)
    \item If $w_i$ is some small value, then the variance of the $i$-th data point would be quite large. In this case, we have a loose restriction of the deviation of the $i$-th data point from the regression line meaning that little emphasis is taken for this data point.
    \item $w_i \rightarrow 0 \implies Var(e_i) \rightarrow \infty$. In this case, we have the variance tending to infinity. Meaning that there is absolutely no restriction/emphasis on the $i$-th data point and it could be simply removed from the set.
\end{itemize}
We define the cost function, WRSS as
\begin{equation*}
    \mathrm{WRSS}=\sum_{i=1}^{n} w_{i}\left(y_{i}-\hat{y}_{W_i}\right)^{2}=\sum_{i=1}^{n} w_{i}\left(y_{i}-b_{0}-b_{1} x_{i}\right)^{2}
\end{equation*}
and the estimators $\bb = [b_0, b_1]^T$ are derived using MLE. 

\paragraph{Intuition behind WRSS} This cost function may seem wierd at first glance, but it intuitively makes sense. Notice that when $w_i$ is large, the $i$-th lost term $w_{i}\left(y_{i}-\hat{y}_{W_i}\right)^{2}$ is payed more emphasis on. On the contrary, when $w_0 \rightarrow 0$, the term $\rightarrow 0$. (Indeed, when Variance of the term $\rightarrow \infty$ we just neglect it.)

\subsection{Deriving LS Regressors}
\paragraph{Derivatives}
\begin{equation}
    \frac{\partial \mathrm{WRSS}}{\partial b_{0}}=-2 \sum_{i=1}^{n} w_{i}\left(y_{i}-b_{0}-b_{1} x_{i}\right)=0
\end{equation}
\begin{equation}
    \frac{\partial \mathrm{WRSS}}{\partial b_{1}}=-2 \sum_{i=1}^{n} w_{i} x_{i}\left(y_{i}-b_{0}-b_{1} x_{i}\right)=0
\end{equation}
\paragraph{Normal Equations} Obtained from rearranging the above equations, we will call them Normal Eq1 and Normal Eq2 respectively for later reference.
\begin{equation}
    \sum_{i=1}^{n} w_{i} y_{i}=b_{0} \sum_{i=1}^{n} w_{i}+b_{1} \sum_{i=1}^{n} w_{i} x_{i}
\end{equation}
\begin{equation}
    \sum_{i=1}^{n} w_{i} x_{i} y_{i}=b_{0} \sum_{i=1}^{n} w_{i} x_{i}+b_{1} \sum_{i=1}^{n} w_{i} x_{i}^{2}
\end{equation}
\paragraph{Rearranging} Use $\text{Normal Eq1} \times\sum_{i=1}^{n} w_{i} x_{i}$ and $\text{Normal Eq2}\times\sum_{i=1}^{n} w_{i}$
\begin{equation}
    \sum_{i=1}^{n} w_{i} x_{i} \sum_{i=1}^{n} w_{i} y_{i}=b_{0} \sum_{i=1}^{n} w_{i} \sum_{i=1}^{n} w_{i} x_{i}+b_{1}\left(\sum_{i=1}^{n} w_{i} x_{i}\right)^{2}
\end{equation}
\begin{equation}
    \sum_{i=1}^{n} w_{i} \sum_{i=1}^{n} w_{i} x_{i} y_{i}=b_{0} \sum_{i=1}^{n} w_{i} \sum_{i=1}^{n} w_{i} x_{i}+b_{1} \sum_{i=1}^{n} w_{i} \sum_{i=1}^{n} w_{i} x_{i}^{2}
\end{equation}
\paragraph{WLS Slope Regressor} \footnote{Note that $\bar{x}_{W}=\sum_{i=1}^{n} w_{i} x_{i} / \sum_{i=1}^{n} w_{i}$ and $\bar{y}_{W}=\sum_{i=1}^{n} w_{i} y_{i} / \sum_{i=1}^{n} w_{i}$}
\begin{align}
    \hat{\beta}_{1W} &= \frac{\sum_{i=1}^{n} w_{i} \sum_{i=1}^{n} w_{i} x_{i} y_{i}-\sum_{i=1}^{n} w_{i} x_{i} \sum_{i=1}^{n} w_{i} y_{i}}{\sum_{i=1}^{n} \sum_{i=1}^{n} w_{i} x_{i}^{2}-\left(\sum_{i=1}^{n} w_{i} x_{i}\right)^{2}} \\
    &= \frac{\sum_{i=11}^{n} x_{i}\left(x_{i}-\bar{x}_{W}\right)\left(y_{i}-\bar{y}_{W}\right)}{\sum_{i=1}^{n} w_{i}\left(x_{i}-\bar{x}_{W}\right)^{2}}
\end{align}
\paragraph{WLS Intercept Regressor}
\begin{equation}
    \hat{\beta}_{0 W}=\frac{\sum_{i=1}^{n} w_{i} y_{i}}{\sum_{i=1}^{n} w_{i}}-\hat{\beta}_{1 W} \frac{\sum_{i=1}^{n} w_{i} x_{i}}{\sum_{i=1}^{n} w_{i}} = \bar{y}_{w}-\hat{\beta}_{1 W} \bar{x}_{W}
\end{equation}

\section{Multiple Linear Regression (Under Construction)}
\subsection{SLR in Matrix Form}
\subsubsection{Set-Up}
The simple linear regression model is
\begin{equation*}
    Y = X\beta + e
\end{equation*}
where $Y \in \mat{n}{1}, X\in \mat{n}{2}, \beta\in \mat{2}{1}, e\in \mat{n}{1}$.
\subsubsection{The Design Matrix}
\begin{equation*}
    X_{n\times 2} = \begin{bmatrix}
        1 & X_1 \\
        1 & X_2 \\
        \vdots & \vdots \\
        1 & X_n
    \end{bmatrix}
    \implies
    X\beta (n\times 1)
    \begin{bmatrix}
        \beta_0 + X_1 \beta_1 \\
        \beta_0 + X_2 \beta_1 \\
        \vdots \\
        \beta_0 + X_n \beta_1
    \end{bmatrix}
\end{equation*}

\subsubsection{Normal Error Regression Model}
\paragraph{Gauss - Markov Conditions}
\begin{itemize}
    \item The errors have zero mean, $E(\be) = \bzero$
    \item The errors have constant variance, $\sigma^2$
    \item The errors are uncorrelated, $V(\be) = \sigma^2 \mathbf{I}$
\end{itemize}
\paragraph{Jointly Normal} The error terms follow a multivariate normal, 
\begin{equation*}
    \be \sim N_n (\Sigma = \bzero, \sigma^2 I)
\end{equation*}

\subsubsection{OLS in Matrix Form}
Consider $\beta = [\beta_0, \beta_1]'$, and the cost function
\begin{align*} 
    RSS(\beta) 
    &= ({Y}-{X} \beta)^{\prime}({Y}-{X} \beta) \\
    &={Y}^{\prime} {Y}+({X} \beta)^{\prime} {X} {\beta}-{Y}^{\prime} {X} {\beta}-({X} {\beta})^{\prime} {Y} \\ 
    &={Y}^{\prime} {Y}+\beta^{\prime}\left({X}^{\prime} {X}\right) \beta-2 {Y}^{\prime} {X} {\beta} 
\end{align*}
\paragraph{Derivative}
\begin{equation*}
    \frac{\partial RSS(\beta)}{\partial \beta} = 0 - 2X'Y + 2X'X\beta
\end{equation*}
\paragraph{Normal Equation} obtained by setting derivate to zero and re-arrange
\begin{equation*}
    2X'X\hat{\beta} = 2X'Y
\end{equation*}
\paragraph{OLS Regressor}
\begin{equation*}
    \hat{\beta} = (X'X)^{-1}X'Y
\end{equation*}

\paragraph{Reduction From Matrix Notation to Scaler Notation} We will now show that the matrix form we just derived is equivalent to the form that we discussed/derived earlier in the chapter, where we computed the two estimators separately.
First, Let's show some identities that will be used in the derivation.
\begin{equation*}
	\mathbf{X}^{\prime} \mathbf{X}=\left(\begin{array}{ccc}{1} & {1} & {\cdots} \\ {x_{1}} & {x_{2}} & {\cdots x_{n}}\end{array}\right)\left(\begin{array}{cc}{1} & {x_{1}} \\ {1} & {x_{2}} \\ {\vdots} & {\vdots} \\ {1} & {x_{n}}\end{array}\right)=\left(\begin{array}{cc}{n} & {\sum_{i=1}^{n} x_{i}} \\ {\sum_{i=1}^{n} x_{i}} & {\sum_{i=1}^{n} x_{i}^{2}}\end{array}\right)=n\left(\begin{array}{cc}{1} & {\bar{x}} \\ {\bar{x}} & {\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}}\end{array}\right)
\end{equation*}
\begin{equation*}
	\implies \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1}=\frac{1}{n\left(\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}-(\bar{x})^{2}\right)}\left(\begin{array}{cc}{\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}} & {-\bar{x}} \\ {-\bar{x}} & {1}\end{array}\right) = \frac{1}{S X X}\left(\begin{array}{cc}{\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}} & {-\bar{x}} \\ {-\bar{x}} & {1}\end{array}\right)
\end{equation*}
and
\begin{equation*}
	\mathbf{X}^{\prime} \mathbf{Y}=\left(\begin{array}{cccc}{1} & {1} & {\cdots} & {1} \\ {x_{1}} & {x_{2}} & {\cdots} & {x_{n}}\end{array}\right) \left(\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{n}}\end{array}\right) = \left(\begin{array}{c}{\sum_{i=1}^{n} y_{i}} \\ {\sum_{i=1}^{n} x_{i} y_{i}}\end{array}\right)
\end{equation*}
Thus, the regressor could be break down into
\begin{align*}
	\hat{\beta} 
	&=\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y} \\
	&=\frac{1}{S X X}\left(\begin{array}{cc}{\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}} & {-\bar{x}} \\ {-\bar{x}} & {1}\end{array}\right)\left(\begin{array}{c}{\sum_{i=1}^{n} y_{i}} \\ {\sum_{i=1}^{n} x_{i} y_{i}}\end{array}\right) \\
	&= \frac{1}{S X X}\left(\begin{array}{c}{\frac{1}{n} \sum_{i=1}^{n} y_{i} \sum_{i=1}^{n} x_{i}^{2}-\bar{x} \sum_{i=1}^{n} x_{i} y_{i}} \\ {\sum_{i=1}^{n} x_{i} y_{i}-\bar{x} \sum_{i=1}^{n} y_{i}}\end{array}\right) \\
	&= \begin{pmatrix}
		\frac{\bar{y}\left\{\sum_{i=1}^{n} x_{i}^{2}-n \bar{x}^{2}\right\}-\bar{x}\left\{\sum_{i=1}^{n} x_{i} y_{i}-n \overline{x y}\right\}}{S X X} \\
		{S X Y}/{S X X}
	\end{pmatrix} \\
	&= \left(\begin{array}{l}{\bar{y}-\frac{S X Y}{S X X} \bar{x}} \\ {\frac{S X Y}{S X X}}\end{array}\right)
\end{align*}
Indeed, the results that we get confirms that our matrix form is equivalent. \qed 


\subsubsection{Properties of OLS Regressors}
\paragraph{Expectation: Unbiased Estimator}
\begin{align*}
    E(\hat{\beta}) &= E((X'X)^{-1}X'Y) \\
    &= E(Y)(X'X)^{-1}X' \\
    &= (X'X)^{-1}X' E(X\beta + e) \\
    &= (X'X)^{-1}X' (X\beta + 0) \\
    &= I\beta = \beta
\end{align*}
\qed
\paragraph{Variance - Covariance Matrix}
\begin{align*}
    VAR(\hat{\beta}) &= VAR( (X'X)^{-1}X'Y ) \\
    &= [(X'X)^{-1}X'] \sigma^2 I [(X'X)^{-1}X']' \\
    &= \sigma^2 I (X'X)^{-1}  \underbrace{X'X (X'X)^{-1}}_{\equiv~I} \\
    &= \sigma^2 (X'X)^{-1} \\
    &=
    \begin{bmatrix}
        VAR(\hat{\beta}_0) =  \frac{\sigma^2\sum x_{i}^{2}}{n S X X}
        & COV(\hat{\beta}_0, \hat{\beta}_1) = \frac{-\sigma^2\bar{x}}{S X X} \\
        COV(\hat{\beta}_1, \hat{\beta}_0) = \frac{-\sigma^2\bar{x}}{S X X}
        & VAR(\hat{\beta}_1) = \frac{\sigma^2}{SXX}
    \end{bmatrix}
\end{align*}

\subsubsection{The Hat Matrix}
\paragraph{Defining the Hat}
We have
\begin{equation*}
    \widehat{\mathbf{e}}=\left(\begin{array}{c}{e_{1}} \\ {e_{2}} \\ {\vdots} \\ {e_{n}}\end{array}\right)=\mathbf{Y}-\widehat{\mathbf{Y}}=\mathbf{Y}-\mathbf{X} \widehat{\boldsymbol{\beta}} \quad \text{and} \quad  \widehat{\mathbf{Y}}=\mathbf{X} \widehat{\boldsymbol{\beta}}=\mathbf{X}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y}=\mathbf{H} \mathbf{Y}
\end{equation*}
where
\begin{equation*}
    \text{HAT Matrix} \quad \mathbf{H} := \mathbf{X}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime}
\end{equation*}
Thus, $\widehat{\mathbf{e}}=\mathbf{Y}-\mathbf{H Y}=(\mathbf{I}-\mathbf{H}) \mathbf{Y}$
\paragraph{Facts About the Hat Matrix}
\begin{itemize}
    \item $\mathbf{H}$ is symmetric
        \begin{equation*}
            H' = (X(X'T)^{-1}X')' = X(X'X)^{-1}X' = H
        \end{equation*}
    \item $\mathbf{H}$ is idempotent
        \begin{align*}
            H^2 &= HH =  X(X'X)^{-1} \underbrace{X' X(X'X)^{-1}}_{\equiv ~I}X' \\
            &=  X(X'X)^{-1}X' = H
        \end{align*}
\end{itemize}

\subsubsection{Properties of the Residuals in Matrix Form}
\begin{align*}
    \hat{e} &= (I - H)Y = (I - H)(X\beta + e) \\
    &= (I - H)X\beta + (I - H)e \\
    &= IX\beta - HX\beta + (I - H)e \\
    &= X\beta - X\underbrace{X(X'X)^{-1}X}_{\equiv ~I} \beta + (I - H)e \\
    &= X\beta - X\beta + (I - H)e \\
    &= (I - H)e
\end{align*}
\paragraph{Expectation}
\begin{equation*}
    E(\hat{e}| X) = E((I - H)Y | X) = E((I - H)e | X) = \underbrace{E(e|X)}_{\equiv 0}(I - H) = 0
\end{equation*}

\paragraph{Variance - Covariance}
\begin{align*}
    VAR(\hat{e}|X) &= VAR((I-H)e|X) \\
    &= (I - H) VAR(e|X) (I - H)' \\
    &= (I - H) \sigma^2 I (I - H) '\\
    &= (I - H) \sigma^2 I (I - H) \quad \text{since $(I - H)$ is symmetric} \\
    &= \sigma^2 (I - H) \quad \text{since $(I - H)$ is idempotent}
\end{align*}

\subsubsection{ANOVA in Matrix Form}
\paragraph{Total Sum Squared (SST)}
\begin{align*}
    SST = \sum\left(y_{i}-\bar{y}\right)^{2} &=\sum y_{i}^{2}-n \bar{y}^{2} \\
    &= Y'Y - \frac{1}{n}Y'J Y \\
    &= \underbrace{Y'\underbrace{(I - \frac{1}{n}J)}_{\text{symmetric square matrix}} Y}_{\text{quadratic form}}
\end{align*}
To find the corresponding df, we first check the idempotency of $I - \frac{1}{n}J$. Indeed,
\begin{equation*}
    (I - \frac{1}{n}J)(I - \frac{1}{n}J) = I^2 - \frac{2}{n}J + \frac{1}{n^2}J^2 = I - \frac{1}{n}J
\end{equation*}
Then, applying $Rank$(idempotent mat) = $Tr$(idempotent mat), we have
\begin{equation*}
    Rank(I - \frac{1}{n}J) = n - 1 = \text{degrees of freedom of} ~SST
\end{equation*}

\paragraph{Residual Sum Squared (RSS)}
Notice that $(I - H)$ is symmetric and idempotent,
\begin{equation*}
    R S S=\sum \hat{e}_{i}^{2} = \hat{e}' \hat{e} = Y'(I - H)(I - H)Y = Y'(I - H)Y
\end{equation*}
and the corresponding degrees of freedom is
\begin{equation*}
    Rank(I - H) = Rank(I) - Rank(H) = Tr(I) - Tr(H) = n-2 = \text{df of MSE}
\end{equation*}

\paragraph{Regression Sum Squared (SSReg)}
\begin{align*}
    SSReg &= SST - RSS \\
    &= Y'(I - 1/n J)Y - Y'(I - H)Y \\
    &= Y'IY - Y' 1/n JY - Y'IY + Y'HY \\
    &= Y'(H - \frac{1}{n}J) Y
\end{align*}
and the corresponding degrees of freedom is
\begin{equation*}
    Rank(H - \frac{1}{n}J) = Rank(H) - Rank(\frac{1}{n}J) = \sum h_{ii} - \sum \frac{1}{n} = 2 - 1 = 1 = \text{df of MSReg}
\end{equation*}

\subsubsection{ANOVA Table in Matrix Form}
\begin{center}
    \begin{tabular}{|c||c|c|}
        \hline
        Source & SS & $df$ \\ \hline
        Regression & $\mathbf{Y}^{\prime}\left(\mathbf{H}-\frac{1}{n} \mathbf{J}\right) \mathbf{Y}$ & 1 \\ \hline
        Error & $\mathbf{Y}^{\prime}(\mathbf{I}-\mathbf{H}) \mathbf{Y}$ & $n-2$ \\ \hline
        Total & $\mathbf{Y}^{\prime}\left(\mathbf{I}-\frac{1}{\mathbf{n}} \mathbf{J}\right) \mathbf{Y}$ & $n-1$ \\ \hline
    \end{tabular}
\end{center}


\subsection{Estimation and Inference in MLR}
\subsubsection{The MLR Model} In the familiar SLR settings, we have that $Y = \beta_0 + \beta_1 x + error$, where we had one predictor variable (so called `simple'). In the MLR settings, we want to add multiple predictors. This leads to the formulation of expectation
\begin{equation*}
    \mathrm{E}\left(Y | X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{p}=x_{p}\right)=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\ldots+\beta_{p} x_{p}
\end{equation*}
Thus,
\begin{equation*}
    Y_{i}=\beta_{0}+\beta_{1} x_{1 i}+\beta_{2} x_{2 i}+\ldots+\beta_{p} x_{p i}+e_{i}
\end{equation*}
where $e_i$ is the random fluctuation (or error) in $Y_i$ such that $E(e_i |X) = 0$. In this case the response variable $Y$ is predicted from $p$ predictor variables $X_1,X_2,...,X_p$ and \color{BurntOrange} the relationship between $Y$ and $X_1,X_2,...,X_p$ is \textit{linear} in the parameters $\beta_{0}, \beta_{1}, \beta_{2}, \ldots, \beta_{p}$.  \color{Black}
\subsubsection{OLS Regressors - Expanded Scaler Form}
\paragraph{The RSS Cost}
The least squares regressors of $\beta_{0}, \beta_{1}, \beta_{2}, \ldots, \beta_{p}$ are the value of $b_{0}, b_{1}, b_{2}, \ldots, b_{p}$ for which the sum of the squared residuals, we define our cost function RSS
\begin{equation*}
    \mathrm{RSS}=\sum_{i=1}^{n} \hat{e}_{i}^{2}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-b_{0}-b_{1} x_{1 i}-b_{2} x_{2 i}-\ldots-b_{p} x_{p i}\right)^{2}
\end{equation*}
\paragraph{Derivatives}
\begin{equation*}
    \begin{matrix}
        &\frac{\partial \operatorname{RSS}}{\partial b_{0}}=-2 \sum_{i=1}^{n}\left(y_{i}-b_{0}-b_{1} x_{1 i}-b_{2} x_{2 i}-\ldots-b_{p} x_{p i}\right)=0 \quad \,\,\\ \\
        &\frac{\partial \operatorname{RSS}}{\partial b_{1}}=-2 \sum_{i=1}^{n} x_{1 i}\left(y_{i}-b_{0}-b_{1} x_{1 i}-b_{2} x_{2 i}-\ldots-b_{p} x_{p i}\right)=0\\ \\
        & \dots \\ \\
        &\frac{\partial \operatorname{RSS}}{\partial b_{p}}=-2 \sum_{i=1}^{n} x_{p i}\left(y_{i}-b_{0}-b_{1} x_{1 i}-b_{2} x_{2 i}-\ldots-b_{p} x_{p i}\right)=0
    \end{matrix}
\end{equation*}
which will give us a system of $p+1$ equations and $p+1$ variables to optimize on. Notice that this usually requires computers to help in the actual optimization calculations. Hence we should see the much more concise matrix formulation.

\subsubsection{OLS Regressors - Matrix Form}
Let $Y \in \mat{n}{(p+1)}, X\in \mat{(n)}{p+1}, \beta\in \mat{(p+1)}{1}$ and $e\in \mat{n}{1}$ given by
\begin{equation*}
    \mathbf{Y}=\left(\begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\vdots} \\ {y_{n}}\end{array}\right), \mathbf{X}=\left(\begin{array}{ccc}{1} & {x_{11}} & {\cdots x_{1 p}} \\ {1} & {x_{21}} & {\cdots x_{2 p}} \\ {\vdots} & {\vdots} & {} \\ {1} & {x_{n 1}} & {\cdots x_{n p}}\end{array}\right), \beta=\left(\begin{array}{c}{\beta_{0}} \\ {\beta_{1}} \\ {\vdots} \\ {\beta_{p}}\end{array}\right), \mathbf{e}=\left(\begin{array}{c}{e_{1}} \\ {e_{2}} \\ {\vdots} \\ {e_{n}}\end{array}\right)
\end{equation*}
Then the multiple linear regression model in matrix notation could be written as
\begin{equation*}
    \mathbf{Y}=\mathbf{X} \beta+\mathbf{e}
\end{equation*}
\paragraph{Notation} Let $\bx_i'$ denote the $i$-th row of the design matrix $\mathbf{X}$, then
\begin{equation*}
    \mathbf{x}_{i}^{\prime}:=\begin{bmatrix}
        1 & x_{i1} & x_{i2} & ... & x_{ip} 
    \end{bmatrix} 
    \in \mat{1}{(p+1)}
\end{equation*}
This enables us to write 
\begin{equation*}
    \mathrm{E}(Y | X=x)=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\ldots+\beta_{p} x_{p}=\mathbf{x}_{i}^{\prime} \beta
\end{equation*}
\paragraph{RSS Cost (Matrix Notation)}
The residual sum of squares as a function of $\beta$ can be written in matrix form as
\begin{align*}
    \operatorname{RSS}(\beta)
    &=(\mathbf{Y}-\mathbf{X} \beta)^{\prime}(\mathbf{Y}-\mathbf{X} \beta) \\
    &= \mathbf{Y}^{\prime} \mathbf{Y}+(\mathbf{X} \beta)^{\prime} \mathbf{X} \boldsymbol{\beta}-\mathbf{Y}^{\prime} \mathbf{X} \boldsymbol{\beta}-(\mathbf{X} \boldsymbol{\beta})^{\prime} \mathbf{Y} \\
    &= \mathbf{Y}^{\prime} \mathbf{Y}+\beta^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right) \beta-2 \mathbf{Y}^{\prime} \mathbf{X} \beta
\end{align*}
\paragraph{Normal Equation} is obtained by setting the derivative to zero
\begin{equation*}
    \left(\mathbf{X}^{\prime} \mathbf{X}\right) \beta=\mathbf{X}^{\prime} \mathbf{Y}
\end{equation*}
\paragraph{OLS Regressor}
\begin{equation*}
    \hat{\beta}=\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y}
\end{equation*}
Hence, our fitted line is given by
\begin{equation*}
    \hat{\mathbf{Y}}=\mathbf{X} \hat{\beta}
\end{equation*}
and the residuals are
\begin{equation*}
    \hat{\mathbf{e}}=\mathbf{Y}-\hat{\mathbf{Y}}=\mathbf{Y}-\mathbf{X} \hat{\beta}
\end{equation*}


\subsubsection{Properties of OLS Regressors}
\paragraph{Expectation: Unbiased Estimator}
\begin{align*}
    E(\hat{\beta}) &= E((X'X)^{-1}X'Y) \\
    &= E(Y)(X'X)^{-1}X' \\
    &= (X'X)^{-1}X' E(X\beta + e) \\
    &= (X'X)^{-1}X' (X\beta + 0) \\
    &= I\beta = \beta
\end{align*}
\qed
\paragraph{Variance - Covariance Matrix}
\begin{align*}
    VAR(\hat{\beta}) &= VAR( (X'X)^{-1}X'Y ) \\
    &= [(X'X)^{-1}X'] \sigma^2 I [(X'X)^{-1}X']' \\
    &= \sigma^2 I (X'X)^{-1}  \underbrace{X'X (X'X)^{-1}}_{\equiv~I} \\
    &= \sigma^2 (X'X)^{-1} \\
%    &=
%    \begin{bmatrix}
%        VAR(\hat{\beta}_0) =  \frac{\sigma^2\sum x_{i}^{2}}{n S X X}
%        & COV(\hat{\beta}_0, \hat{\beta}_1) = \frac{-\sigma^2\bar{x}}{S X X} \\
%        COV(\hat{\beta}_1, \hat{\beta}_0) = \frac{-\sigma^2\bar{x}}{S X X}
%        & VAR(\hat{\beta}_1) = \frac{\sigma^2}{SXX}
%    \end{bmatrix}
\end{align*}

\subsubsection{The Hat Matrix}
\paragraph{Defining the Hat}
We have
\begin{equation*}
    \widehat{\mathbf{e}}=\left(\begin{array}{c}{e_{1}} \\ {e_{2}} \\ {\vdots} \\ {e_{n}}\end{array}\right)=\mathbf{Y}-\widehat{\mathbf{Y}}=\mathbf{Y}-\mathbf{X} \widehat{\boldsymbol{\beta}} \quad \text{and} \quad  \widehat{\mathbf{Y}}=\mathbf{X} \widehat{\boldsymbol{\beta}}=\mathbf{X}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y}=\mathbf{H} \mathbf{Y}
\end{equation*}
where
\begin{equation*}
    \text{HAT Matrix} \quad \mathbf{H} := \mathbf{X}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime}
\end{equation*}
Thus, $\widehat{\mathbf{e}}=\mathbf{Y}-\mathbf{H Y}=(\mathbf{I}-\mathbf{H}) \mathbf{Y}$
\paragraph{Facts About the Hat Matrix}
\begin{itemize}
    \item $\mathbf{H}$ is symmetric
        \begin{equation*}
            H' = (X(X'T)^{-1}X')' = X(X'X)^{-1}X' = H
        \end{equation*}
    \item $\mathbf{H}$ is idempotent
        \begin{align*}
            H^2 &= HH =  X(X'X)^{-1} \underbrace{X' X(X'X)^{-1}}_{\equiv ~I}X' \\
            &=  X(X'X)^{-1}X' = H
        \end{align*}
\end{itemize}

\subsubsection{Properties of the Residuals in Matrix Form}
\begin{align*}
    \hat{e} &= (I - H)Y = (I - H)(X\beta + e) \\
    &= (I - H)X\beta + (I - H)e \\
    &= IX\beta - HX\beta + (I - H)e \\
    &= X\beta - X\underbrace{X(X'X)^{-1}X}_{\equiv ~I} \beta + (I - H)e \\
    &= X\beta - X\beta + (I - H)e \\
    &= (I - H)e
\end{align*}
\paragraph{Expectation}
\begin{equation*}
    E(\hat{e}| X) = E((I - H)Y | X) = E((I - H)e | X) = \underbrace{E(e|X)}_{\equiv 0}(I - H) = 0
\end{equation*}

\paragraph{Variance - Covariance}
\begin{align*}
    VAR(\hat{e}|X) &= VAR((I-H)e|X) \\
    &= (I - H) VAR(e|X) (I - H)' \\
    &= (I - H) \sigma^2 I (I - H) '\\
    &= (I - H) \sigma^2 I (I - H) \quad \text{since $(I - H)$ is symmetric} \\
    &= \sigma^2 (I - H) \quad \text{since $(I - H)$ is idempotent}
\end{align*}


\subsubsection{Residual Sum of Squares (RSS)}
The residual sum of squares as a function of the OLS regressors $\hat{\beta}$ can be written in matrix form as follows
\begin{equation*}
    \operatorname{RSS}=\operatorname{RSS}(\hat{\beta})=(\mathbf{Y}-\mathbf{X} \hat{\beta})^{\prime}(\mathbf{Y}-\mathbf{X} \hat{\beta})=\hat{\mathbf{e}}^{\prime } \hat{\mathbf{e}}=\sum_{i=1}^{n} \hat{e}_{i}^{2}
\end{equation*}

\subsubsection{Estimating Error Variance}
\begin{equation*}
    \text{Unbiased Estimator}\quad \hat{\sigma}^2 = S^2 = \frac{\mathrm{RSS}}{n-p-1}=\frac{1}{n-p-1} \sum_{i=1}^{n} \hat{e}_{i}^{2}
\end{equation*}


\subsubsection{CIs and Significance Tests}
Assuming that the errors are normally distributed\footnote{It is worth noticing that errors are almost always assumed to be normal when we are trying to do inferences on the regression results (Estimates).} with constant variance, then for each $i = 0,1,...,p$ we have the test statistic
\begin{equation*}
    T_{i}=\frac{\hat{\beta}_{i}-\beta_{i}}{\operatorname{se}(\hat{\beta_i})} \sim t_{df = (n-p-1)}
\end{equation*}
where we recall that the variance-covariance matrix is $VAR(\hat{\beta}) = \sigma^2 (X'X)^{-1}$, and thus
\begin{equation*}
    \mathrm{se}(\hat{\beta}_i) = \hat{\sigma}^2 \left[(X'X)^{-1}\right]_{ii} =  \frac{\left[(X'X)^{-1}\right]_{ii}}{n-p-1} \sum_{i=1}^{n} \hat{e}_{i}^{2}
\end{equation*}


\subsubsection{ANOVA Test for Linear Association (Y with all/subset of predictors)}
\paragraph{Hypothesis} The goal is to test if there is a linear association between the explanatory variables and the explained variable. So the null hypothesis we will use is $H_{0}: \beta_{1}=\beta_{2}=\ldots=\beta_{p}=0$ against the alternative hypothesis that $H_{A}: \text { at least some of the } \beta_{i} \neq 0$. 
\paragraph{Test Statistic} Under the assumption that $e_1,...,e_n$ are independent and normally distributed, it can be shown that the $F$ test statistic is defined as follows and follows a $\mathcal{F}$ distribution.
\begin{equation*}
    F=\frac{\text { SSreg } / p}{\operatorname{RSS} /(n-p-1)} {\sim} \mathcal{F}_{(p, n-p-1)}
\end{equation*}
\paragraph{MLR ANOVA Table}
\begin{center}
    \begin{tabular}{|c||c|c|c|c|}
        \hline
        Source of Variation & $df$ & SS & MS = SS/$df$ & $F$ \\ \hline
        Regression & $p$ & SSReg & SSReg/$p$ & $F=\frac{\text { SSreg } / p}{\operatorname{RSS} /(n-p-1)}$ \\ \hline
        Residual & $n-p-1$ & RSS & $S^2 = \frac{\text{RSS}}{n-p-1}$ & \\ \hline
        Total & $n-1$ & SST = $SYY$ & & \\ \hline
    \end{tabular}
\end{center}
\paragraph{Notes}
\begin{itemize}
    \item $R^2$, the coefficient of determination of the regression line, is defined as the proportion of the total sample variability in the $Y$'s explained by the regression model, that is 
        \begin{equation*}
            R^{2}=\frac{\text { SSreg }}{\mathrm{SST}}=1-\frac{\mathrm{RSS}}{\mathrm{SST}}
        \end{equation*}
    \item The above formulation doesn't capture the problem where we have irrelevant predictor variables in the regression equation. To compensate, we define the \textbf{adjusted coefficient of determination}, $R_{adj}^2$
        \begin{equation*}
            R_{{adj}}^{2}=1-\frac{\operatorname{RSS} /(n-p-1)}{\operatorname{SST} /(n-1)} 
        \end{equation*}
        Here $S^2 = \frac{\mathrm{RSS}}{n-p-1}$ is an unbiased estimate of $\sigma^2 = \mathrm{Var}(Y_i) = \mathrm{Var}(e_i)$ while $SST/(n-1)$ is an unbiased estimate of $\sigma^2 = \mathrm{Var}(Y_i)$ when $\beta_1 = \beta_2 = ... = \beta_p = 0$. \color{BurntOrange} Thus, when comparing models with different numbers of predictors one should use $R_{{adj}}^{2}$ rather than $R^2$. \color{Black}
    \item The F-test is always used first to test for the existence of a linear association between Y and ANY of the p x-variables. If the F-test is significant then a natural question to ask is
        \begin{quote}
            For which of the $p$ $x$-variables is there evidence of a linear association with $Y$? To answer this question we could perform $p$ separate $t$-tests of $H_0 : b_1 = 0$. However, as we shall see later there are problems with interpreting these $t$-tests when the predictor variables are highly correlated.
        \end{quote}
        The above problem could be addressed by the \textbf{partial F-test}\footnote{Notice that here the partial means we are not testing all the $\beta_1$ up to $\beta_p$'s but only a subset of them.} which is defined below.
        
\end{itemize}

\subsubsection{Partial F-Test}
\paragraph{Goal} Test whether a specified subset of the predictors have regression coefficients equal to zero.
\paragraph{Hypothesis}  Suppose that we are interested in testing 
\begin{equation*}
    \begin{array}{c}{H_{0}: \beta_{1}=\beta_{2}=\ldots=\beta_{k}=0 \text { where } k<p} \\ {\text { i.e., } Y=\beta_{0}+\beta_{k+1} x_{k+1}+\ldots+\beta_{p} x_{p}+e \text { (reduced model) }}\end{array}
\end{equation*}
against
\begin{equation*}
    \begin{array}{c}{H_{A}: H_{0} \text { is not true }} \\ {\text { i.e., } Y=\beta_{0}+\beta_{1} x_{1}+\ldots+\beta_{k} x_{k}+\beta_{k+1} x_{k+1}+\ldots+\beta_{p} x_{p}+e ~(\text{full model})}\end{array}
\end{equation*}
This can be done using the $F$-Test. Define
\begin{align*}
    &\text{RSS(Full)} := \text{RSS under the full model} \\
    &\text{RSS(reduced)} := \text{RSS under the reduced model}
\end{align*}
Then, the $F$-statistic is given by
\begin{align*}
    F &= \frac{(\mathrm{RSS}(\mathrm{reduced})-\mathrm{RSS}(\mathrm{full})) /\left(\mathrm{df}_{\mathrm{reduced}}-\mathrm{df}_{\mathrm{full}}\right)}{\operatorname{RSS}(\mathrm{full}) / \mathrm{df}_{\mathrm{full}}} \\
    &= \frac{(\mathrm{RSS}(\mathrm{reduced})-\mathrm{RSS}(\mathrm{full})) / k}{\operatorname{RSS}(\mathrm{full}) /(n-p-1)}
\end{align*}
The reduction on the second line above is done by noticing that the reduced model has $p + 1 - k$ predictors and thus $\left(\mathrm{df}_{\mathrm{reduced}}-\mathrm{df}_{\mathrm{full}}\right) = [n-(p+1-k)]-[n-(p+1)]=k$.

\subsection{Analysis of Covariance}


% TODO: Adding to MLR

\newpage
\section{Selected Properties, Formulae, and Theorems}
This section contains various properties mentioned in the slides/book. They may or may not have appeared in precious sections. 
\subsection{Properties of Fitted Regression Line}
\begin{itemize}
    \item $\sum_{i = 1}^n \hat{e}_i = 0$
    \item $RSS = \sum_{i = 1}^n \hat{e}_i^2 \neq 0$, generally. Except for when we have perfect fit.
    \item $\sum_{i= 1}^n \hat{e}_i x_i = 0$
    \item $\sum_{i= 1}^n \hat{e}_i \hat{y}_i = 0$
    \item $\sum_{i = 1}^n \hat{y}_i = \sum_{i= 1}^n y_i$
\end{itemize}

\subsection{Rules of Expectation}
\begin{itemize}
    \item $E(a) = a, \forall a \in \real)$
    \item $E(aY) = aE(Y)$
    \item $E(X \pm Y) = E(X) \pm E(Y)$
    \item $X\independent Y \implies E(EY) = E(X) E(Y)$
    \item Tower Rule: $E(Y) = E(E(Y|X))$
\end{itemize}

\subsection{Variance and Covariance}
\begin{itemize}
    \item $V(a) = 0, \forall a \in \real$
    \item $V(aY) = a^2V(Y)$
    \item $\operatorname{Cov}(X, Y)= \mathrm{E}\{(X-\mathrm{E}(X))(Y-\mathrm{E}(Y))\}=\mathrm{E}(X Y)-\mathrm{E}(X) \mathrm{E}(Y)$
    \item $\operatorname{Cov}(Y, Y)=V(Y)$
    \item $\mathrm{V}(Y)=\mathrm{V}[\mathrm{E}(Y | X)]+\mathrm{E}[\mathrm{V}(Y | X)]$
    \item $\mathrm{V}(X \pm Y)=\mathrm{V}(X)+\mathrm{V}(Y) \pm 2 \operatorname{Cov}(X, Y)$
    \item $\operatorname{Cov}(X, Y)=0, \text { if } X \text { and } Y \text { are independent }$
    \item $\operatorname{Cov}(a X+b Y, c U+d W)=a c \operatorname{Cov}(X, U)+a d \operatorname{Cov}(X, W)+b c \operatorname{Cov}(Y, U)+b d \operatorname{Cov}(Y, W)$
    \item $\text {Correlation: } \rho=\frac{\operatorname{Cov}(X, Y)}{\sqrt{\mathrm{V}(X) \mathrm{V}(Y)}}$
\end{itemize}

\subsection{The Theorem of Gauss-Markov}
Under the conditions of the simple linear regression model, the OLS regressors are BLUE (``Best Linear Unbiased Estimators'').
\begin{itemize}
    \item Best - obtains the minimum variance among all unbiased linear estimators. 
    \item Linear - Linear in the parameter space. That is, feature maps are linear, although the actual curve of regression is `non-linear'.
    \item Unbiased - The estimators are unbiased, namely $\hat{\beta}_0, \hat{\beta}_1$.
    \item Estimator - Estimators $\hat{\beta}_0, \hat{\beta}_1$ for $\beta_0$ and $\beta_1$ respectively.
\end{itemize}

\subsection{Matrix Form Rules}
\subsubsection{Summations}
Consider $\mathbf{A}, \mathbf{B}$ as compatible matrices where appropriate and $k \in \real$ then
\begin{itemize}
    \item $\mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A}$
    \item $(\mathbf{A}+\mathbf{B})+\mathbf{C}=\mathbf{A}+(\mathbf{B}+\mathbf{C})$
    \item $(\mathbf{A B}) \mathbf{C}=\mathbf{A}(\mathbf{B C})$
    \item $\mathbf{C}(\mathbf{A}+\mathbf{B})=\mathbf{C} \mathbf{A}+\mathbf{C B}$
    \item $k(\mathbf{A}+\mathbf{B})=k \mathbf{A}+k \mathbf{B}$
\end{itemize}
\subsubsection{Transpositions}
\begin{itemize}
    \item $\left(\mathbf{A}^{\prime}\right)^{\prime}=\mathbf{A}$
    \item $(\mathbf{A}+\mathbf{B})^{\prime}=\mathbf{A}^{\prime}+\mathbf{B}^{\prime}$
    \item $(\mathbf{A B})^{\prime}=\mathbf{B}^{\prime} \mathbf{A}^{\prime}$
    \item $(\mathbf{A B C})^{\prime}=\mathbf{C}^{\prime} \mathbf{B}^{\prime} \mathbf{A}^{\prime}$, this properties is known as `cyclic'
\end{itemize}
\subsubsection{Inversions}
\begin{itemize}
    \item $(\mathbf{A B})^{-1}=\mathbf{B}^{-1} \mathbf{A}^{-1}$
    \item $(\mathbf{A B C})^{-1}=\mathbf{C}^{-1} \mathbf{B}^{-1} \mathbf{A}^{-1}$
    \item $\left(\mathbf{A}^{-1}\right)^{-1}=\mathbf{A}$
    \item $\left(\mathbf{A}^{T}\right)^{-1}=\left(\mathbf{A}^{-1}\right)^{T}$
\end{itemize}
\subsubsection{Covariance Matrix}
\begin{itemize}
    \item The variance-covariance matrix of a random vector $\mathbf{Y}$ is a symmetric, positive semi-definite matrix, defined as
    \begin{align*}
        \operatorname{Var}(\mathbf{Y}) 
        &= \mathrm{E}\left[(\mathbf{Y}-\mathbf{E}(\mathbf{Y}))(\mathbf{Y}-\mathbf{E}(\mathbf{Y}))^{\prime}\right] \\
        &= E\left(
            \begin{array}{ccc}
                {\left(Y_{1}-E\left(Y_{1}\right)\right)^{2}} & {\left(Y_{1}-E\left(Y_{1}\right)\right)\left(Y_{2}-E\left(Y_{2}\right)\right)} & {\ldots} \\ {\left(Y_{2}-E\left(Y_{2}\right)\right)\left(Y_{1}-E\left(Y_{1}\right)\right)} & {\left(Y_{2}-E\left(Y_{2}\right)\right)^{2}} & {\cdots} \\ {\vdots} & {\vdots} & {\ddots}
            \end{array}
        \right)
    \end{align*}
    \item Let $\mathbf{A}$ be a square matrix of constants
        \begin{align*}
            \operatorname{Var}(\mathbf{A Y}) 
            &=\mathrm{E}\left[(\mathbf{A} \mathbf{Y}-\mathbf{E}(\mathbf{A} \mathbf{Y}))(\mathbf{A} \mathbf{Y}-\mathbf{E}(\mathbf{A} \mathbf{Y}))^{\prime}\right] \\ 
            &=\mathbf{E}\left[\mathbf{A}(\mathbf{Y}-\mathbf{E}(\mathbf{Y}))(\mathbf{Y}-\mathbf{E}(\mathbf{Y}))^{\prime} \mathbf{A}^{\prime}\right] \\ 
            &=\mathbf{A} \mathbf{E}\left[(\mathbf{Y}-\mathbf{E}(\mathbf{Y}))(\mathbf{Y}-\mathbf{E}(\mathbf{Y}))^{\prime}\right] \mathbf{A}^{\prime} \\ 
            &=\mathbf{A} \mathbf{V} \mathbf{a r}(\mathbf{V}) \mathbf{A}^{\prime}
        \end{align*}
\end{itemize}


\end{document}
